{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to extract hidden-states and attention heads activations from roberta model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from model import RobertaExtractor\n",
    "from tokenizer import tokenize\n",
    "\n",
    "\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils import set_seed\n",
    "from numpy import linalg as la\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_folder(path):\n",
    "    \"\"\"Create adequate folders if necessary.\"\"\"\n",
    "    try:\n",
    "        if not os.path.isdir(path):\n",
    "            check_folder(os.path.dirname(path))\n",
    "            os.mkdir(path)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(activations, path, name, run_index, n_layers_hidden=13, n_layers_attention=12, hidden_size=768):\n",
    "    assert activations.values.shape[1] == (n_layers_hidden + n_layers_attention) * hidden_size\n",
    "    indexes = [[index*hidden_size, (index+1)*hidden_size] for index in range(n_layers_hidden + n_layers_attention)]\n",
    "    for order in [2]:\n",
    "        matrices = []\n",
    "        for i, index in enumerate(indexes):\n",
    "            matrix = activations.values[:, index[0]:index[1]]\n",
    "            #with_std = True if order=='std' else False\n",
    "            #scaler = StandardScaler(with_mean=True, with_std=with_std)\n",
    "            #scaler.fit(matrix)\n",
    "            #matrix = scaler.transform(matrix)\n",
    "            if order is not None and order != 'std':\n",
    "                matrix = matrix / np.mean(la.norm(matrix, ord=order, axis=1))\n",
    "            matrices.append(matrix)\n",
    "        matrices = np.hstack(matrices)\n",
    "        new_data = pd.DataFrame(matrices, columns=activations.columns)\n",
    "        new_path = path + '_norm-' + str(order).replace('np.', '')\n",
    "        check_folder(new_path)\n",
    "        new_data.to_csv(os.path.join(new_path, name + '_run{}.csv'.format(run_index + 1)), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/english/text_english_run*.txt' # path to text input\n",
    "language = 'english'\n",
    "#template = '/Users/alexpsq/Code/Parietal/data/text_english_run*.txt' # path to text input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating iterator for each run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = sorted(glob.glob(template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:00<00:00, 529682.92it/s]\n",
      "100%|██████████| 135/135 [00:00<00:00, 784253.52it/s]\n",
      "100%|██████████| 176/176 [00:00<00:00, 967493.45it/s]\n",
      "100%|██████████| 173/173 [00:00<00:00, 1012014.77it/s]\n",
      "100%|██████████| 177/177 [00:00<00:00, 987223.15it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1047363.77it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 1100513.50it/s]\n",
      "100%|██████████| 145/145 [00:00<00:00, 716341.67it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 766302.67it/s]\n"
     ]
    }
   ],
   "source": [
    "iterator_list = [tokenize(path, language, train=False) for path in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from transformers import AutoTokenizer, RobertaTokenizer\n",
    "#\n",
    "#tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "#\n",
    "#for index in [0]:\n",
    "#    batches, indexes = utils.batchify_per_sentence_with_pre_and_post_context(\n",
    "#                iterator_list[index], \n",
    "#                1, \n",
    "#                1, \n",
    "#                0, \n",
    "#                'roberta-base', \n",
    "#                max_length=512)\n",
    "#\n",
    "#for index, batch in enumerate(batches):\n",
    "#    batch = '<s> ' + batch + ' </s>'\n",
    "#    tokenized_text = tokenizer.tokenize(batch, add_special_tokens=False)\n",
    "#    print(batch)\n",
    "#    beg = indexes[index][0] + 1 # because of the special token at the beginning\n",
    "#    end = indexes[index][1] + 1\n",
    "#    print(indexes[index][0], indexes[index][1])\n",
    "#    print(tokenized_text)\n",
    "#    print(tokenized_text[beg:end])\n",
    "#    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_roberta_models = ['roberta-base'] \n",
    "names = [\n",
    "         'roberta-base_pre-1_1_post-0',\n",
    "         'roberta-base_pre-2_1_post-0',\n",
    "         'roberta-base_pre-5_1_post-0',\n",
    "         'roberta-base_pre-7_1_post-0',\n",
    "         'roberta-base_pre-10_1_post-0',\n",
    "         'roberta-base_pre-15_1_post-0',\n",
    "         'roberta-base_pre-20_1_post-0'\n",
    "         ]\n",
    "config_paths = [None] * 8\n",
    "saving_path_folders = [\n",
    "    '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/{}/roberta_pre-1_1_post-0'.format(language),\n",
    "    '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/{}/roberta_pre-2_1_post-0'.format(language),\n",
    "    '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/{}/roberta_pre-5_1_post-0'.format(language),\n",
    "    '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/{}/roberta_pre-7_1_post-0'.format(language),\n",
    "    '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/{}/roberta_pre-10_1_post-0'.format(language),\n",
    "    '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/{}/roberta_pre-15_1_post-0'.format(language),\n",
    "    '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/{}/roberta_pre-20_1_post-0'.format(language)\n",
    "]\n",
    "prediction_types = ['control-context'] * 8\n",
    "number_of_sentence_list = [1] * 8\n",
    "number_of_sentence_before_list = [1, 2, 5, 7, 10, 15, 20] #\n",
    "number_of_sentence_after_list = [0] * 8\n",
    "\n",
    "\n",
    "attention_length_before_list = [2] * 34\n",
    "attention_length_after_list = [3] * 34\n",
    "stop_attention_at_sent_before_list = [None] * 34\n",
    "stop_attention_before_sent_list = [0] * 34\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta-base_pre-1_1_post-0  - Extracting activations ...\n",
      "############# Run 1 #############\n",
      "(13, 32, 768)\n",
      "(13, 13, 768)\n",
      "(13, 8, 768)\n",
      "(13, 21, 768)\n",
      "(13, 22, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:06, ?it/s]\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-9-82c11d23a320>\", line 20, in <module>\n",
      "    activations  = extractor.extract_activations(iterator, language)\n",
      "  File \"/Users/alexpsq/Code/Parietal/NLP_models/ROBERTA/model.py\", line 118, in extract_activations\n",
      "    activations = self.get_token_level_activations(iterator, language)\n",
      "  File \"/Users/alexpsq/Code/Parietal/NLP_models/ROBERTA/model.py\", line 254, in get_token_level_activations\n",
      "    encoded_layers = self.model(inputs_ids, attention_mask=attention_mask) # last_hidden_state, pooler_output, hidden_states, attentions\n",
      "  File \"/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/Users/alexpsq/Code/Parietal/NLP_models/ROBERTA/modeling_hacked_bert.py\", line 780, in forward\n",
      "    encoder_attention_mask=encoder_extended_attention_mask,\n",
      "  File \"/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/Users/alexpsq/Code/Parietal/NLP_models/ROBERTA/modeling_hacked_bert.py\", line 396, in forward\n",
      "    hidden_states, attention_mask, head_mask[i], encoder_hidden_states, encoder_attention_mask\n",
      "  File \"/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/Users/alexpsq/Code/Parietal/NLP_models/ROBERTA/modeling_hacked_bert.py\", line 369, in forward\n",
      "    layer_output = self.output(intermediate_output, attention_output)\n",
      "  File \"/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/Users/alexpsq/Code/Parietal/NLP_models/ROBERTA/modeling_hacked_bert.py\", line 333, in forward\n",
      "    hidden_states = self.dense(hidden_states)\n",
      "  File \"/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/nn/modules/linear.py\", line 93, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/nn/functional.py\", line 1692, in linear\n",
      "    output = input.matmul(weight.t())\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Applications/anaconda3/envs/parietal/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Applications/anaconda3/envs/parietal/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Applications/anaconda3/envs/parietal/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Applications/anaconda3/envs/parietal/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/Applications/anaconda3/envs/parietal/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/Applications/anaconda3/envs/parietal/lib/python3.7/posixpath.py\", line 428, in _joinrealpath\n",
      "    newpath = join(path, name)\n",
      "  File \"/Applications/anaconda3/envs/parietal/lib/python3.7/posixpath.py\", line 86, in join\n",
      "    for b in map(os.fspath, p):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "for index, roberta_model in enumerate(pretrained_roberta_models):\n",
    "    extractor = RobertaExtractor(roberta_model, \n",
    "                                 language, \n",
    "                                 names[index], \n",
    "                                 prediction_types[index], \n",
    "                                 output_hidden_states=True, \n",
    "                                 output_attentions=False, \n",
    "                                 config_path=config_paths[index],\n",
    "                                 max_length=512,\n",
    "                                 number_of_sentence=number_of_sentence_list[index], \n",
    "                                 number_of_sentence_before=number_of_sentence_before_list[index], \n",
    "                                 number_of_sentence_after=number_of_sentence_after_list[index],\n",
    "                                 attention_length_before=attention_length_before_list[index],\n",
    "                                 attention_length_after=attention_length_after_list[index],\n",
    "                                )\n",
    "    print(extractor.name, ' - Extracting activations ...')\n",
    "    for run_index, iterator in tqdm(enumerate(iterator_list)):\n",
    "        gc.collect()\n",
    "        print(\"############# Run {} #############\".format(run_index+1))\n",
    "        activations  = extractor.extract_activations(iterator, language)\n",
    "        hidden_states_activations = activations[0]\n",
    "        attention_heads_activations = activations[1]\n",
    "        #(cls_hidden_states_activations, cls_attention_activations) = activations[2]\n",
    "        #(sep_hidden_states_activations, sep_attention_activations) = activations[3]\n",
    "        #activations = pd.concat([hidden_states_activations, attention_heads_activations], axis=1)\n",
    "        #cls_activations = pd.concat([cls_hidden_states_activations, cls_attention_activations], axis=1)\n",
    "        #sep_activations = pd.concat([sep_hidden_states_activations, sep_attention_activations], axis=1)\n",
    "        \n",
    "        transform(\n",
    "            hidden_states_activations, \n",
    "            saving_path_folders[index], \n",
    "            'activations', \n",
    "            run_index=run_index,\n",
    "            n_layers_hidden=13,\n",
    "            n_layers_attention=0, \n",
    "            hidden_size=768)\n",
    "                \n",
    "        #transform(cls_activations, saving_path_folders[index], 'cls')\n",
    "        #transform(sep_activations, saving_path_folders[index], 'sep')\n",
    "        \n",
    "        #activations.to_csv(os.path.join(saving_path_folders[index], 'activations_run{}.csv'.format(run_index + 1)), index=False)\n",
    "        #cls_activations.to_csv(os.path.join(saving_path_folders[index], 'cls_run{}.csv'.format(run_index + 1)), index=False)\n",
    "        #sep_activations.to_csv(os.path.join(saving_path_folders[index], 'sep_run{}.csv'.format(run_index + 1)), index=False)\n",
    "        del activations\n",
    "        #del cls_activations\n",
    "        #del sep_activations\n",
    "        #del hidden_states_activations\n",
    "        #del attention_heads_activations\n",
    "        #del cls_hidden_states_activations\n",
    "        #del cls_attention_activations\n",
    "        #del sep_hidden_states_activations\n",
    "        #del sep_attention_activations\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "294912/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "24576.0/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2048.0/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
