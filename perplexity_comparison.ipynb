{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to evaluate model entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, GPT2Tokenizer,  GPT2LMHeadModel, GPT2Tokenizer, BertForMaskedLM, RobertaForMaskedLM, RobertaTokenizer\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT2.tokenizer import tokenize\n",
    "from LSTM.tokenizer import unk_transform\n",
    "#from LSTM.model import LSTMExtractor\n",
    "from LSTM.data import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT2 import utils as utils_gpt2\n",
    "from BERT import utils as utils_bert\n",
    "from ROBERTA import utils as utils_roberta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(pk):\n",
    "    pk = pk.numpy()\n",
    "    entropy = -np.sum(pk * np.log2(pk), axis=0)\n",
    "    return entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_output(out): \n",
    "    print(out[0].detach().squeeze(0).shape[0])\n",
    "    result = np.sum([entropy(scipy.special.softmax(out[0].detach().squeeze(0)[ax])) for ax in range(out[0].detach().squeeze(0).shape[0])]) \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model instanciation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "t_base = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_medium = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "t_medium = GPT2Tokenizer.from_pretrained('gpt2-medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bert = BertForMaskedLM.from_pretrained('bert-base-cased')\n",
    "t_bert = BertTokenizer.from_pretrained('bert-base-cased') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_roberta = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
    "t_roberta = RobertaTokenizer.from_pretrained('roberta-base') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = LSTMExtractor(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/stimuli-representations/english/LSTM_embedding-size_600_nhid_300_nlayers_1_dropout_02_wiki_kristina_english/activations_run1.csv')\n",
    "lstm_result = data['entropy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/english/text_english_run*.txt' # path to text input  \n",
    "#template = '/USers/alexpsq/Code/Parietal/data/text_english_run*.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = sorted(glob.glob(template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:00<00:00, 233112.82it/s]\n",
      "100%|██████████| 135/135 [00:00<00:00, 202950.19it/s]\n",
      "100%|██████████| 176/176 [00:00<00:00, 273103.04it/s]\n",
      "100%|██████████| 173/173 [00:00<00:00, 270348.21it/s]\n",
      "100%|██████████| 177/177 [00:00<00:00, 220033.14it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 252429.55it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 308242.81it/s]\n",
      "100%|██████████| 145/145 [00:00<00:00, 197139.09it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 75893.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iterator_list = [tokenize(path, language, train=False) for path in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015\n"
     ]
    }
   ],
   "source": [
    "res = ' '.join(iterator_list[0])\n",
    "res = res.split(' ')\n",
    "print(len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/english/lstm_training'\n",
    "#vocab_path = '/Users/alexpsq/Code/data/'\n",
    "vocab = Dictionary(vocab_path, language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator_list_lstm = [[unk_transform(word, vocab) for item in iterator_ for word in item.strip().split(' ')] for iterator_ in iterator_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchity(iterator, context_length, pretrained_bert, max_length=512):\n",
    "    \"\"\"Batchify iterator sentence, to get minimum context length \n",
    "    when possible.\n",
    "    Arguments:\n",
    "        - iterator: sentence iterator\n",
    "        - context_length: int\n",
    "    Returns:\n",
    "        - batch: sequence iterator\n",
    "        - indexes: tuple of int\n",
    "    \"\"\"\n",
    "    iterator = [item.strip() for item in iterator]\n",
    "    max_length -= 2 # for special tokens\n",
    "    tokenizer = BertTokenizer.from_pretrained(pretrained_bert)\n",
    "    \n",
    "    batch = []\n",
    "    indexes = []\n",
    "    sentence_count = 0\n",
    "    n = len(iterator)\n",
    "    \n",
    "    assert context_length < max_length\n",
    "    token_count = 0\n",
    "    while sentence_count < n and token_count < max_length:\n",
    "        token_count += len(tokenizer.wordpiece_tokenizer.tokenize(iterator[sentence_count]))\n",
    "        if token_count < max_length:\n",
    "            sentence_count += 1\n",
    "    batch.append(' '.join(iterator[:sentence_count]))\n",
    "    indexes.append((0, len(tokenizer.wordpiece_tokenizer.tokenize(batch[-1]))))\n",
    "    \n",
    "    while sentence_count < n:\n",
    "        token_count = 0\n",
    "        sentence_index = sentence_count - 1\n",
    "        tmp = sentence_count\n",
    "        while token_count < context_length:\n",
    "            token_count += len(tokenizer.wordpiece_tokenizer.tokenize(iterator[sentence_index]))\n",
    "            sentence_index -= 1\n",
    "        while sentence_count < n and token_count < max_length:\n",
    "            token_count += len(tokenizer.wordpiece_tokenizer.tokenize(iterator[sentence_count]))\n",
    "            if token_count < max_length:\n",
    "                sentence_count += 1\n",
    "        batch.append(' '.join(iterator[sentence_index+1:sentence_count]))\n",
    "        indexes.append((len(tokenizer.wordpiece_tokenizer.tokenize(' '.join(iterator[sentence_index+1:tmp]))), len(tokenizer.wordpiece_tokenizer.tokenize(batch[-1]))))\n",
    "    return batch, indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Straight ahead... ”'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_bert.decode(t_bert.convert_tokens_to_ids(t_bert.wordpiece_tokenizer.tokenize(' Straight ahead ... ”')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ĠStraight', 'Ġahead', 'Ġ...', 'ĠâĢ', 'Ŀ']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_base.tokenize(' Straight ahead ... ”')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []                                                                                                                                                                                        \n",
    "\n",
    "for line in iterator_list[0]:  \n",
    "    result.append(len([word for word in line.strip().split(' ')])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result:\n",
    "    out_lstm.append(np.sum(lstm_result[index:index+i]))\n",
    "    index+=i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_output(out):\n",
    "    result = np.sum([entropy(scipy.special.softmax(out[ax])) for ax in range(out.shape[0])]) \n",
    "    return result\n",
    "\n",
    "def entropy(pk):\n",
    "    pk = pk\n",
    "    entropy = -np.sum(pk * np.log2(pk), axis=0)\n",
    "    return entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1479.7416\n",
      "81.449234\n",
      "133.16344\n",
      "84.96507\n",
      "65.91492\n",
      "57.852043\n",
      "67.326385\n",
      "67.71767\n",
      "107.38642\n",
      "57.373596\n",
      "45.209236\n",
      "48.960426\n",
      "194.10556\n",
      "86.80297\n",
      "85.67359\n",
      "138.64224\n",
      "36.772038\n",
      "80.569664\n",
      "76.45224\n",
      "144.53545\n",
      "40.44752\n",
      "176.63167\n",
      "53.160664\n",
      "63.024975\n",
      "121.399635\n",
      "98.385025\n",
      "98.946335\n",
      "35.67877\n",
      "41.66576\n",
      "32.041798\n",
      "35.979294\n",
      "67.63603\n",
      "25.65775\n",
      "44.13664\n",
      "96.6409\n",
      "80.69574\n",
      "94.12051\n",
      "41.170517\n",
      "224.02808\n",
      "66.01508\n",
      "75.64002\n",
      "148.27258\n",
      "115.84273\n",
      "100.31509\n",
      "95.7048\n",
      "25.196476\n",
      "78.75001\n",
      "161.28549\n",
      "213.25603\n",
      "44.87957\n",
      "31.980438\n",
      "146.24057\n",
      "59.030235\n",
      "81.71396\n",
      "9.538782\n",
      "70.57346\n",
      "84.76148\n",
      "49.91638\n",
      "32.467304\n",
      "24.687328\n",
      "24.578554\n",
      "46.58152\n",
      "50.78563\n",
      "27.506622\n",
      "105.17947\n",
      "76.638115\n",
      "25.125643\n",
      "169.71748\n",
      "67.78848\n",
      "196.56427\n",
      "57.977867\n",
      "139.05779\n",
      "68.33978\n",
      "29.470459\n",
      "86.83928\n",
      "38.392086\n",
      "66.54461\n",
      "74.31881\n",
      "10.809681\n",
      "41.35556\n",
      "64.11928\n",
      "54.646378\n",
      "96.48307\n",
      "87.59054\n",
      "240.96822\n",
      "47.792496\n",
      "15.6032505\n",
      "30.054953\n",
      "44.417683\n",
      "78.10411\n",
      "35.53222\n",
      "33.70642\n",
      "47.416634\n",
      "19.873497\n",
      "19.407288\n",
      "16.716162\n",
      "91.17547\n",
      "61.873688\n",
      "83.09551\n",
      "30.125158\n",
      "124.356224\n",
      "33.502968\n",
      "145.25267\n",
      "68.33378\n",
      "108.18374\n",
      "130.36346\n",
      "85.13462\n",
      "71.269806\n",
      "42.735275\n",
      "188.25302\n",
      "44.176872\n",
      "127.904686\n",
      "36.090508\n",
      "81.62971\n",
      "41.48958\n",
      "120.00113\n",
      "115.36537\n",
      "22.997406\n",
      "27.166103\n",
      "134.57529\n",
      "110.64503\n"
     ]
    }
   ],
   "source": [
    "batches, indexes = utils_gpt2.batchify_per_sentence_with_context(iterator_list[0], 1, 15, 'gpt2', max_length=512)\n",
    "for index, batch in enumerate(batches):\n",
    "    batch = batch.strip()\n",
    "    tokenized_text = t_base.tokenize(batch, add_prefix_space=True)\n",
    "    inputs_ids = torch.tensor([t_base.convert_tokens_to_ids(tokenized_text)])\n",
    "    attention_mask = torch.tensor([[1 for x in tokenized_text]])\n",
    "    out_gpt2_base = model_base(inputs_ids, attention_mask=attention_mask)\n",
    "    mapping = utils_gpt2.match_tokenized_to_untokenized(tokenized_text, batch)\n",
    "\n",
    "    out = out_gpt2_base[0].detach().squeeze(0).numpy()\n",
    "    key = None\n",
    "    new_activations = []\n",
    "    for key_, value in mapping.items(): \n",
    "        if value[0] == indexes[index][0]:\n",
    "            key = key_\n",
    "    #print(indexes[index][0], mapping)\n",
    "    for word_index in range(key, len(mapping.keys())):\n",
    "        word_activation = []\n",
    "        word_activation.append([out[index, :] for index in mapping[word_index]])\n",
    "        word_activation = np.vstack(word_activation)\n",
    "        new_activations.append(np.mean(word_activation, axis=0).reshape(1,-1))\n",
    "    activations = np.vstack(new_activations)\n",
    "    print(eval_output(activations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.726067\n",
      "0.51056916\n",
      "15.8018265\n",
      "1.6382804\n",
      "0.89195096\n",
      "0.073676035\n",
      "2.8023162\n",
      "0.24247593\n",
      "4.106795\n",
      "1.2153734\n",
      "0.16375905\n",
      "3.4561903\n",
      "1.9005369\n",
      "0.33084705\n",
      "2.488223\n",
      "12.904313\n",
      "0.10350615\n",
      "1.3819168\n",
      "2.1053207\n",
      "6.3975186\n",
      "0.2907766\n",
      "7.2758303\n",
      "0.33432707\n",
      "0.9509853\n",
      "1.6948833\n",
      "8.109937\n",
      "2.5655208\n",
      "3.175221\n",
      "1.1034843\n",
      "1.9702392\n",
      "2.962284\n",
      "0.38524824\n",
      "0.048696566\n",
      "1.1148791\n",
      "0.7471783\n",
      "0.17330718\n",
      "0.9248373\n",
      "0.14171925\n",
      "14.549717\n",
      "6.0737476\n",
      "2.1092517\n",
      "7.7870703\n",
      "0.2677342\n",
      "4.1574235\n",
      "3.08417\n",
      "2.1414485\n",
      "7.6469812\n",
      "8.539354\n",
      "2.9270263\n",
      "2.006554\n",
      "0.90666777\n",
      "6.918694\n",
      "3.5429232\n",
      "7.5012136\n",
      "0.06419663\n",
      "5.3476486\n",
      "12.037139\n",
      "0.16684225\n",
      "0.13371281\n",
      "0.5327476\n",
      "0.03672949\n",
      "1.7643049\n",
      "0.17249109\n",
      "0.6117816\n",
      "8.716273\n",
      "0.18557273\n",
      "1.754605\n",
      "7.043815\n",
      "0.748481\n",
      "15.958378\n",
      "0.90590805\n",
      "5.558101\n",
      "1.0060807\n",
      "1.8724229\n",
      "4.915218\n",
      "3.2075346\n",
      "0.7280161\n",
      "3.3455396\n",
      "0.3391056\n",
      "2.4124892\n",
      "0.25327185\n",
      "0.08924365\n",
      "1.2096369\n",
      "0.16785105\n",
      "2.4065912\n",
      "1.503155\n",
      "0.10804603\n",
      "0.07097508\n",
      "0.6218133\n",
      "0.04384697\n",
      "1.8683732\n",
      "0.9556374\n",
      "5.203377\n",
      "1.3145803\n",
      "0.050640065\n",
      "1.9549698\n",
      "2.306864\n",
      "2.104768\n",
      "2.25039\n",
      "0.9141239\n",
      "2.322032\n",
      "0.06143702\n",
      "5.561799\n",
      "4.0099826\n",
      "17.779102\n",
      "5.549099\n",
      "1.7219323\n",
      "10.094501\n",
      "1.1150745\n",
      "3.756314\n",
      "1.3768283\n",
      "1.3725815\n",
      "1.0219004\n",
      "8.20953\n",
      "1.0782617\n",
      "2.0648017\n",
      "5.5912924\n",
      "5.817927\n",
      "2.2757297\n",
      "6.086371\n",
      "4.0977817\n"
     ]
    }
   ],
   "source": [
    "batches, indexes = utils_bert.batchify_per_sentence_with_context(iterator_list[0], 1, 15, 'bert-base-cased', max_length=512)\n",
    "for index, batch in enumerate(batches):\n",
    "    batch = '[CLS] ' + batch.strip() + ' [SEP]'\n",
    "    tokenized_text = t_bert.wordpiece_tokenizer.tokenize(batch)\n",
    "    inputs_ids = torch.tensor([t_bert.convert_tokens_to_ids(tokenized_text)])\n",
    "    attention_mask = torch.tensor([[1 for x in tokenized_text]])\n",
    "    out_bert = model_bert(inputs_ids, attention_mask=attention_mask)\n",
    "    mapping = utils_bert.match_tokenized_to_untokenized(tokenized_text, batch)\n",
    "\n",
    "    out = out_bert[0].detach().squeeze(0).numpy()\n",
    "    new_activations = []\n",
    "    \n",
    "    key_start = None\n",
    "    key_stop = None\n",
    "    for key_, value in mapping.items(): \n",
    "        if (value[0] - 1) == (indexes[index][0]): #because we added [CLS] token at the beginning\n",
    "            key_start = key_\n",
    "    for key_, value in mapping.items(): \n",
    "        if value[-1] == (indexes[index][1]): #because we added [CLS] token at the beginning\n",
    "            key_stop = key_\n",
    "    for word_index in range(key_start, key_stop + 1): # len(mapping.keys()) - 1\n",
    "        word_activation = []\n",
    "        word_activation.append([out[index, :] for index in mapping[word_index]])\n",
    "        word_activation = np.vstack(word_activation)\n",
    "        new_activations.append(np.mean(word_activation, axis=0).reshape(1,-1))\n",
    "    activations = np.vstack(new_activations)\n",
    "    print(eval_output(activations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Once , when I was six years old , I saw a magnificent picture in a book about the primeval forest called ‘ Real - life Stories . ’']\n",
      "0 0 135\n",
      "--\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a792588f8dfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils_roberta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchify_per_sentence_with_pre_and_post_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'roberta-base'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'<s> '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' </s>'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_roberta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_prefix_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minputs_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt_roberta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/models/ROBERTA/utils.py\u001b[0m in \u001b[0;36mbatchify_per_sentence_with_pre_and_post_context\u001b[0;34m(iterator, number_of_sentence, number_sentence_before, number_sentence_after, pretrained_roberta, max_length)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnumber_sentence_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnumber_sentence_before\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnumber_sentence_before\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_prefix_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_prefix_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mindexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnumber_sentence_before\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_prefix_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_prefix_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/parietal/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    757\u001b[0m         \"\"\"\n\u001b[1;32m    758\u001b[0m         \u001b[0mall_special_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_for_tokenization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mlowercase_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/parietal/lib/python3.7/site-packages/transformers/tokenization_roberta.py\u001b[0m in \u001b[0;36mprepare_for_tokenization\u001b[0;34m(self, text, add_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0madd_prefix_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0madd_prefix_space\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "batches, indexes = utils_roberta.batchify_per_sentence_with_pre_and_post_context(iterator_list[0], 1, 0, 0, 'roberta-base', max_length=512)\n",
    "for index, batch in enumerate(batches):\n",
    "    batch = '<s> ' + batch.strip() + ' </s>'\n",
    "    tokenized_text = t_roberta.tokenize(batch, add_prefix_space=True)\n",
    "    inputs_ids = torch.tensor([t_roberta.convert_tokens_to_ids(tokenized_text)])\n",
    "    attention_mask = torch.tensor([[1 for x in tokenized_text]])\n",
    "    out_roberta = model_roberta(inputs_ids, attention_mask=attention_mask)\n",
    "    mapping = utils_roberta.match_tokenized_to_untokenized(tokenized_text, batch)\n",
    "\n",
    "    out = out_roberta[0].detach().squeeze(0).numpy()\n",
    "    new_activations = []\n",
    "    key_start = None\n",
    "    key_stop = None\n",
    "    for key_, value in mapping.items(): \n",
    "        if (value[0] - 1) == (indexes[index][0]): #because we added [CLS] token at the beginning\n",
    "            key_start = key_\n",
    "    for key_, value in mapping.items(): \n",
    "        if value[-1] == (indexes[index][1]): #because we added [CLS] token at the beginning\n",
    "            key_stop = key_\n",
    "    for word_index in range(key_start, key_stop + 1): # len(mapping.keys()) - 1\n",
    "        word_activation = []\n",
    "        word_activation.append([out[index, :] for index in mapping[word_index]])\n",
    "        word_activation = np.vstack(word_activation)\n",
    "        new_activations.append(np.mean(word_activation, axis=0).reshape(1,-1))\n",
    "    activations = np.vstack(new_activations)\n",
    "    print(eval_output(activations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-daf0996f4c62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_roberta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_prefix_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/parietal/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    757\u001b[0m         \"\"\"\n\u001b[1;32m    758\u001b[0m         \u001b[0mall_special_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_for_tokenization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mlowercase_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/parietal/lib/python3.7/site-packages/transformers/tokenization_roberta.py\u001b[0m in \u001b[0;36mprepare_for_tokenization\u001b[0;34m(self, text, add_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0madd_prefix_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0madd_prefix_space\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "len(t_roberta.tokenize('', add_prefix_space=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Once , when I was six years old , I saw a magnificent picture in a book about the primeval forest called ‘ Real - life Stories . ’ </s>\n",
      "['<s>', 'ĠOnce', 'Ġ,', 'Ġwhen', 'ĠI', 'Ġwas', 'Ġsix', 'Ġyears', 'Ġold', 'Ġ,', 'ĠI', 'Ġsaw', 'Ġa', 'Ġmagnificent', 'Ġpicture', 'Ġin', 'Ġa', 'Ġbook', 'Ġabout', 'Ġthe', 'Ġprime', 'val', 'Ġforest', 'Ġcalled', 'ĠâĢ', 'ĺ', 'ĠReal', 'Ġ-', 'Ġlife', 'ĠStories', 'Ġ.', 'ĠâĢ', 'Ļ', '</s>']\n",
      "<s> It showed a boa constrictor swallowing a wild animal . </s>\n",
      "['<s>', 'ĠIt', 'Ġshowed', 'Ġa', 'Ġbo', 'a', 'Ġconst', 'rict', 'or', 'Ġswallowing', 'Ġa', 'Ġwild', 'Ġanimal', 'Ġ.', '</s>']\n",
      "<s> Here is a copy of the drawing . </s>\n",
      "['<s>', 'ĠHere', 'Ġis', 'Ġa', 'Ġcopy', 'Ġof', 'Ġthe', 'Ġdrawing', 'Ġ.', '</s>']\n",
      "<s> It said in the book : “ Boa constrictors swallow their prey whole , without chewing . </s>\n",
      "['<s>', 'ĠIt', 'Ġsaid', 'Ġin', 'Ġthe', 'Ġbook', 'Ġ:', 'ĠâĢ', 'ľ', 'ĠBo', 'a', 'Ġconst', 'rict', 'ors', 'Ġswallow', 'Ġtheir', 'Ġprey', 'Ġwhole', 'Ġ,', 'Ġwithout', 'Ġchewing', 'Ġ.', '</s>']\n",
      "<s> Then they are not able to move , and they sleep for the six months it takes for digestion . ” </s>\n",
      "['<s>', 'ĠThen', 'Ġthey', 'Ġare', 'Ġnot', 'Ġable', 'Ġto', 'Ġmove', 'Ġ,', 'Ġand', 'Ġthey', 'Ġsleep', 'Ġfor', 'Ġthe', 'Ġsix', 'Ġmonths', 'Ġit', 'Ġtakes', 'Ġfor', 'Ġdigestion', 'Ġ.', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> So I thought a lot about the adventures of the jungle and , in turn , I managed , with a coloured pencil , to make my first drawing . </s>\n",
      "['<s>', 'ĠSo', 'ĠI', 'Ġthought', 'Ġa', 'Ġlot', 'Ġabout', 'Ġthe', 'Ġadventures', 'Ġof', 'Ġthe', 'Ġjungle', 'Ġand', 'Ġ,', 'Ġin', 'Ġturn', 'Ġ,', 'ĠI', 'Ġmanaged', 'Ġ,', 'Ġwith', 'Ġa', 'Ġcoloured', 'Ġpencil', 'Ġ,', 'Ġto', 'Ġmake', 'Ġmy', 'Ġfirst', 'Ġdrawing', 'Ġ.', '</s>']\n",
      "<s> My Drawing Number one . </s>\n",
      "['<s>', 'ĠMy', 'ĠDrawing', 'ĠNumber', 'Ġone', 'Ġ.', '</s>']\n",
      "<s> It looked like this : I showed my masterpiece to the grownups and I asked them if my drawing frightened them . </s>\n",
      "['<s>', 'ĠIt', 'Ġlooked', 'Ġlike', 'Ġthis', 'Ġ:', 'ĠI', 'Ġshowed', 'Ġmy', 'Ġmasterpiece', 'Ġto', 'Ġthe', 'Ġgrown', 'ups', 'Ġand', 'ĠI', 'Ġasked', 'Ġthem', 'Ġif', 'Ġmy', 'Ġdrawing', 'Ġfrightened', 'Ġthem', 'Ġ.', '</s>']\n",
      "<s> They answered me : “ Why would anyone be frightened by a hat ? ” </s>\n",
      "['<s>', 'ĠThey', 'Ġanswered', 'Ġme', 'Ġ:', 'ĠâĢ', 'ľ', 'ĠWhy', 'Ġwould', 'Ġanyone', 'Ġbe', 'Ġfrightened', 'Ġby', 'Ġa', 'Ġhat', 'Ġ?', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> My drawing was not of a hat . </s>\n",
      "['<s>', 'ĠMy', 'Ġdrawing', 'Ġwas', 'Ġnot', 'Ġof', 'Ġa', 'Ġhat', 'Ġ.', '</s>']\n",
      "<s> It showed a boa constrictor digesting an elephant . </s>\n",
      "['<s>', 'ĠIt', 'Ġshowed', 'Ġa', 'Ġbo', 'a', 'Ġconst', 'rict', 'or', 'Ġdigest', 'ing', 'Ġan', 'Ġelephant', 'Ġ.', '</s>']\n",
      "<s> I then drew the inside of the boa constrictor , so that the grownups could understand . </s>\n",
      "['<s>', 'ĠI', 'Ġthen', 'Ġdrew', 'Ġthe', 'Ġinside', 'Ġof', 'Ġthe', 'Ġbo', 'a', 'Ġconst', 'rict', 'or', 'Ġ,', 'Ġso', 'Ġthat', 'Ġthe', 'Ġgrown', 'ups', 'Ġcould', 'Ġunderstand', 'Ġ.', '</s>']\n",
      "<s> They always need to have things explained . </s>\n",
      "['<s>', 'ĠThey', 'Ġalways', 'Ġneed', 'Ġto', 'Ġhave', 'Ġthings', 'Ġexplained', 'Ġ.', '</s>']\n",
      "<s> My Drawing Number two looked like this : The grownups advised me to leave aside drawings of boa constrictors , open or closed , and to apply myself instead to geography , history , arithmetic and grammar . </s>\n",
      "['<s>', 'ĠMy', 'ĠDrawing', 'ĠNumber', 'Ġtwo', 'Ġlooked', 'Ġlike', 'Ġthis', 'Ġ:', 'ĠThe', 'Ġgrown', 'ups', 'Ġadvised', 'Ġme', 'Ġto', 'Ġleave', 'Ġaside', 'Ġdrawings', 'Ġof', 'Ġbo', 'a', 'Ġconst', 'rict', 'ors', 'Ġ,', 'Ġopen', 'Ġor', 'Ġclosed', 'Ġ,', 'Ġand', 'Ġto', 'Ġapply', 'Ġmyself', 'Ġinstead', 'Ġto', 'Ġgeography', 'Ġ,', 'Ġhistory', 'Ġ,', 'Ġarithmetic', 'Ġand', 'Ġgrammar', 'Ġ.', '</s>']\n",
      "<s> Thus I abandoned , at the age of six , a magnificent career as a painter . </s>\n",
      "['<s>', 'ĠThus', 'ĠI', 'Ġabandoned', 'Ġ,', 'Ġat', 'Ġthe', 'Ġage', 'Ġof', 'Ġsix', 'Ġ,', 'Ġa', 'Ġmagnificent', 'Ġcareer', 'Ġas', 'Ġa', 'Ġpainter', 'Ġ.', '</s>']\n",
      "<s> I was discouraged by the failure of my Drawing Number one and of my Drawing Number two . </s>\n",
      "['<s>', 'ĠI', 'Ġwas', 'Ġdiscouraged', 'Ġby', 'Ġthe', 'Ġfailure', 'Ġof', 'Ġmy', 'ĠDrawing', 'ĠNumber', 'Ġone', 'Ġand', 'Ġof', 'Ġmy', 'ĠDrawing', 'ĠNumber', 'Ġtwo', 'Ġ.', '</s>']\n",
      "<s> Grownups never understand anything by themselves , and it ’ s tiresome for children to always explain things for them again and again . </s>\n",
      "['<s>', 'ĠG', 'rown', 'ups', 'Ġnever', 'Ġunderstand', 'Ġanything', 'Ġby', 'Ġthemselves', 'Ġ,', 'Ġand', 'Ġit', 'ĠâĢ', 'Ļ', 'Ġs', 'Ġtires', 'ome', 'Ġfor', 'Ġchildren', 'Ġto', 'Ġalways', 'Ġexplain', 'Ġthings', 'Ġfor', 'Ġthem', 'Ġagain', 'Ġand', 'Ġagain', 'Ġ.', '</s>']\n",
      "<s> So I had to choose another profession , and I learned to fly airplanes . </s>\n",
      "['<s>', 'ĠSo', 'ĠI', 'Ġhad', 'Ġto', 'Ġchoose', 'Ġanother', 'Ġprofession', 'Ġ,', 'Ġand', 'ĠI', 'Ġlearned', 'Ġto', 'Ġfly', 'Ġairplanes', 'Ġ.', '</s>']\n",
      "<s> I flew a little in many places around the world . </s>\n",
      "['<s>', 'ĠI', 'Ġflew', 'Ġa', 'Ġlittle', 'Ġin', 'Ġmany', 'Ġplaces', 'Ġaround', 'Ġthe', 'Ġworld', 'Ġ.', '</s>']\n",
      "<s> And geography , it ' s true , has served me well . </s>\n",
      "['<s>', 'ĠAnd', 'Ġgeography', 'Ġ,', 'Ġit', \"Ġ'\", 'Ġs', 'Ġtrue', 'Ġ,', 'Ġhas', 'Ġserved', 'Ġme', 'Ġwell', 'Ġ.', '</s>']\n",
      "<s> I could recognize , at first glance , China from Arizona . </s>\n",
      "['<s>', 'ĠI', 'Ġcould', 'Ġrecognize', 'Ġ,', 'Ġat', 'Ġfirst', 'Ġglance', 'Ġ,', 'ĠChina', 'Ġfrom', 'ĠArizona', 'Ġ.', '</s>']\n",
      "<s> It ’ s very useful if you get lost at night . </s>\n",
      "['<s>', 'ĠIt', 'ĠâĢ', 'Ļ', 'Ġs', 'Ġvery', 'Ġuseful', 'Ġif', 'Ġyou', 'Ġget', 'Ġlost', 'Ġat', 'Ġnight', 'Ġ.', '</s>']\n",
      "<s> I have had , during my life , a lot of contact with many persons of consequence . </s>\n",
      "['<s>', 'ĠI', 'Ġhave', 'Ġhad', 'Ġ,', 'Ġduring', 'Ġmy', 'Ġlife', 'Ġ,', 'Ġa', 'Ġlot', 'Ġof', 'Ġcontact', 'Ġwith', 'Ġmany', 'Ġpersons', 'Ġof', 'Ġconsequence', 'Ġ.', '</s>']\n",
      "<s> I have lived a lot amongst the grownups . </s>\n",
      "['<s>', 'ĠI', 'Ġhave', 'Ġlived', 'Ġa', 'Ġlot', 'Ġamongst', 'Ġthe', 'Ġgrown', 'ups', 'Ġ.', '</s>']\n",
      "<s> I have seen them from close up . </s>\n",
      "['<s>', 'ĠI', 'Ġhave', 'Ġseen', 'Ġthem', 'Ġfrom', 'Ġclose', 'Ġup', 'Ġ.', '</s>']\n",
      "<s> It hasnt much improved my opinion of them . </s>\n",
      "['<s>', 'ĠIt', 'Ġhas', 'nt', 'Ġmuch', 'Ġimproved', 'Ġmy', 'Ġopinion', 'Ġof', 'Ġthem', 'Ġ.', '</s>']\n",
      "<s> Whenever I met one of them that seemed a bit more clear - sighted , I tried the experiment of showing them my Drawing Number one , that I ' ve always kept . </s>\n",
      "['<s>', 'ĠWhenever', 'ĠI', 'Ġmet', 'Ġone', 'Ġof', 'Ġthem', 'Ġthat', 'Ġseemed', 'Ġa', 'Ġbit', 'Ġmore', 'Ġclear', 'Ġ-', 'Ġsight', 'ed', 'Ġ,', 'ĠI', 'Ġtried', 'Ġthe', 'Ġexperiment', 'Ġof', 'Ġshowing', 'Ġthem', 'Ġmy', 'ĠDrawing', 'ĠNumber', 'Ġone', 'Ġ,', 'Ġthat', 'ĠI', \"Ġ'\", 'Ġve', 'Ġalways', 'Ġkept', 'Ġ.', '</s>']\n",
      "<s> I wanted to know if they were really a person of true understanding . </s>\n",
      "['<s>', 'ĠI', 'Ġwanted', 'Ġto', 'Ġknow', 'Ġif', 'Ġthey', 'Ġwere', 'Ġreally', 'Ġa', 'Ġperson', 'Ġof', 'Ġtrue', 'Ġunderstanding', 'Ġ.', '</s>']\n",
      "<s> But they always responded : “ It ' s a hat . ” </s>\n",
      "['<s>', 'ĠBut', 'Ġthey', 'Ġalways', 'Ġresponded', 'Ġ:', 'ĠâĢ', 'ľ', 'ĠIt', \"Ġ'\", 'Ġs', 'Ġa', 'Ġhat', 'Ġ.', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> So I would never speak to them of boa constrictors , nor of primeval forests , nor of the stars . </s>\n",
      "['<s>', 'ĠSo', 'ĠI', 'Ġwould', 'Ġnever', 'Ġspeak', 'Ġto', 'Ġthem', 'Ġof', 'Ġbo', 'a', 'Ġconst', 'rict', 'ors', 'Ġ,', 'Ġnor', 'Ġof', 'Ġprime', 'val', 'Ġforests', 'Ġ,', 'Ġnor', 'Ġof', 'Ġthe', 'Ġstars', 'Ġ.', '</s>']\n",
      "<s> I put myself at their level . </s>\n",
      "['<s>', 'ĠI', 'Ġput', 'Ġmyself', 'Ġat', 'Ġtheir', 'Ġlevel', 'Ġ.', '</s>']\n",
      "<s> I talked to them about bridge , golf , politics and neckties . </s>\n",
      "['<s>', 'ĠI', 'Ġtalked', 'Ġto', 'Ġthem', 'Ġabout', 'Ġbridge', 'Ġ,', 'Ġgolf', 'Ġ,', 'Ġpolitics', 'Ġand', 'Ġneck', 'ties', 'Ġ.', '</s>']\n",
      "<s> And the grownup was glad to know such a sensible man . </s>\n",
      "['<s>', 'ĠAnd', 'Ġthe', 'Ġgrown', 'up', 'Ġwas', 'Ġglad', 'Ġto', 'Ġknow', 'Ġsuch', 'Ġa', 'Ġsensible', 'Ġman', 'Ġ.', '</s>']\n",
      "<s> So I lived alone , without anyone I could really talk to , until a breakdown in the Sahara desert , six years ago . </s>\n",
      "['<s>', 'ĠSo', 'ĠI', 'Ġlived', 'Ġalone', 'Ġ,', 'Ġwithout', 'Ġanyone', 'ĠI', 'Ġcould', 'Ġreally', 'Ġtalk', 'Ġto', 'Ġ,', 'Ġuntil', 'Ġa', 'Ġbreakdown', 'Ġin', 'Ġthe', 'ĠSahara', 'Ġdesert', 'Ġ,', 'Ġsix', 'Ġyears', 'Ġago', 'Ġ.', '</s>']\n",
      "<s> Something had broken in my engine . </s>\n",
      "['<s>', 'ĠSomething', 'Ġhad', 'Ġbroken', 'Ġin', 'Ġmy', 'Ġengine', 'Ġ.', '</s>']\n",
      "<s> And as I had with me neither a mechanic nor any passengers , I readied myself to try and carry out , all alone , the difficult repairs . </s>\n",
      "['<s>', 'ĠAnd', 'Ġas', 'ĠI', 'Ġhad', 'Ġwith', 'Ġme', 'Ġneither', 'Ġa', 'Ġmechanic', 'Ġnor', 'Ġany', 'Ġpassengers', 'Ġ,', 'ĠI', 'Ġread', 'ied', 'Ġmyself', 'Ġto', 'Ġtry', 'Ġand', 'Ġcarry', 'Ġout', 'Ġ,', 'Ġall', 'Ġalone', 'Ġ,', 'Ġthe', 'Ġdifficult', 'Ġrepairs', 'Ġ.', '</s>']\n",
      "<s> For me it was a matter of life or death . </s>\n",
      "['<s>', 'ĠFor', 'Ġme', 'Ġit', 'Ġwas', 'Ġa', 'Ġmatter', 'Ġof', 'Ġlife', 'Ġor', 'Ġdeath', 'Ġ.', '</s>']\n",
      "<s> I had hardly enough water to drink for a week . </s>\n",
      "['<s>', 'ĠI', 'Ġhad', 'Ġhardly', 'Ġenough', 'Ġwater', 'Ġto', 'Ġdrink', 'Ġfor', 'Ġa', 'Ġweek', 'Ġ.', '</s>']\n",
      "<s> The first night I went to sleep on the sand , a thousand miles from any human habitation . </s>\n",
      "['<s>', 'ĠThe', 'Ġfirst', 'Ġnight', 'ĠI', 'Ġwent', 'Ġto', 'Ġsleep', 'Ġon', 'Ġthe', 'Ġsand', 'Ġ,', 'Ġa', 'Ġthousand', 'Ġmiles', 'Ġfrom', 'Ġany', 'Ġhuman', 'Ġhabit', 'ation', 'Ġ.', '</s>']\n",
      "<s> I was more isolated than a shipwrecked sailor on a raft in the middle of the ocean . </s>\n",
      "['<s>', 'ĠI', 'Ġwas', 'Ġmore', 'Ġisolated', 'Ġthan', 'Ġa', 'Ġship', 'wreck', 'ed', 'Ġsailor', 'Ġon', 'Ġa', 'Ġraft', 'Ġin', 'Ġthe', 'Ġmiddle', 'Ġof', 'Ġthe', 'Ġocean', 'Ġ.', '</s>']\n",
      "<s> So you can imagine my surprise when at daybreak , a funny little voice woke me up . </s>\n",
      "['<s>', 'ĠSo', 'Ġyou', 'Ġcan', 'Ġimagine', 'Ġmy', 'Ġsurprise', 'Ġwhen', 'Ġat', 'Ġday', 'break', 'Ġ,', 'Ġa', 'Ġfunny', 'Ġlittle', 'Ġvoice', 'Ġwoke', 'Ġme', 'Ġup', 'Ġ.', '</s>']\n",
      "<s> It said : “ Please ... </s>\n",
      "['<s>', 'ĠIt', 'Ġsaid', 'Ġ:', 'ĠâĢ', 'ľ', 'ĠPlease', 'Ġ...', '</s>']\n",
      "<s> draw me a sheep ! ” </s>\n",
      "['<s>', 'Ġdraw', 'Ġme', 'Ġa', 'Ġsheep', 'Ġ!', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> “ What ? ” </s>\n",
      "['<s>', 'ĠâĢ', 'ľ', 'ĠWhat', 'Ġ?', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> “ Draw me a sheep ! ” </s>\n",
      "['<s>', 'ĠâĢ', 'ľ', 'ĠDraw', 'Ġme', 'Ġa', 'Ġsheep', 'Ġ!', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> I jumped to my feet as if I ’ d been struck by lightning . </s>\n",
      "['<s>', 'ĠI', 'Ġjumped', 'Ġto', 'Ġmy', 'Ġfeet', 'Ġas', 'Ġif', 'ĠI', 'ĠâĢ', 'Ļ', 'Ġd', 'Ġbeen', 'Ġstruck', 'Ġby', 'Ġlightning', 'Ġ.', '</s>']\n",
      "<s> I rubbed my eyes . </s>\n",
      "['<s>', 'ĠI', 'Ġrubbed', 'Ġmy', 'Ġeyes', 'Ġ.', '</s>']\n",
      "<s> I took a good look around me . </s>\n",
      "['<s>', 'ĠI', 'Ġtook', 'Ġa', 'Ġgood', 'Ġlook', 'Ġaround', 'Ġme', 'Ġ.', '</s>']\n",
      "<s> And I saw a quite extraordinary little man , who was examining me seriously . </s>\n",
      "['<s>', 'ĠAnd', 'ĠI', 'Ġsaw', 'Ġa', 'Ġquite', 'Ġextraordinary', 'Ġlittle', 'Ġman', 'Ġ,', 'Ġwho', 'Ġwas', 'Ġexamining', 'Ġme', 'Ġseriously', 'Ġ.', '</s>']\n",
      "<s> Here is the best portrait that , later , I managed to do of him . </s>\n",
      "['<s>', 'ĠHere', 'Ġis', 'Ġthe', 'Ġbest', 'Ġportrait', 'Ġthat', 'Ġ,', 'Ġlater', 'Ġ,', 'ĠI', 'Ġmanaged', 'Ġto', 'Ġdo', 'Ġof', 'Ġhim', 'Ġ.', '</s>']\n",
      "<s> But my drawing , of course , is much less charming than its model . </s>\n",
      "['<s>', 'ĠBut', 'Ġmy', 'Ġdrawing', 'Ġ,', 'Ġof', 'Ġcourse', 'Ġ,', 'Ġis', 'Ġmuch', 'Ġless', 'Ġcharming', 'Ġthan', 'Ġits', 'Ġmodel', 'Ġ.', '</s>']\n",
      "<s> It ' s not my fault . </s>\n",
      "['<s>', 'ĠIt', \"Ġ'\", 'Ġs', 'Ġnot', 'Ġmy', 'Ġfault', 'Ġ.', '</s>']\n",
      "<s> I was discouraged in my career as a painter by the grownups , at the age of six , and I hadn ' t learned to draw anything except boa constrictors , closed and open . </s>\n",
      "['<s>', 'ĠI', 'Ġwas', 'Ġdiscouraged', 'Ġin', 'Ġmy', 'Ġcareer', 'Ġas', 'Ġa', 'Ġpainter', 'Ġby', 'Ġthe', 'Ġgrown', 'ups', 'Ġ,', 'Ġat', 'Ġthe', 'Ġage', 'Ġof', 'Ġsix', 'Ġ,', 'Ġand', 'ĠI', 'Ġhadn', \"Ġ'\", 'Ġt', 'Ġlearned', 'Ġto', 'Ġdraw', 'Ġanything', 'Ġexcept', 'Ġbo', 'a', 'Ġconst', 'rict', 'ors', 'Ġ,', 'Ġclosed', 'Ġand', 'Ġopen', 'Ġ.', '</s>']\n",
      "<s> I stared at this sudden apparition wide eyed with astonishment . </s>\n",
      "['<s>', 'ĠI', 'Ġstared', 'Ġat', 'Ġthis', 'Ġsudden', 'Ġappar', 'ition', 'Ġwide', 'Ġeyed', 'Ġwith', 'Ġaston', 'ishment', 'Ġ.', '</s>']\n",
      "<s> Remember that I was a thousand miles from any inhabited region . </s>\n",
      "['<s>', 'ĠRemember', 'Ġthat', 'ĠI', 'Ġwas', 'Ġa', 'Ġthousand', 'Ġmiles', 'Ġfrom', 'Ġany', 'Ġinhabited', 'Ġregion', 'Ġ.', '</s>']\n",
      "<s> And yet this little fellow seemed neither lost , nor half - dead with fatigue , nor starved or dying of thirst or fear . </s>\n",
      "['<s>', 'ĠAnd', 'Ġyet', 'Ġthis', 'Ġlittle', 'Ġfellow', 'Ġseemed', 'Ġneither', 'Ġlost', 'Ġ,', 'Ġnor', 'Ġhalf', 'Ġ-', 'Ġdead', 'Ġwith', 'Ġfatigue', 'Ġ,', 'Ġnor', 'Ġstarved', 'Ġor', 'Ġdying', 'Ġof', 'Ġthirst', 'Ġor', 'Ġfear', 'Ġ.', '</s>']\n",
      "<s> He looked nothing like a child lost in the middle of the desert , a thousand miles from any inhabited region . </s>\n",
      "['<s>', 'ĠHe', 'Ġlooked', 'Ġnothing', 'Ġlike', 'Ġa', 'Ġchild', 'Ġlost', 'Ġin', 'Ġthe', 'Ġmiddle', 'Ġof', 'Ġthe', 'Ġdesert', 'Ġ,', 'Ġa', 'Ġthousand', 'Ġmiles', 'Ġfrom', 'Ġany', 'Ġinhabited', 'Ġregion', 'Ġ.', '</s>']\n",
      "<s> When I finally managed to speak , I said : “ But — what are you doing here ? ” </s>\n",
      "['<s>', 'ĠWhen', 'ĠI', 'Ġfinally', 'Ġmanaged', 'Ġto', 'Ġspeak', 'Ġ,', 'ĠI', 'Ġsaid', 'Ġ:', 'ĠâĢ', 'ľ', 'ĠBut', 'ĠâĢĶ', 'Ġwhat', 'Ġare', 'Ġyou', 'Ġdoing', 'Ġhere', 'Ġ?', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> And he repeated , very slowly , as if it was something very serious : “ Please ... </s>\n",
      "['<s>', 'ĠAnd', 'Ġhe', 'Ġrepeated', 'Ġ,', 'Ġvery', 'Ġslowly', 'Ġ,', 'Ġas', 'Ġif', 'Ġit', 'Ġwas', 'Ġsomething', 'Ġvery', 'Ġserious', 'Ġ:', 'ĠâĢ', 'ľ', 'ĠPlease', 'Ġ...', '</s>']\n",
      "<s> draw me a sheep ... ” </s>\n",
      "['<s>', 'Ġdraw', 'Ġme', 'Ġa', 'Ġsheep', 'Ġ...', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> When a mystery is too overpowering , one dare not disobey . </s>\n",
      "['<s>', 'ĠWhen', 'Ġa', 'Ġmystery', 'Ġis', 'Ġtoo', 'Ġoverpower', 'ing', 'Ġ,', 'Ġone', 'Ġdare', 'Ġnot', 'Ġdisob', 'ey', 'Ġ.', '</s>']\n",
      "<s> Absurd as it seemed to me a thousand miles from any human habitation and in danger of death , I took out of my pocket a sheet of paper and a pen . </s>\n",
      "['<s>', 'ĠAbs', 'urd', 'Ġas', 'Ġit', 'Ġseemed', 'Ġto', 'Ġme', 'Ġa', 'Ġthousand', 'Ġmiles', 'Ġfrom', 'Ġany', 'Ġhuman', 'Ġhabit', 'ation', 'Ġand', 'Ġin', 'Ġdanger', 'Ġof', 'Ġdeath', 'Ġ,', 'ĠI', 'Ġtook', 'Ġout', 'Ġof', 'Ġmy', 'Ġpocket', 'Ġa', 'Ġsheet', 'Ġof', 'Ġpaper', 'Ġand', 'Ġa', 'Ġpen', 'Ġ.', '</s>']\n",
      "<s> But then I remembered that I had mainly studied geography , history , arithmetic and grammar , and I told the little fellow ( a little crossly ) that I didn ’ t know how to draw . </s>\n",
      "['<s>', 'ĠBut', 'Ġthen', 'ĠI', 'Ġremembered', 'Ġthat', 'ĠI', 'Ġhad', 'Ġmainly', 'Ġstudied', 'Ġgeography', 'Ġ,', 'Ġhistory', 'Ġ,', 'Ġarithmetic', 'Ġand', 'Ġgrammar', 'Ġ,', 'Ġand', 'ĠI', 'Ġtold', 'Ġthe', 'Ġlittle', 'Ġfellow', 'Ġ(', 'Ġa', 'Ġlittle', 'Ġcross', 'ly', 'Ġ)', 'Ġthat', 'ĠI', 'Ġdidn', 'ĠâĢ', 'Ļ', 'Ġt', 'Ġknow', 'Ġhow', 'Ġto', 'Ġdraw', 'Ġ.', '</s>']\n",
      "<s> He replied : “ It doesn ' t matter . </s>\n",
      "['<s>', 'ĠHe', 'Ġreplied', 'Ġ:', 'ĠâĢ', 'ľ', 'ĠIt', 'Ġdoesn', \"Ġ'\", 'Ġt', 'Ġmatter', 'Ġ.', '</s>']\n",
      "<s> Draw me a sheep . ” </s>\n",
      "['<s>', 'ĠDraw', 'Ġme', 'Ġa', 'Ġsheep', 'Ġ.', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> As I ’ d never drawn a sheep , I redrew for him one of the only two drawings that I was capable of . </s>\n",
      "['<s>', 'ĠAs', 'ĠI', 'ĠâĢ', 'Ļ', 'Ġd', 'Ġnever', 'Ġdrawn', 'Ġa', 'Ġsheep', 'Ġ,', 'ĠI', 'Ġred', 'rew', 'Ġfor', 'Ġhim', 'Ġone', 'Ġof', 'Ġthe', 'Ġonly', 'Ġtwo', 'Ġdrawings', 'Ġthat', 'ĠI', 'Ġwas', 'Ġcapable', 'Ġof', 'Ġ.', '</s>']\n",
      "<s> The one of the closed boa constrictor . </s>\n",
      "['<s>', 'ĠThe', 'Ġone', 'Ġof', 'Ġthe', 'Ġclosed', 'Ġbo', 'a', 'Ġconst', 'rict', 'or', 'Ġ.', '</s>']\n",
      "<s> And I was astounded to hear the little fellow respond : “ No ! </s>\n",
      "['<s>', 'ĠAnd', 'ĠI', 'Ġwas', 'Ġast', 'ounded', 'Ġto', 'Ġhear', 'Ġthe', 'Ġlittle', 'Ġfellow', 'Ġrespond', 'Ġ:', 'ĠâĢ', 'ľ', 'ĠNo', 'Ġ!', '</s>']\n",
      "<s> No ! </s>\n",
      "['<s>', 'ĠNo', 'Ġ!', '</s>']\n",
      "<s> I don ’ t want an elephant inside a boa constrictor . </s>\n",
      "['<s>', 'ĠI', 'Ġdon', 'ĠâĢ', 'Ļ', 'Ġt', 'Ġwant', 'Ġan', 'Ġelephant', 'Ġinside', 'Ġa', 'Ġbo', 'a', 'Ġconst', 'rict', 'or', 'Ġ.', '</s>']\n",
      "<s> A boa constrictor is very dangerous , and an elephant is very cumbersome . </s>\n",
      "['<s>', 'ĠA', 'Ġbo', 'a', 'Ġconst', 'rict', 'or', 'Ġis', 'Ġvery', 'Ġdangerous', 'Ġ,', 'Ġand', 'Ġan', 'Ġelephant', 'Ġis', 'Ġvery', 'Ġcumbersome', 'Ġ.', '</s>']\n",
      "<s> Where I live everything is very small . </s>\n",
      "['<s>', 'ĠWhere', 'ĠI', 'Ġlive', 'Ġeverything', 'Ġis', 'Ġvery', 'Ġsmall', 'Ġ.', '</s>']\n",
      "<s> I need a sheep . </s>\n",
      "['<s>', 'ĠI', 'Ġneed', 'Ġa', 'Ġsheep', 'Ġ.', '</s>']\n",
      "<s> Draw me a sheep . ” </s>\n",
      "['<s>', 'ĠDraw', 'Ġme', 'Ġa', 'Ġsheep', 'Ġ.', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> So I drew . </s>\n",
      "['<s>', 'ĠSo', 'ĠI', 'Ġdrew', 'Ġ.', '</s>']\n",
      "<s> He looked carefully , then said : “ No ! </s>\n",
      "['<s>', 'ĠHe', 'Ġlooked', 'Ġcarefully', 'Ġ,', 'Ġthen', 'Ġsaid', 'Ġ:', 'ĠâĢ', 'ľ', 'ĠNo', 'Ġ!', '</s>']\n",
      "<s> This one ’ s already very sick . </s>\n",
      "['<s>', 'ĠThis', 'Ġone', 'ĠâĢ', 'Ļ', 'Ġs', 'Ġalready', 'Ġvery', 'Ġsick', 'Ġ.', '</s>']\n",
      "<s> Make another one . ” </s>\n",
      "['<s>', 'ĠMake', 'Ġanother', 'Ġone', 'Ġ.', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> I drew again : My friend smiled gently and indulgently : “ You can see yourself ... </s>\n",
      "['<s>', 'ĠI', 'Ġdrew', 'Ġagain', 'Ġ:', 'ĠMy', 'Ġfriend', 'Ġsmiled', 'Ġgently', 'Ġand', 'Ġindul', 'gently', 'Ġ:', 'ĠâĢ', 'ľ', 'ĠYou', 'Ġcan', 'Ġsee', 'Ġyourself', 'Ġ...', '</s>']\n",
      "<s> this isn ’ t a sheep , it ' s a ram . </s>\n",
      "['<s>', 'Ġthis', 'Ġisn', 'ĠâĢ', 'Ļ', 'Ġt', 'Ġa', 'Ġsheep', 'Ġ,', 'Ġit', \"Ġ'\", 'Ġs', 'Ġa', 'Ġram', 'Ġ.', '</s>']\n",
      "<s> It has horns ... </s>\n",
      "['<s>', 'ĠIt', 'Ġhas', 'Ġhorns', 'Ġ...', '</s>']\n",
      "<s> “ So once again I redid my drawing : But it was rejected , like the previous ones : “ This one ’ s too old . </s>\n",
      "['<s>', 'ĠâĢ', 'ľ', 'ĠSo', 'Ġonce', 'Ġagain', 'ĠI', 'Ġred', 'id', 'Ġmy', 'Ġdrawing', 'Ġ:', 'ĠBut', 'Ġit', 'Ġwas', 'Ġrejected', 'Ġ,', 'Ġlike', 'Ġthe', 'Ġprevious', 'Ġones', 'Ġ:', 'ĠâĢ', 'ľ', 'ĠThis', 'Ġone', 'ĠâĢ', 'Ļ', 'Ġs', 'Ġtoo', 'Ġold', 'Ġ.', '</s>']\n",
      "<s> I want a sheep that will live a long time . ” </s>\n",
      "['<s>', 'ĠI', 'Ġwant', 'Ġa', 'Ġsheep', 'Ġthat', 'Ġwill', 'Ġlive', 'Ġa', 'Ġlong', 'Ġtime', 'Ġ.', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> So , getting impatient , as I was eager to start dismantling my engine , I hastily sketched this drawing : And I snapped : “ This here is the box . </s>\n",
      "['<s>', 'ĠSo', 'Ġ,', 'Ġgetting', 'Ġimpatient', 'Ġ,', 'Ġas', 'ĠI', 'Ġwas', 'Ġeager', 'Ġto', 'Ġstart', 'Ġdismantling', 'Ġmy', 'Ġengine', 'Ġ,', 'ĠI', 'Ġhastily', 'Ġsket', 'ched', 'Ġthis', 'Ġdrawing', 'Ġ:', 'ĠAnd', 'ĠI', 'Ġsnapped', 'Ġ:', 'ĠâĢ', 'ľ', 'ĠThis', 'Ġhere', 'Ġis', 'Ġthe', 'Ġbox', 'Ġ.', '</s>']\n",
      "<s> The sheep you want is inside . ” </s>\n",
      "['<s>', 'ĠThe', 'Ġsheep', 'Ġyou', 'Ġwant', 'Ġis', 'Ġinside', 'Ġ.', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> But I was very surprised to see the face of my young judge light up : “ It ' s exactly the way I wanted ! </s>\n",
      "['<s>', 'ĠBut', 'ĠI', 'Ġwas', 'Ġvery', 'Ġsurprised', 'Ġto', 'Ġsee', 'Ġthe', 'Ġface', 'Ġof', 'Ġmy', 'Ġyoung', 'Ġjudge', 'Ġlight', 'Ġup', 'Ġ:', 'ĠâĢ', 'ľ', 'ĠIt', \"Ġ'\", 'Ġs', 'Ġexactly', 'Ġthe', 'Ġway', 'ĠI', 'Ġwanted', 'Ġ!', '</s>']\n",
      "<s> Do you think this sheep needs a lot of grass ? ” </s>\n",
      "['<s>', 'ĠDo', 'Ġyou', 'Ġthink', 'Ġthis', 'Ġsheep', 'Ġneeds', 'Ġa', 'Ġlot', 'Ġof', 'Ġgrass', 'Ġ?', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> “ Why ? ” </s>\n",
      "['<s>', 'ĠâĢ', 'ľ', 'ĠWhy', 'Ġ?', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> “ Because where I ' m from everything is very small ... ” </s>\n",
      "['<s>', 'ĠâĢ', 'ľ', 'ĠBecause', 'Ġwhere', 'ĠI', \"Ġ'\", 'Ġm', 'Ġfrom', 'Ġeverything', 'Ġis', 'Ġvery', 'Ġsmall', 'Ġ...', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> “ There will certainly be enough . </s>\n",
      "['<s>', 'ĠâĢ', 'ľ', 'ĠThere', 'Ġwill', 'Ġcertainly', 'Ġbe', 'Ġenough', 'Ġ.', '</s>']\n",
      "<s> I gave you a very small sheep . ” </s>\n",
      "['<s>', 'ĠI', 'Ġgave', 'Ġyou', 'Ġa', 'Ġvery', 'Ġsmall', 'Ġsheep', 'Ġ.', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> He leaned his head towards the drawing : “ Not that small ... </s>\n",
      "['<s>', 'ĠHe', 'Ġleaned', 'Ġhis', 'Ġhead', 'Ġtowards', 'Ġthe', 'Ġdrawing', 'Ġ:', 'ĠâĢ', 'ľ', 'ĠNot', 'Ġthat', 'Ġsmall', 'Ġ...', '</s>']\n",
      "<s> Look ! </s>\n",
      "['<s>', 'ĠLook', 'Ġ!', '</s>']\n",
      "<s> He ' s fallen asleep ... ” </s>\n",
      "['<s>', 'ĠHe', \"Ġ'\", 'Ġs', 'Ġfallen', 'Ġasleep', 'Ġ...', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> And that ' s how I met the little prince . </s>\n",
      "['<s>', 'ĠAnd', 'Ġthat', \"Ġ'\", 'Ġs', 'Ġhow', 'ĠI', 'Ġmet', 'Ġthe', 'Ġlittle', 'Ġprince', 'Ġ.', '</s>']\n",
      "<s> It took me a long time to find out where he came from . </s>\n",
      "['<s>', 'ĠIt', 'Ġtook', 'Ġme', 'Ġa', 'Ġlong', 'Ġtime', 'Ġto', 'Ġfind', 'Ġout', 'Ġwhere', 'Ġhe', 'Ġcame', 'Ġfrom', 'Ġ.', '</s>']\n",
      "<s> The little prince , who asked me many questions , never seemed to hear my own . </s>\n",
      "['<s>', 'ĠThe', 'Ġlittle', 'Ġprince', 'Ġ,', 'Ġwho', 'Ġasked', 'Ġme', 'Ġmany', 'Ġquestions', 'Ġ,', 'Ġnever', 'Ġseemed', 'Ġto', 'Ġhear', 'Ġmy', 'Ġown', 'Ġ.', '</s>']\n",
      "<s> It was the words spoken by chance that , little by little , revealed everything to me . </s>\n",
      "['<s>', 'ĠIt', 'Ġwas', 'Ġthe', 'Ġwords', 'Ġspoken', 'Ġby', 'Ġchance', 'Ġthat', 'Ġ,', 'Ġlittle', 'Ġby', 'Ġlittle', 'Ġ,', 'Ġrevealed', 'Ġeverything', 'Ġto', 'Ġme', 'Ġ.', '</s>']\n",
      "<s> So , when he saw my airplane for the first time ( I won ’ t draw my airplane , it would be a drawing far too complicated for me ) , he asked me : “ What ' s that thing there ? ” </s>\n",
      "['<s>', 'ĠSo', 'Ġ,', 'Ġwhen', 'Ġhe', 'Ġsaw', 'Ġmy', 'Ġairplane', 'Ġfor', 'Ġthe', 'Ġfirst', 'Ġtime', 'Ġ(', 'ĠI', 'Ġwon', 'ĠâĢ', 'Ļ', 'Ġt', 'Ġdraw', 'Ġmy', 'Ġairplane', 'Ġ,', 'Ġit', 'Ġwould', 'Ġbe', 'Ġa', 'Ġdrawing', 'Ġfar', 'Ġtoo', 'Ġcomplicated', 'Ġfor', 'Ġme', 'Ġ)', 'Ġ,', 'Ġhe', 'Ġasked', 'Ġme', 'Ġ:', 'ĠâĢ', 'ľ', 'ĠWhat', \"Ġ'\", 'Ġs', 'Ġthat', 'Ġthing', 'Ġthere', 'Ġ?', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> “ It ' s not a thing . </s>\n",
      "['<s>', 'ĠâĢ', 'ľ', 'ĠIt', \"Ġ'\", 'Ġs', 'Ġnot', 'Ġa', 'Ġthing', 'Ġ.', '</s>']\n",
      "<s> It flies . </s>\n",
      "['<s>', 'ĠIt', 'Ġflies', 'Ġ.', '</s>']\n",
      "<s> It ' s an airplane . </s>\n",
      "['<s>', 'ĠIt', \"Ġ'\", 'Ġs', 'Ġan', 'Ġairplane', 'Ġ.', '</s>']\n",
      "<s> It ’ s my airplane . ” </s>\n",
      "['<s>', 'ĠIt', 'ĠâĢ', 'Ļ', 'Ġs', 'Ġmy', 'Ġairplane', 'Ġ.', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> And I was proud to have him know that I could fly . </s>\n",
      "['<s>', 'ĠAnd', 'ĠI', 'Ġwas', 'Ġproud', 'Ġto', 'Ġhave', 'Ġhim', 'Ġknow', 'Ġthat', 'ĠI', 'Ġcould', 'Ġfly', 'Ġ.', '</s>']\n",
      "<s> Then he cried : “ What ? </s>\n",
      "['<s>', 'ĠThen', 'Ġhe', 'Ġcried', 'Ġ:', 'ĠâĢ', 'ľ', 'ĠWhat', 'Ġ?', '</s>']\n",
      "<s> You fell from the sky ! ” </s>\n",
      "['<s>', 'ĠYou', 'Ġfell', 'Ġfrom', 'Ġthe', 'Ġsky', 'Ġ!', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> “ Yes , ” I said modestly . </s>\n",
      "['<s>', 'ĠâĢ', 'ľ', 'ĠYes', 'Ġ,', 'ĠâĢ', 'Ŀ', 'ĠI', 'Ġsaid', 'Ġmodest', 'ly', 'Ġ.', '</s>']\n",
      "<s> “ Oh ! </s>\n",
      "['<s>', 'ĠâĢ', 'ľ', 'ĠOh', 'Ġ!', '</s>']\n",
      "<s> That ' s funny ! </s>\n",
      "['<s>', 'ĠThat', \"Ġ'\", 'Ġs', 'Ġfunny', 'Ġ!', '</s>']\n",
      "<s> ... ” </s>\n",
      "['<s>', 'Ġ...', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> And the little prince broke into a lovely peal of laughter , which irritated me very much . </s>\n",
      "['<s>', 'ĠAnd', 'Ġthe', 'Ġlittle', 'Ġprince', 'Ġbroke', 'Ġinto', 'Ġa', 'Ġlovely', 'Ġpe', 'al', 'Ġof', 'Ġlaughter', 'Ġ,', 'Ġwhich', 'Ġirritated', 'Ġme', 'Ġvery', 'Ġmuch', 'Ġ.', '</s>']\n",
      "<s> I prefer people to take my misfortunes seriously . </s>\n",
      "['<s>', 'ĠI', 'Ġprefer', 'Ġpeople', 'Ġto', 'Ġtake', 'Ġmy', 'Ġmis', 'f', 'ortun', 'es', 'Ġseriously', 'Ġ.', '</s>']\n",
      "<s> Then he added : “ So , you also come from the sky ! </s>\n",
      "['<s>', 'ĠThen', 'Ġhe', 'Ġadded', 'Ġ:', 'ĠâĢ', 'ľ', 'ĠSo', 'Ġ,', 'Ġyou', 'Ġalso', 'Ġcome', 'Ġfrom', 'Ġthe', 'Ġsky', 'Ġ!', '</s>']\n",
      "<s> What planet are you from ? ” </s>\n",
      "['<s>', 'ĠWhat', 'Ġplanet', 'Ġare', 'Ġyou', 'Ġfrom', 'Ġ?', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> I caught a glimpse into the mystery of his presence , and I asked abruptly : “ So you come from another planet then ? ” </s>\n",
      "['<s>', 'ĠI', 'Ġcaught', 'Ġa', 'Ġglimpse', 'Ġinto', 'Ġthe', 'Ġmystery', 'Ġof', 'Ġhis', 'Ġpresence', 'Ġ,', 'Ġand', 'ĠI', 'Ġasked', 'Ġabruptly', 'Ġ:', 'ĠâĢ', 'ľ', 'ĠSo', 'Ġyou', 'Ġcome', 'Ġfrom', 'Ġanother', 'Ġplanet', 'Ġthen', 'Ġ?', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> But he didn ’ t answer . </s>\n",
      "['<s>', 'ĠBut', 'Ġhe', 'Ġdidn', 'ĠâĢ', 'Ļ', 'Ġt', 'Ġanswer', 'Ġ.', '</s>']\n",
      "<s> He shook his head slowly whilst looking at my airplane : “ It ' s true that you can ' t have come from far away in that thing ... ” </s>\n",
      "['<s>', 'ĠHe', 'Ġshook', 'Ġhis', 'Ġhead', 'Ġslowly', 'Ġwhilst', 'Ġlooking', 'Ġat', 'Ġmy', 'Ġairplane', 'Ġ:', 'ĠâĢ', 'ľ', 'ĠIt', \"Ġ'\", 'Ġs', 'Ġtrue', 'Ġthat', 'Ġyou', 'Ġcan', \"Ġ'\", 'Ġt', 'Ġhave', 'Ġcome', 'Ġfrom', 'Ġfar', 'Ġaway', 'Ġin', 'Ġthat', 'Ġthing', 'Ġ...', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> And he drifted into a daydream which lasted a long while . </s>\n",
      "['<s>', 'ĠAnd', 'Ġhe', 'Ġdrifted', 'Ġinto', 'Ġa', 'Ġday', 'dream', 'Ġwhich', 'Ġlasted', 'Ġa', 'Ġlong', 'Ġwhile', 'Ġ.', '</s>']\n",
      "<s> Then , taking my sheep out of his pocket , he sank himself into the contemplation of his treasure . </s>\n",
      "['<s>', 'ĠThen', 'Ġ,', 'Ġtaking', 'Ġmy', 'Ġsheep', 'Ġout', 'Ġof', 'Ġhis', 'Ġpocket', 'Ġ,', 'Ġhe', 'Ġsank', 'Ġhimself', 'Ġinto', 'Ġthe', 'Ġcontemplation', 'Ġof', 'Ġhis', 'Ġtreasure', 'Ġ.', '</s>']\n",
      "<s> You can imagine how my curiosity was aroused by this small disclosure about ‘ the other planets . ’ </s>\n",
      "['<s>', 'ĠYou', 'Ġcan', 'Ġimagine', 'Ġhow', 'Ġmy', 'Ġcuriosity', 'Ġwas', 'Ġaroused', 'Ġby', 'Ġthis', 'Ġsmall', 'Ġdisclosure', 'Ġabout', 'ĠâĢ', 'ĺ', 'Ġthe', 'Ġother', 'Ġplanets', 'Ġ.', 'ĠâĢ', 'Ļ', '</s>']\n",
      "<s> So I tried to find out more : “ Where are you from my little fellow ? </s>\n",
      "['<s>', 'ĠSo', 'ĠI', 'Ġtried', 'Ġto', 'Ġfind', 'Ġout', 'Ġmore', 'Ġ:', 'ĠâĢ', 'ľ', 'ĠWhere', 'Ġare', 'Ġyou', 'Ġfrom', 'Ġmy', 'Ġlittle', 'Ġfellow', 'Ġ?', '</s>']\n",
      "<s> Where ’ s this ‘ where I live ’ of yours ? </s>\n",
      "['<s>', 'ĠWhere', 'ĠâĢ', 'Ļ', 'Ġs', 'Ġthis', 'ĠâĢ', 'ĺ', 'Ġwhere', 'ĠI', 'Ġlive', 'ĠâĢ', 'Ļ', 'Ġof', 'Ġyours', 'Ġ?', '</s>']\n",
      "<s> Where do you take my sheep off to ? ” </s>\n",
      "['<s>', 'ĠWhere', 'Ġdo', 'Ġyou', 'Ġtake', 'Ġmy', 'Ġsheep', 'Ġoff', 'Ġto', 'Ġ?', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> After a reflective silence he answered : “ What ' s good about the box you ’ ve given me is that at night , he can use it as a house . ” </s>\n",
      "['<s>', 'ĠAfter', 'Ġa', 'Ġreflective', 'Ġsilence', 'Ġhe', 'Ġanswered', 'Ġ:', 'ĠâĢ', 'ľ', 'ĠWhat', \"Ġ'\", 'Ġs', 'Ġgood', 'Ġabout', 'Ġthe', 'Ġbox', 'Ġyou', 'ĠâĢ', 'Ļ', 'Ġve', 'Ġgiven', 'Ġme', 'Ġis', 'Ġthat', 'Ġat', 'Ġnight', 'Ġ,', 'Ġhe', 'Ġcan', 'Ġuse', 'Ġit', 'Ġas', 'Ġa', 'Ġhouse', 'Ġ.', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> “ That ’ s right . </s>\n",
      "['<s>', 'ĠâĢ', 'ľ', 'ĠThat', 'ĠâĢ', 'Ļ', 'Ġs', 'Ġright', 'Ġ.', '</s>']\n",
      "<s> And if you ’ re good , I ' ll give you a rope to tie him up with during the day . </s>\n",
      "['<s>', 'ĠAnd', 'Ġif', 'Ġyou', 'ĠâĢ', 'Ļ', 'Ġre', 'Ġgood', 'Ġ,', 'ĠI', \"Ġ'\", 'Ġll', 'Ġgive', 'Ġyou', 'Ġa', 'Ġrope', 'Ġto', 'Ġtie', 'Ġhim', 'Ġup', 'Ġwith', 'Ġduring', 'Ġthe', 'Ġday', 'Ġ.', '</s>']\n",
      "<s> And a stake . ” </s>\n",
      "['<s>', 'ĠAnd', 'Ġa', 'Ġstake', 'Ġ.', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> The offer seemed to shock the little prince : “ Tie him up ? </s>\n",
      "['<s>', 'ĠThe', 'Ġoffer', 'Ġseemed', 'Ġto', 'Ġshock', 'Ġthe', 'Ġlittle', 'Ġprince', 'Ġ:', 'ĠâĢ', 'ľ', 'ĠTie', 'Ġhim', 'Ġup', 'Ġ?', '</s>']\n",
      "<s> What a funny idea ! ” </s>\n",
      "['<s>', 'ĠWhat', 'Ġa', 'Ġfunny', 'Ġidea', 'Ġ!', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> “ But if you don ' t tie him up , he ’ ll wander off , and get lost . ” </s>\n",
      "['<s>', 'ĠâĢ', 'ľ', 'ĠBut', 'Ġif', 'Ġyou', 'Ġdon', \"Ġ'\", 'Ġt', 'Ġtie', 'Ġhim', 'Ġup', 'Ġ,', 'Ġhe', 'ĠâĢ', 'Ļ', 'Ġll', 'Ġwander', 'Ġoff', 'Ġ,', 'Ġand', 'Ġget', 'Ġlost', 'Ġ.', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> My friend broke into another peal of laughter : “ Where do you think he ’ d go ! ” </s>\n",
      "['<s>', 'ĠMy', 'Ġfriend', 'Ġbroke', 'Ġinto', 'Ġanother', 'Ġpe', 'al', 'Ġof', 'Ġlaughter', 'Ġ:', 'ĠâĢ', 'ľ', 'ĠWhere', 'Ġdo', 'Ġyou', 'Ġthink', 'Ġhe', 'ĠâĢ', 'Ļ', 'Ġd', 'Ġgo', 'Ġ!', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> “ Anywhere . </s>\n",
      "['<s>', 'ĠâĢ', 'ľ', 'ĠAny', 'where', 'Ġ.', '</s>']\n",
      "<s> Straight ahead ... ” </s>\n",
      "['<s>', 'ĠStraight', 'Ġahead', 'Ġ...', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> Then the little prince said gravely : “ That doesn ’ t matter ; where I live , everything is so small ! ” </s>\n",
      "['<s>', 'ĠThen', 'Ġthe', 'Ġlittle', 'Ġprince', 'Ġsaid', 'Ġgrave', 'ly', 'Ġ:', 'ĠâĢ', 'ľ', 'ĠThat', 'Ġdoesn', 'ĠâĢ', 'Ļ', 'Ġt', 'Ġmatter', 'Ġ;', 'Ġwhere', 'ĠI', 'Ġlive', 'Ġ,', 'Ġeverything', 'Ġis', 'Ġso', 'Ġsmall', 'Ġ!', 'ĠâĢ', 'Ŀ', '</s>']\n",
      "<s> And perhaps with a hint of sadness , he added : “ Straight ahead you can ' t go far ... ” </s>\n",
      "['<s>', 'ĠAnd', 'Ġperhaps', 'Ġwith', 'Ġa', 'Ġhint', 'Ġof', 'Ġsadness', 'Ġ,', 'Ġhe', 'Ġadded', 'Ġ:', 'ĠâĢ', 'ľ', 'ĠStraight', 'Ġahead', 'Ġyou', 'Ġcan', \"Ġ'\", 'Ġt', 'Ġgo', 'Ġfar', 'Ġ...', 'ĠâĢ', 'Ŀ', '</s>']\n"
     ]
    }
   ],
   "source": [
    "for line in iterator_list[0]:\n",
    "    line = '<s> ' + line.strip() + ' </s>'\n",
    "    print(line)\n",
    "    print(t_roberta.tokenize(line, add_prefix_space=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 50257])\n",
      "32\n",
      "torch.Size([1, 32, 50257])\n",
      "32\n",
      "torch.Size([1, 32, 28996])\n",
      "32\n",
      "torch.Size([1, 34, 50265])\n",
      "34\n",
      "torch.Size([1, 13, 50257])\n",
      "13\n",
      "torch.Size([1, 13, 50257])\n",
      "13\n",
      "torch.Size([1, 16, 28996])\n",
      "16\n",
      "torch.Size([1, 15, 50265])\n",
      "15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e7b6f8ebdb00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0minputs_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt_medium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mout_gpt2_medium\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_medium\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# BERT-base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/parietal/lib/python3.7/site-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels)\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m         )\n\u001b[1;32m    601\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/parietal/lib/python3.7/site-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             outputs = block(\n\u001b[0;32m--> 484\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m             )\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/parietal/lib/python3.7/site-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, layer_past, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         output_attn = self.attn(\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         )\n\u001b[1;32m    228\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_attn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# output_attn: a, present, (attentions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/parietal/lib/python3.7/site-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, layer_past, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mpresent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# transpose to have same shapes for stacking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mattn_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/parietal/lib/python3.7/site-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36m_attn\u001b[0;34m(self, q, k, v, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "35.969994\n",
    "0.15918078\n",
    "8.293572\n",
    "0.16216268\n",
    "0.4701601\n",
    "0.09935355\n",
    "4.9531755\n",
    "0.41039103\n",
    "0.78353435\n",
    "1.0025072\n",
    "0.027972687\n",
    "1.6763369\n",
    "1.3630848\n",
    "0.07719013\n",
    "7.683632"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(results, columns=['GPT2-base', 'GPT2-medium', 'BERT-base-cased', 'ROBERTA-base'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2-base          13650.756184\n",
       "GPT2-medium        12541.113115\n",
       "BERT-base-cased     2469.644808\n",
       "ROBERTA-base         466.423326\n",
       "dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
