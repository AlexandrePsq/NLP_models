{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to extract hidden-states and attention heads activations from ALBERT model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'modeling_hacked_bert'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9df126a719ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlbertExtractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/Parietal/NLP_models/ALBERT/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodeling_hacked_bert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlbertModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'modeling_hacked_bert'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from model import AlbertExtractor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tokenizer import tokenize\n",
    "from utils import set_seed\n",
    "from numpy import linalg as la\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_folder(path):\n",
    "    \"\"\"Create adequate folders if necessary.\"\"\"\n",
    "    try:\n",
    "        if not os.path.isdir(path):\n",
    "            check_folder(os.path.dirname(path))\n",
    "            os.mkdir(path)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(activations, path, name, run_index, n_layers_hidden=13, n_layers_attention=12, hidden_size=768):\n",
    "    assert activations.values.shape[1] == (n_layers_hidden + n_layers_attention) * hidden_size\n",
    "    indexes = [[index*hidden_size, (index+1)*hidden_size] for index in range(n_layers_hidden + n_layers_attention)]\n",
    "    for order in [None]:\n",
    "        matrices = []\n",
    "        for index in indexes:\n",
    "            matrix = activations.values[:, index[0]:index[1]]\n",
    "            with_std = True if order=='std' else False\n",
    "            scaler = StandardScaler(with_mean=True, with_std=with_std)\n",
    "            scaler.fit(matrix)\n",
    "            matrix = scaler.transform(matrix)\n",
    "            if order is not None and order != 'std':\n",
    "                matrix = matrix / np.mean(la.norm(matrix, ord=order, axis=1))\n",
    "            matrices.append(matrix)\n",
    "        matrices = np.hstack(matrices)\n",
    "        new_data = pd.DataFrame(matrices, columns=activations.columns)\n",
    "        new_path = path + '_norm-' + str(order).replace('np.', '')\n",
    "        check_folder(new_path)\n",
    "        new_data.to_csv(os.path.join(new_path, name + '_run{}.csv'.format(run_index + 1)), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/english/text_english_run*.txt' # path to text input\n",
    "language = 'english'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating iterator for each run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#template = '/Users/alexpsq/Code/Parietal/data/text_english_run*.txt' # path to text input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = sorted(glob.glob(template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:00<00:00, 181484.31it/s]\n",
      "100%|██████████| 135/135 [00:00<00:00, 181484.31it/s]\n",
      "100%|██████████| 176/176 [00:00<00:00, 225610.48it/s]\n",
      "100%|██████████| 173/173 [00:00<00:00, 247312.40it/s]\n",
      "100%|██████████| 177/177 [00:00<00:00, 146486.15it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 282850.35it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 246059.14it/s]\n",
      "100%|██████████| 145/145 [00:00<00:00, 265230.74it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 266162.15it/s]\n"
     ]
    }
   ],
   "source": [
    "iterator_list = [tokenize(path, language, train=False) for path in paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_albert_models = ['albert-base-v1'] * 8\n",
    "names = [\n",
    "    'albert-base-v1_pre-0_1_post-0',\n",
    "    'albert-base-v1_pre-1_1_post-0',\n",
    "    'albert-base-v1_pre-2_1_post-0',\n",
    "    'albert-base-v1_pre-5_1_post-0',\n",
    "    'albert-base-v1_pre-7_1_post-0',\n",
    "    'albert-base-v1_pre-10_1_post-0',\n",
    "    'albert-base-v1_pre-15_1_post-0',\n",
    "    'albert-base-v1_pre-20_1_post-0'\n",
    "         ]\n",
    "config_paths = [None] * 8\n",
    "saving_path_folders = [\n",
    "    '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/{}/albert-base-v1_pre-0_1_post-0'.format(language),\n",
    "    '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/{}/albert-base-v1_pre-1_1_post-0'.format(language),\n",
    "    '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/{}/albert-base-v1_pre-2_1_post-0'.format(language),\n",
    "    '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/{}/albert-base-v1_pre-5_1_post-0'.format(language),\n",
    "    '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/{}/albert-base-v1_pre-7_1_post-0'.format(language),\n",
    "    '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/{}/albert-base-v1_pre-10_1_post-0'.format(language),\n",
    "    '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/{}/albert-base-v1_pre-15_1_post-0'.format(language),\n",
    "    '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/{}/albert-base-v1_pre-20_1_post-0'.format(language)\n",
    "]\n",
    "prediction_types = ['sentence'] * 8\n",
    "number_of_sentence_list = [1] * 8\n",
    "number_of_sentence_before_list = [0, 1, 2, 5, 7, 10, 15, 20] # \n",
    "number_of_sentence_after_list = [0] * 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-cased_pre-15_1_post-0  - Extracting activations ...\n",
      "############# Run 0 #############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [05:01, 301.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Run 1 #############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [10:30, 309.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Run 2 #############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [16:47, 329.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Run 3 #############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [22:50, 339.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Run 4 #############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [28:09, 333.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Run 5 #############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [34:25, 346.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Run 6 #############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [40:47, 357.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Run 7 #############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [46:05, 345.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Run 8 #############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [53:16, 355.17s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-cased_pre-20_1_post-0  - Extracting activations ...\n",
      "############# Run 0 #############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [05:01, 301.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Run 1 #############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [10:32, 310.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Run 2 #############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [16:29, 324.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Run 3 #############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [22:10, 329.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Run 4 #############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [27:36, 328.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Run 5 #############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [34:04, 346.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Run 6 #############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [40:36, 359.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Run 7 #############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [45:59, 348.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Run 8 #############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [53:37, 357.53s/it]\n"
     ]
    }
   ],
   "source": [
    "for index, albert_model in enumerate(pretrained_albert_models):\n",
    "    extractor = AlbertExtractor(albert_model, \n",
    "                              language, \n",
    "                              names[index], \n",
    "                              prediction_types[index], \n",
    "                              output_hidden_states=True, \n",
    "                              output_attentions=False, \n",
    "                              config_path=config_paths[index], \n",
    "                              max_length=512, \n",
    "                              number_of_sentence=number_of_sentence_list[index], \n",
    "                              number_of_sentence_before=number_of_sentence_before_list[index], \n",
    "                              number_of_sentence_after=number_of_sentence_after_list[index])\n",
    "    print(extractor.name, ' - Extracting activations ...')\n",
    "    for run_index, iterator in tqdm(enumerate(iterator_list)):\n",
    "        print(\"############# Run {} #############\".format(run_index))\n",
    "        activations  = extractor.extract_activations(iterator, language)\n",
    "        hidden_states_activations = activations[0]\n",
    "        #attention_heads_activations = activations[1]\n",
    "        #(cls_hidden_states_activations, cls_attention_activations) = activations[2]\n",
    "        #(sep_hidden_states_activations, sep_attention_activations) = activations[3]\n",
    "        #activations = pd.concat([hidden_states_activations, attention_heads_activations], axis=1)\n",
    "        #cls_activations = pd.concat([cls_hidden_states_activations, cls_attention_activations], axis=1)\n",
    "        #sep_activations = pd.concat([sep_hidden_states_activations, sep_attention_activations], axis=1)\n",
    "        \n",
    "        transform(\n",
    "            hidden_states_activations, \n",
    "            saving_path_folders[index], \n",
    "            'activations', \n",
    "            run_index=run_index,\n",
    "            n_layers_hidden=13,\n",
    "            n_layers_attention=0, \n",
    "            hidden_size=768))\n",
    "        #transform(activations, saving_path_folders[index], 'activations', run_index=run_index)\n",
    "        #transform(cls_activations, saving_path_folders[index], 'cls')\n",
    "        #transform(sep_activations, saving_path_folders[index], 'sep')\n",
    "        \n",
    "        #activations.to_csv(os.path.join(saving_path_folders[index], 'activations_run{}.csv'.format(run_index + 1)), index=False)\n",
    "        #cls_activations.to_csv(os.path.join(saving_path_folders[index], 'cls_run{}.csv'.format(run_index + 1)), index=False)\n",
    "        #sep_activations.to_csv(os.path.join(saving_path_folders[index], 'sep_run{}.csv'.format(run_index + 1)), index=False)\n",
    "        del activations\n",
    "        #del cls_activations\n",
    "        #del sep_activations\n",
    "        del hidden_states_activations\n",
    "        #del attention_heads_activations\n",
    "        #del cls_hidden_states_activations\n",
    "        #del cls_attention_activations\n",
    "        #del sep_hidden_states_activations\n",
    "        #del sep_attention_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
