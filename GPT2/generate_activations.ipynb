{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to extract hidden-states and attention heads activations from gpt2 model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from model import GPT2Extractor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tokenizer import tokenize\n",
    "from utils import set_seed\n",
    "from numpy import linalg as la\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_folder(path):\n",
    "    \"\"\"Create adequate folders if necessary.\"\"\"\n",
    "    try:\n",
    "        if not os.path.isdir(path):\n",
    "            check_folder(os.path.dirname(path))\n",
    "            os.mkdir(path)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(activations, path, name, run_index, n_layers_hidden=13, n_layers_attention=12, hidden_size=768):\n",
    "    assert activations.values.shape[1] == (n_layers_hidden + n_layers_attention) * hidden_size\n",
    "    indexes = [[index*hidden_size, (index+1)*hidden_size] for index in range(n_layers_hidden + n_layers_attention)]\n",
    "    for order in [2]: # np.inf\n",
    "        matrices = []\n",
    "        for i, index in enumerate(indexes):\n",
    "            matrix = activations.values[:, index[0]:index[1]]\n",
    "            #with_std = True if order=='std' else False\n",
    "            #scaler = StandardScaler(with_mean=True, with_std=with_std)\n",
    "            #scaler.fit(matrix)\n",
    "            #matrix = scaler.transform(matrix)\n",
    "            if order is not None and order != 'std':\n",
    "                matrix = matrix / np.mean(la.norm(matrix, ord=order, axis=1))\n",
    "            matrices.append(matrix)\n",
    "        matrices = np.hstack(matrices)\n",
    "        new_data = pd.DataFrame(matrices, columns=activations.columns)\n",
    "        new_path = path + '_norm-' + str(order).replace('np.', '')\n",
    "        check_folder(new_path)\n",
    "        new_data.to_csv(os.path.join(new_path, name + '_run{}.csv'.format(run_index + 1)), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/english/text_english_run*.txt' # path to text input\n",
    "language = 'english'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating iterator for each run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '/Users/alexpsq/Code/Parietal/data/text_english_run*.txt' # path to text input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = sorted(glob.glob(template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:00<00:00, 475824.40it/s]\n",
      "100%|██████████| 135/135 [00:00<00:00, 507830.53it/s]\n",
      "100%|██████████| 176/176 [00:00<00:00, 615677.65it/s]\n",
      "100%|██████████| 173/173 [00:00<00:00, 685836.10it/s]\n",
      "100%|██████████| 177/177 [00:00<00:00, 455175.85it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 817662.15it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 731391.09it/s]\n",
      "100%|██████████| 145/145 [00:00<00:00, 681046.00it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 967916.31it/s]\n"
     ]
    }
   ],
   "source": [
    "iterator_list = [tokenize(path, language, train=False) for path in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import utils\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "#from transformers import GPT2Tokenizer\n",
    "#\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "#\n",
    "#lengths = []\n",
    "#\n",
    "#for index in range(9):\n",
    "#    batches, indexes = utils.batchify_per_sentence_with_context(\n",
    "#                iterator_list[index], \n",
    "#                1, \n",
    "#                5, \n",
    "#                'gpt2', \n",
    "#                max_length=512)\n",
    "#    #lengths.append(np.array(sorted([len(item.split()) for item in batches])))\n",
    "#    lengths.append(np.array(sorted([len(tokenizer.tokenize(item)) for item in batches])))\n",
    "#\n",
    "#    sns.boxplot(lengths[-1])\n",
    "#    plt.show()\n",
    "#    print()\n",
    "#\n",
    "#print(np.mean(np.array([np.mean(item) for item in lengths])))\n",
    "#print(np.median(np.array([np.median(item) for item in lengths])))\n",
    "#print(np.mean(np.array([np.median(item) for item in lengths])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import GPT2Model\n",
    "#GPT2Model.from_pretrained('gpt2').config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import utils\n",
    "#config = {'stop_attention_at_sent': None,\n",
    "#          'number_of_sentence': 1,\n",
    "#          'stop_attention_before_sent': 0\n",
    "#         }\n",
    "#\n",
    "#batches, indexes = utils.batchify_with_detailed_indexes(\n",
    "#            iterator_list[0], \n",
    "#            config['number_of_sentence'], \n",
    "#            20, \n",
    "#            'gpt2', \n",
    "#            1024,\n",
    "#            config['stop_attention_at_sent'],\n",
    "#            config['stop_attention_before_sent'],\n",
    "#    True\n",
    "#        )\n",
    "#\n",
    "#\n",
    "#\n",
    "#indexes_tmp = [(indexes[i][-config['number_of_sentence']][0], indexes[i][-1][1]) for i in range(len(indexes))]\n",
    "#indexes_tmp[0] = (indexes[0][0][0], indexes[0][-1][1])\n",
    "#print(indexes_tmp)\n",
    "#for i in indexes_tmp:\n",
    "#    print(i[1]-i[0])\n",
    "#\n",
    "#\n",
    "#\n",
    "#from transformers import GPT2Tokenizer, GPT2Model\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "#model = GPT2Model.from_pretrained('gpt2', output_hidden_states=True)\n",
    "#\n",
    "#m = 0\n",
    "#hidden_states_activations = []\n",
    "#\n",
    "#for index, batch in enumerate(batches):\n",
    "#    batch = batch.strip() # Remove trailing character\n",
    "#    #batch = '<|endoftext|> ' + batch + ' <|endoftext|>'\n",
    "#    \n",
    "#    tokenized_text = tokenizer.tokenize(batch, add_prefix_space=False)\n",
    "#    inputs_ids = torch.tensor([tokenizer.convert_tokens_to_ids(tokenized_text)])\n",
    "#    attention_mask = torch.tensor([[1 for x in tokenized_text]])\n",
    "#\n",
    "#    if (config['stop_attention_at_sent'] is not None) and (index > 0):\n",
    "#        attention_mask[:, :1 + indexes[index][-config['stop_attention_at_sent']-config['number_of_sentence']][0]] = 0\n",
    "#        if self.config['stop_attention_before_sent'] < 0:\n",
    "#            attention_mask[:, 1 + indexes[index][-config['stop_attention_at_sent']-config['number_of_sentence']][0]: 1 + indexes[index][-config['stop_attention_at_sent']-config['number_of_sentence']][0]-config['stop_attention_before_sent']] = 0\n",
    "#        elif self.config['stop_attention_before_sent'] > 0:\n",
    "#            attention_mask[:, 1 + indexes[index][-config['stop_attention_at_sent']-config['number_of_sentence']][0]-config['stop_attention_before_sent']: 1 + indexes[index][-config['stop_attention_at_sent']-config['number_of_sentence']][0]] = 1\n",
    "#    mapping = utils.match_tokenized_to_untokenized(tokenized_text, batch)\n",
    "#    \n",
    "#    with torch.no_grad():\n",
    "#        encoded_layers = model(inputs_ids, attention_mask=attention_mask) \n",
    "#    hidden_states_activations_ = np.vstack(encoded_layers[2]) # retrieve all the hidden states (dimension = layer_count * len(tokenized_text) * feature_count)\n",
    "#    a = utils.extract_activations_from_token_activations(hidden_states_activations_, mapping, indexes_tmp[index])\n",
    "#    #m += len(tokenized_text[indexes_tmp[index][0]:indexes_tmp[index][1]])\n",
    "#    m += len(a)\n",
    "#    #print(len(a), a[0].shape)\n",
    "#    #print(len(tokenized_text), indexes_tmp[index][0], indexes_tmp[index][1])\n",
    "#    #print(tokenized_text[indexes_tmp[index][0]:indexes_tmp[index][1]])\n",
    "#    #print(np.array(tokenized_text)[attention_mask.detach().numpy()[0].astype(bool)])\n",
    "#    #print(mapping)\n",
    "#    key_start = None\n",
    "#    key_stop = None\n",
    "#    for key_, value in mapping.items(): \n",
    "#        if (value[0])== (indexes_tmp[index][0]): #because we added <|endoftext|> token at the beginning\n",
    "#            key_start = key_\n",
    "#\n",
    "#    #print(key_start, len(mapping.keys()))\n",
    "#    #print(tokenized_text)\n",
    "#    for word_index in range(key_start, len(mapping.keys())): # len(mapping.keys()) - 1\n",
    "#        print([index for index in mapping[word_index]])\n",
    "#    #print()\n",
    "#    #a=input()\n",
    "#    #if a!='a':\n",
    "#    #    break\n",
    "#print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_gpt2_models = ['gpt2'] \n",
    "names = [\n",
    "    'gpt2_pre-5_1_token-2'\n",
    "        ]\n",
    "config_paths = [None] * 56\n",
    "saving_path_folders = [\n",
    "    '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/{}/gpt2_pre-5_1_token-2'.format(language)\n",
    "    \n",
    "]\n",
    "prediction_types = ['sentence'] * 56\n",
    "number_of_sentence_list = [1] * 56\n",
    "number_of_sentence_before_list = [2]\n",
    "attention_length_before_list = [2] * 56\n",
    "stop_attention_at_sent_list = [None] * 56\n",
    "stop_attention_before_sent_list = [0] * 56\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_attentions = False\n",
    "output_hidden_states = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2_pre-5_1_token-2  - Extracting activations ...\n",
      "############# Run 0 #############\n",
      " Once ,  when  I  was  six  years  old ,  I  saw  a  magnificent  picture  in  a  book  about  the  primeval  forest  called  ‘  Real  -  life  Stories .  ’  It  showed  a  boa  constrictor  swallowing  a  wild  animal .\n",
      " Here  is  a  copy  of  the  drawing .\n",
      " It  said  in  the  book  :  “  Boa  constrictors  swallow  their  prey  whole ,  without  chewing .\n",
      " Then  they  are  not  able  to  move ,  and  they  sleep  for  the  six  months  it  takes  for  digestion .  ”\n",
      " So  I  thought  a  lot  about  the  adventures  of  the  jungle  and ,  in  turn ,  I  managed ,  with  a  coloured  pencil ,  to  make  my  first  drawing .\n",
      " My  Drawing  Number  one .\n",
      " It  looked  like  this  :  I  showed  my  masterpiece  to  the  grownups  and  I  asked  them  if  my  drawing  frightened  them .\n",
      " They  answered  me  :  “  Why  would  anyone  be  frightened  by  a  hat ?  ”\n",
      " My  drawing  was  not  of  a  hat .\n",
      " It  showed  a  boa  constrictor  digesting  an  elephant .\n",
      " I  then  drew  the  inside  of  the  boa  constrictor ,  so  that  the  grownups  could  understand .\n",
      " They  always  need  to  have  things  explained .\n",
      " My  Drawing  Number  two  looked  like  this  :  The  grownups  advised  me  to  leave  aside  drawings  of  boa  constrictors ,  open  or  closed ,  and  to  apply  myself  instead  to  geography ,  history ,  arithmetic  and  grammar .\n",
      " Thus  I  abandoned ,  at  the  age  of  six ,  a  magnificent  career  as  a  painter .\n",
      " I  was  discouraged  by  the  failure  of  my  Drawing  Number  one  and  of  my  Drawing  Number  two .\n",
      " Grownups  never  understand  anything  by  themselves ,  and  it  ’  s  tiresome  for  children  to  always  explain  things  for  them  again  and  again .\n",
      " So  I  had  to  choose  another  profession ,  and  I  learned  to  fly  airplanes .\n",
      " I  flew  a  little  in  many  places  around  the  world .\n",
      " And  geography ,  it  '  s  true ,  has  served  me  well .\n",
      " I  could  recognize ,  at  first  glance ,  China  from  Arizona .\n",
      " It  ’  s  very  useful  if  you  get  lost  at  night .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:03, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-61d576725776>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"############# Run {} #############\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mactivations\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mhidden_states_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mattention_heads_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/Parietal/NLP_models/GPT2/model.py\u001b[0m in \u001b[0;36mextract_activations\u001b[0;34m(self, iterator, language)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'token-level'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_classic_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m'control-context'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/Parietal/NLP_models/GPT2/model.py\u001b[0m in \u001b[0;36mget_classic_activations\u001b[0;34m(self, iterator, language)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                 \u001b[0mencoded_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# last_hidden_state, pooler_output, hidden_states, attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m                 \u001b[0;31m# last_hidden_state dimension: (batch_size, sequence_length, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0;31m# pooler_output dimension: (batch_size, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/Parietal/NLP_models/GPT2/modeling_hacked_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache)\u001b[0m\n\u001b[1;32m    517\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                 \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m             )\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/Parietal/NLP_models/GPT2/modeling_hacked_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, layer_past, attention_mask, head_mask, use_cache)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/Parietal/NLP_models/GPT2/modeling_hacked_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1609\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m         \u001b[0msize_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1611\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1612\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msize_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for index, gpt2_model in enumerate(pretrained_gpt2_models):\n",
    "    extractor = GPT2Extractor(gpt2_model, \n",
    "                              language, \n",
    "                              names[index], \n",
    "                              prediction_types[index],\n",
    "                              output_hidden_states=output_hidden_states, \n",
    "                              output_attentions=output_attentions,\n",
    "                              attention_length_before=attention_length_before_list[index],\n",
    "                              config_path=config_paths[index],\n",
    "                              max_length=512, \n",
    "                              number_of_sentence=number_of_sentence_list[index], \n",
    "                              number_of_sentence_before=number_of_sentence_before_list[index],\n",
    "                              stop_attention_at_sent=stop_attention_at_sent_list[index],\n",
    "                              stop_attention_before_sent=stop_attention_before_sent_list[index]\n",
    "                             )\n",
    "    print(extractor.name, ' - Extracting activations ...')\n",
    "    for run_index, iterator in tqdm(enumerate(iterator_list)):\n",
    "        gc.collect()\n",
    "        print(\"############# Run {} #############\".format(run_index + 1))\n",
    "        activations  = extractor.extract_activations(iterator, language)\n",
    "        hidden_states_activations = activations[0]\n",
    "        attention_heads_activations = activations[1]\n",
    "        #activations = pd.concat([hidden_states_activations, attention_heads_activations], axis=1)\n",
    "        print(hidden_states_activations.shape)\n",
    "        #transform(\n",
    "        #    hidden_states_activations, \n",
    "        #    saving_path_folders[index], \n",
    "        #    'activations', \n",
    "        #    run_index=run_index,\n",
    "        #    n_layers_hidden=13,\n",
    "        #    n_layers_attention=0, \n",
    "        #    hidden_size=768)\n",
    "        \n",
    "        #transform(cls_activations, saving_path_folders[index], 'cls')\n",
    "        #transform(sep_activations, saving_path_folders[index], 'sep')\n",
    "        \n",
    "        #check_folder(saving_path_folders[index])\n",
    "        #hidden_states_activations.to_csv(os.path.join(saving_path_folders[index], 'activations_run{}.csv'.format(run_index + 1)), index=False)\n",
    "        #activations.to_csv(os.path.join(saving_path_folders[index], 'activations_run{}.csv'.format(run_index + 1)), index=False)\n",
    "        #cls_activations.to_csv(os.path.join(saving_path_folders[index], 'cls_run{}.csv'.format(run_index + 1)), index=False)\n",
    "        #sep_activations.to_csv(os.path.join(saving_path_folders[index], 'sep_run{}.csv'.format(run_index + 1)), index=False)\n",
    "        del activations\n",
    "        del hidden_states_activations\n",
    "        #del attention_heads_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, bert_model in enumerate(pretrained_bert_models):\n",
    "    extractor = BertExtractor(bert_model, \n",
    "                              language, \n",
    "                              names[index], \n",
    "                              prediction_types[index], \n",
    "                              output_hidden_states=True, \n",
    "                              output_attentions=True, \n",
    "                              config_path=config_paths[index], \n",
    "                              max_length=512, \n",
    "                              number_of_sentence=number_of_sentence_list[index], \n",
    "                              number_of_sentence_before=number_of_sentence_before_list[index], \n",
    "                              number_of_sentence_after=number_of_sentence_after_list[index])\n",
    "    print(extractor.name, ' - Extracting activations ...')\n",
    "    for run_index, iterator in tqdm(enumerate(iterator_list)):\n",
    "        print(\"############# Run {} #############\".format(run_index))\n",
    "        activations  = extractor.extract_activations(iterator, language)\n",
    "        hidden_states_activations = activations[0]\n",
    "        attention_heads_activations = activations[1]\n",
    "        (cls_hidden_states_activations, cls_attention_activations) = activations[2]\n",
    "        (sep_hidden_states_activations, sep_attention_activations) = activations[3]\n",
    "        activations = pd.concat([hidden_states_activations, attention_heads_activations], axis=1)\n",
    "        cls_activations = pd.concat([cls_hidden_states_activations, cls_attention_activations], axis=1)\n",
    "        sep_activations = pd.concat([sep_hidden_states_activations, sep_attention_activations], axis=1)\n",
    "        \n",
    "        transform(activations, saving_path_folders[index], 'activations', run_index=run_index)\n",
    "        #transform(cls_activations, saving_path_folders[index], 'cls')\n",
    "        #transform(sep_activations, saving_path_folders[index], 'sep')\n",
    "        \n",
    "        #activations.to_csv(os.path.join(saving_path_folders[index], 'activations_run{}.csv'.format(run_index + 1)), index=False)\n",
    "        #cls_activations.to_csv(os.path.join(saving_path_folders[index], 'cls_run{}.csv'.format(run_index + 1)), index=False)\n",
    "        #sep_activations.to_csv(os.path.join(saving_path_folders[index], 'sep_run{}.csv'.format(run_index + 1)), index=False)\n",
    "        del activations\n",
    "        del cls_activations\n",
    "        del sep_activations\n",
    "        del hidden_states_activations\n",
    "        del attention_heads_activations\n",
    "        del cls_hidden_states_activations\n",
    "        del cls_attention_activations\n",
    "        del sep_hidden_states_activations\n",
    "        del sep_attention_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for p in list(extractor.model.named_parameters()):\n",
    "    print(p[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate control activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = 'gpt2'\n",
    "language = 'english'\n",
    "name = 'gpt2_control_'\n",
    "prediction_type = 'sentence'\n",
    "saving_path_folder = '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/{}'.format(language)\n",
    "seeds = [24, 213, 1111, 61, 183]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_layer(model, layer_nb):\n",
    "    \"\"\"Randomize layer weights and put bias to zero.\n",
    "    The input \"layer_nb\" goes from 1 to 12 to be coherent with the rest of the analysis.\n",
    "    It is then transfomed in the function.\n",
    "    \"\"\"\n",
    "    layer_nb = layer_nb - 1\n",
    "    model.h[layer_nb].ln_1.weight = torch.nn.parameter.Parameter(torch.rand_like(model.h[layer_nb].ln_1.weight))\n",
    "    model.h[layer_nb].ln_1.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.h[layer_nb].ln_1.bias))\n",
    "    model.h[layer_nb].attn.c_attn.weight = torch.nn.parameter.Parameter(torch.rand_like(model.h[layer_nb].attn.c_attn.weight))\n",
    "    model.h[layer_nb].attn.c_attn.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.h[layer_nb].attn.c_attn.bias))\n",
    "    model.h[layer_nb].attn.c_proj.weight = torch.nn.parameter.Parameter(torch.rand_like(model.h[layer_nb].attn.c_proj.weight))\n",
    "    model.h[layer_nb].attn.c_proj.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.h[layer_nb].attn.c_proj.bias))\n",
    "    model.h[layer_nb].ln_2.weight = torch.nn.parameter.Parameter(torch.rand_like(model.h[layer_nb].ln_2.weight))\n",
    "    model.h[layer_nb].ln_2.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.h[layer_nb].ln_2.bias))\n",
    "    model.h[layer_nb].mlp.c_fc.weight = torch.nn.parameter.Parameter(torch.rand_like(model.h[layer_nb].mlp.c_fc.weight))\n",
    "    model.h[layer_nb].mlp.c_fc.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.h[layer_nb].mlp.c_fc.bias))\n",
    "    model.h[layer_nb].mlp.c_proj.weight = torch.nn.parameter.Parameter(torch.rand_like(model.h[layer_nb].mlp.c_proj.weight))\n",
    "    model.h[layer_nb].mlp.c_proj.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.h[layer_nb].mlp.c_proj.bias))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_ln_1(model, layer_nb):\n",
    "    \"\"\"Randomize attention query weights of a given layer and put bias to zero.\n",
    "    \"\"\"\n",
    "    layer_nb = layer_nb - 1\n",
    "    model.h[layer_nb].ln_1.weight = torch.nn.parameter.Parameter(torch.rand_like(model.h[layer_nb].ln_1.weight))\n",
    "    model.h[layer_nb].ln_1.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.h[layer_nb].ln_1.bias))\n",
    "    return model\n",
    "\n",
    "def randomize_attention_c_attn(model, layer_nb):\n",
    "    \"\"\"Randomize attention key weights of a given layer and put bias to zero.\n",
    "    \"\"\"\n",
    "    layer_nb = layer_nb - 1\n",
    "    model.h[layer_nb].attn.c_attn.weight = torch.nn.parameter.Parameter(torch.rand_like(model.h[layer_nb].attn.c_attn.weight))\n",
    "    model.h[layer_nb].attn.c_attn.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.h[layer_nb].attn.c_attn.bias))\n",
    "    return model\n",
    "\n",
    "def randomize_attention_c_proj(model, layer_nb):\n",
    "    \"\"\"Randomize attention value weights of a given layer and put bias to zero.\n",
    "    \"\"\"\n",
    "    layer_nb = layer_nb - 1\n",
    "    model.h[layer_nb].attn.c_proj.weight = torch.nn.parameter.Parameter(torch.rand_like(model.h[layer_nb].attn.c_proj.weight))\n",
    "    model.h[layer_nb].attn.c_proj.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.h[layer_nb].attn.c_proj.bias))\n",
    "    return model\n",
    "\n",
    "def randomize_ln_2(model, layer_nb):\n",
    "    \"\"\"Randomize attention dense network weights of a given layer and put bias to zero.\n",
    "    \"\"\"\n",
    "    layer_nb = layer_nb - 1\n",
    "    model.h[layer_nb].ln_2.weight = torch.nn.parameter.Parameter(torch.rand_like(model.h[layer_nb].ln_2.weight))\n",
    "    model.h[layer_nb].ln_2.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.h[layer_nb].ln_2.bias))\n",
    "    return model\n",
    "\n",
    "\n",
    "def randomize_mlp_c_fc(model, layer_nb):\n",
    "    \"\"\"Randomize intermediate dense network weights of a given layer and put bias to zero.\n",
    "    \"\"\"\n",
    "    layer_nb = layer_nb - 1\n",
    "    model.h[layer_nb].mlp.c_fc.weight = torch.nn.parameter.Parameter(torch.rand_like(model.h[layer_nb].mlp.c_fc.weight))\n",
    "    model.h[layer_nb].mlp.c_fc.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.h[layer_nb].mlp.c_fc.bias))\n",
    "    return model\n",
    "\n",
    "def randomize_mlp_c_proj(model, layer_nb):\n",
    "    \"\"\"Randomize output dense network weights of a given layer and put bias to zero.\n",
    "    \"\"\"\n",
    "    layer_nb = layer_nb - 1\n",
    "    model.h[layer_nb].mlp.c_proj.weight = torch.nn.parameter.Parameter(torch.rand_like(model.h[layer_nb].mlp.c_proj.weight))\n",
    "    model.h[layer_nb].mlp.c_proj.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.h[layer_nb].mlp.c_proj.bias))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_embeddings(model):\n",
    "    \"\"\"Randomize embeddings weights and put bias to zero.\n",
    "    \"\"\"\n",
    "    model.wte.weight = torch.nn.parameter.Parameter(torch.rand_like(model.wte.weight))\n",
    "    model.wpe.weight = torch.nn.parameter.Parameter(torch.rand_like(model.wpe.weight))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "    for layer in range(13):\n",
    "        extractor = GPT2Extractor(gpt2_model, language, name, prediction_type, output_hidden_states=True, output_attentions=True, config_path=None)\n",
    "        if layer==0:\n",
    "            extractor.model = randomize_embeddings(extractor.model)\n",
    "        else:\n",
    "            extractor.model = randomize_layer(extractor.model, layer)\n",
    "        print(extractor.name + str(seed), ' - Extracting activations for layer {}...'.format(layer))\n",
    "        for run_index, iterator in tqdm(enumerate(iterator_list)):\n",
    "            print(\"############# Run {} #############\".format(run_index))\n",
    "            activations  = extractor.extract_activations(iterator, language)\n",
    "            hidden_states_activations = activations[0]\n",
    "            attention_heads_activations = activations[1]\n",
    "            activations = pd.concat([hidden_states_activations, attention_heads_activations], axis=1)\n",
    "\n",
    "            # activations\n",
    "            heads = np.arange(1, 13)\n",
    "            columns_to_retrieve = ['hidden_state-layer-{}-{}'.format(layer, i) for i in range(1, 769)]\n",
    "            if layer > 0:\n",
    "                columns_to_retrieve += ['attention-layer-{}-head-{}-{}'.format(layer, head, i) for head in heads for i in range(1, 65)]\n",
    "            activations = activations[columns_to_retrieve]\n",
    "\n",
    "            save_path = os.path.join(saving_path_folder, name + str(seed) + '_layer-{}'.format(layer))\n",
    "            check_folder(save_path)\n",
    "            print('\\tSaving in {}.'.format(save_path))\n",
    "            activations.to_csv(os.path.join(save_path, 'activations_run{}.csv'.format(run_index + 1)), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test activation extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils \n",
    "import random\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "config = {\n",
    "    'number_of_sentence': 1, \n",
    "    'number_of_sentence_before': 3, \n",
    "    'attention_length_before': 3, \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "extractor_full = GPT2Extractor('gpt2', \n",
    "                              'english', \n",
    "                              'test', \n",
    "                              'sentence', \n",
    "                              output_hidden_states=True, \n",
    "                              output_attentions=False, \n",
    "                              attention_length_before=config['attention_length_before'],\n",
    "                              config_path=None, \n",
    "                              number_of_sentence=config['number_of_sentence'], \n",
    "                              number_of_sentence_before=config['number_of_sentence_before'], \n",
    "                             )\n",
    "extractor_masked = GPT2Extractor('gpt2', \n",
    "                              'english', \n",
    "                              'test', \n",
    "                              'control-context', \n",
    "                              output_hidden_states=True, \n",
    "                              output_attentions=False, \n",
    "                              attention_length_before=config['attention_length_before'],\n",
    "                              config_path=None, \n",
    "                              number_of_sentence=config['number_of_sentence'], \n",
    "                              number_of_sentence_before=config['number_of_sentence_before'], \n",
    "                             )\n",
    "extractor_shuffle = GPT2Extractor('gpt2', \n",
    "                              'english', \n",
    "                              'test', \n",
    "                              'shuffle', \n",
    "                              output_hidden_states=True, \n",
    "                              output_attentions=False, \n",
    "                              attention_length_before=config['attention_length_before'],\n",
    "                              config_path=None, \n",
    "                              number_of_sentence=config['number_of_sentence'], \n",
    "                              number_of_sentence_before=config['number_of_sentence_before'], \n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full sentences\n",
    "batches_full, indexes_full = utils.batchify_with_detailed_indexes(\n",
    "            iterator_list[0], \n",
    "            config['number_of_sentence'], \n",
    "            config['number_of_sentence_before'], \n",
    "            'gpt2',\n",
    "            add_prefix_space=True\n",
    "        )\n",
    "\n",
    "# Tokens are masked\n",
    "batches_masked, indexes_masked = utils.batchify_with_detailed_indexes(\n",
    "            iterator_list[0], \n",
    "            config['number_of_sentence'], \n",
    "            config['number_of_sentence_before'], \n",
    "            'gpt2',\n",
    "            add_prefix_space=True\n",
    "            )\n",
    "\n",
    "# Shuffling\n",
    "batches_shuffle, indexes_shuffle = utils.batchify_sentences(\n",
    "            iterator_list[0], \n",
    "            config['number_of_sentence'], \n",
    "            config['number_of_sentence_before'], \n",
    "            pretrained_model='gpt2', \n",
    "            past_context_size=config['attention_length_before'],\n",
    "            transformation='shuffle',\n",
    "            vocabulary=None,\n",
    "            dictionary=None,\n",
    "            seed=1111,\n",
    "            add_prefix_space=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing full\n",
    "indexes_full_tmp = [(indexes_full[i][-config['number_of_sentence']][0], indexes_full[i][-1][1]) for i in range(len(indexes_full))]\n",
    "indexes_full_tmp[0] = (indexes_full[0][0][0], indexes_full[0][-1][1])\n",
    "\n",
    "for i in range(len(indexes_full_tmp)):\n",
    "    indexes_full_tmp[i] = (indexes_full_tmp[i][0] + 1, indexes_full_tmp[i][1] + 1)\n",
    "    \n",
    "# Preprocessing masked\n",
    "indexes_masked_tmp = [(indexes_masked[i][-config['number_of_sentence']][0], indexes_masked[i][-1][1]) for i in range(len(indexes_masked))]\n",
    "indexes_masked_tmp[0] = (indexes_masked[0][0][0], indexes_masked[0][-1][1])\n",
    "\n",
    "for i in range(len(indexes_masked_tmp)):\n",
    "    indexes_masked_tmp[i] = (indexes_masked_tmp[i][0] + 1, indexes_masked_tmp[i][1] + 1)\n",
    "\n",
    "# Preprocessing shuffled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once , when I was six years old , I saw a magnificent picture in a book about the primeval forest called ‘ Real - life Stories . ’ It showed a boa constrictor swallowing a wild animal . Here is a copy of the drawing .\n",
      "It said in the book : “ Boa constrictors swallow their prey whole , without chewing .\n",
      "Then they are not able to move , and they sleep for the six months it takes for digestion . ”\n",
      "So I thought a lot about the adventures of the jungle and , in turn , I managed , with a coloured pencil , to make my first drawing .\n",
      "My Drawing Number one .\n",
      "It looked like this : I showed my masterpiece to the grownups and I asked them if my drawing frightened them .\n",
      "They answered me : “ Why would anyone be frightened by a hat ? ”\n",
      "My drawing was not of a hat .\n",
      "It showed a boa constrictor digesting an elephant .\n",
      "I then drew the inside of the boa constrictor , so that the grownups could understand .\n",
      "They always need to have things explained .\n",
      "My Drawing Number two looked like this : The grownups advised me to leave aside drawings of boa constrictors , open or closed , and to apply myself instead to geography , history , arithmetic and grammar .\n",
      "Thus I abandoned , at the age of six , a magnificent career as a painter .\n",
      "I was discouraged by the failure of my Drawing Number one and of my Drawing Number two .\n",
      "Grownups never understand anything by themselves , and it ’ s tiresome for children to always explain things for them again and again .\n",
      "So I had to choose another profession , and I learned to fly airplanes .\n",
      "I flew a little in many places around the world .\n",
      "And geography , it ' s true , has served me well .\n",
      "I could recognize , at first glance , China from Arizona .\n",
      "It ’ s very useful if you get lost at night .\n",
      "I have had , during my life , a lot of contact with many persons of consequence .\n",
      "I have lived a lot amongst the grownups .\n",
      "I have seen them from close up .\n",
      "It hasnt much improved my opinion of them .\n",
      "Whenever I met one of them that seemed a bit more clear - sighted , I tried the experiment of showing them my Drawing Number one , that I ' ve always kept .\n",
      "I wanted to know if they were really a person of true understanding .\n",
      "But they always responded : “ It ' s a hat . ”\n",
      "So I would never speak to them of boa constrictors , nor of primeval forests , nor of the stars .\n",
      "I put myself at their level .\n",
      "I talked to them about bridge , golf , politics and neckties .\n",
      "And the grownup was glad to know such a sensible man .\n",
      "So I lived alone , without anyone I could really talk to , until a breakdown in the Sahara desert , six years ago .\n",
      "Something had broken in my engine .\n",
      "And as I had with me neither a mechanic nor any passengers , I readied myself to try and carry out , all alone , the difficult repairs .\n",
      "For me it was a matter of life or death .\n",
      "I had hardly enough water to drink for a week .\n",
      "The first night I went to sleep on the sand , a thousand miles from any human habitation .\n",
      "I was more isolated than a shipwrecked sailor on a raft in the middle of the ocean .\n",
      "So you can imagine my surprise when at daybreak , a funny little voice woke me up .\n",
      "It said : “ Please ...\n",
      "draw me a sheep ! ”\n",
      "“ What ? ”\n",
      "“ Draw me a sheep ! ”\n",
      "I jumped to my feet as if I ’ d been struck by lightning .\n",
      "I rubbed my eyes .\n",
      "I took a good look around me .\n",
      "And I saw a quite extraordinary little man , who was examining me seriously .\n",
      "Here is the best portrait that , later , I managed to do of him .\n",
      "But my drawing , of course , is much less charming than its model .\n",
      "It ' s not my fault .\n",
      "I was discouraged in my career as a painter by the grownups , at the age of six , and I hadn ' t learned to draw anything except boa constrictors , closed and open .\n",
      "I stared at this sudden apparition wide eyed with astonishment .\n",
      "Remember that I was a thousand miles from any inhabited region .\n",
      "And yet this little fellow seemed neither lost , nor half - dead with fatigue , nor starved or dying of thirst or fear .\n",
      "He looked nothing like a child lost in the middle of the desert , a thousand miles from any inhabited region .\n",
      "When I finally managed to speak , I said : “ But — what are you doing here ? ”\n",
      "And he repeated , very slowly , as if it was something very serious : “ Please ...\n",
      "draw me a sheep ... ”\n",
      "When a mystery is too overpowering , one dare not disobey .\n",
      "Absurd as it seemed to me a thousand miles from any human habitation and in danger of death , I took out of my pocket a sheet of paper and a pen .\n",
      "But then I remembered that I had mainly studied geography , history , arithmetic and grammar , and I told the little fellow ( a little crossly ) that I didn ’ t know how to draw .\n",
      "He replied : “ It doesn ' t matter .\n",
      "Draw me a sheep . ”\n",
      "As I ’ d never drawn a sheep , I redrew for him one of the only two drawings that I was capable of .\n",
      "The one of the closed boa constrictor .\n",
      "And I was astounded to hear the little fellow respond : “ No !\n",
      "No !\n",
      "I don ’ t want an elephant inside a boa constrictor .\n",
      "A boa constrictor is very dangerous , and an elephant is very cumbersome .\n",
      "Where I live everything is very small .\n",
      "I need a sheep .\n",
      "Draw me a sheep . ”\n",
      "So I drew .\n",
      "He looked carefully , then said : “ No !\n",
      "This one ’ s already very sick .\n",
      "Make another one . ”\n",
      "I drew again : My friend smiled gently and indulgently : “ You can see yourself ...\n",
      "this isn ’ t a sheep , it ' s a ram .\n",
      "It has horns ...\n",
      "“ So once again I redid my drawing : But it was rejected , like the previous ones : “ This one ’ s too old .\n",
      "I want a sheep that will live a long time . ”\n",
      "So , getting impatient , as I was eager to start dismantling my engine , I hastily sketched this drawing : And I snapped : “ This here is the box .\n",
      "The sheep you want is inside . ”\n",
      "But I was very surprised to see the face of my young judge light up : “ It ' s exactly the way I wanted !\n",
      "Do you think this sheep needs a lot of grass ? ”\n",
      "“ Why ? ”\n",
      "“ Because where I ' m from everything is very small ... ”\n",
      "“ There will certainly be enough .\n",
      "I gave you a very small sheep . ”\n",
      "He leaned his head towards the drawing : “ Not that small ...\n",
      "Look !\n",
      "He ' s fallen asleep ... ”\n",
      "And that ' s how I met the little prince .\n",
      "It took me a long time to find out where he came from .\n",
      "The little prince , who asked me many questions , never seemed to hear my own .\n",
      "It was the words spoken by chance that , little by little , revealed everything to me .\n",
      "So , when he saw my airplane for the first time ( I won ’ t draw my airplane , it would be a drawing far too complicated for me ) , he asked me : “ What ' s that thing there ? ”\n",
      "“ It ' s not a thing .\n",
      "It flies .\n",
      "It ' s an airplane .\n",
      "It ’ s my airplane . ”\n",
      "And I was proud to have him know that I could fly .\n",
      "Then he cried : “ What ?\n",
      "You fell from the sky ! ”\n",
      "“ Yes , ” I said modestly .\n",
      "“ Oh !\n",
      "That ' s funny !\n",
      "... ”\n",
      "And the little prince broke into a lovely peal of laughter , which irritated me very much .\n",
      "I prefer people to take my misfortunes seriously .\n",
      "Then he added : “ So , you also come from the sky !\n",
      "What planet are you from ? ”\n",
      "I caught a glimpse into the mystery of his presence , and I asked abruptly : “ So you come from another planet then ? ”\n",
      "But he didn ’ t answer .\n",
      "He shook his head slowly whilst looking at my airplane : “ It ' s true that you can ' t have come from far away in that thing ... ”\n",
      "And he drifted into a daydream which lasted a long while .\n",
      "Then , taking my sheep out of his pocket , he sank himself into the contemplation of his treasure .\n",
      "You can imagine how my curiosity was aroused by this small disclosure about ‘ the other planets . ’\n",
      "So I tried to find out more : “ Where are you from my little fellow ?\n",
      "Where ’ s this ‘ where I live ’ of yours ?\n",
      "Where do you take my sheep off to ? ”\n",
      "After a reflective silence he answered : “ What ' s good about the box you ’ ve given me is that at night , he can use it as a house . ”\n",
      "“ That ’ s right .\n",
      "And if you ’ re good , I ' ll give you a rope to tie him up with during the day .\n",
      "And a stake . ”\n",
      "The offer seemed to shock the little prince : “ Tie him up ?\n",
      "What a funny idea ! ”\n",
      "“ But if you don ' t tie him up , he ’ ll wander off , and get lost . ”\n",
      "My friend broke into another peal of laughter : “ Where do you think he ’ d go ! ”\n",
      "“ Anywhere .\n",
      "Straight ahead ... ”\n",
      "Then the little prince said gravely : “ That doesn ’ t matter ; where I live , everything is so small ! ”\n",
      "And perhaps with a hint of sadness , he added : “ Straight ahead you can ' t go far ... ”\n"
     ]
    }
   ],
   "source": [
    "# activation generation full\n",
    "output = []\n",
    "for index, batch in enumerate(batches_full):\n",
    "    batch = batch.strip() # Remove trailing character\n",
    "    batch = '<|endoftext|> ' + batch + ' <|endoftext|>'\n",
    "\n",
    "    tokenized_text = tokenizer.tokenize(batch, add_prefix_space=False)\n",
    "    mapping = utils.match_tokenized_to_untokenized(tokenized_text, batch)\n",
    "    inputs_ids = torch.tensor([tokenizer.convert_tokens_to_ids(tokenized_text)])\n",
    "\n",
    "    attention_mask = torch.tensor([[1 for x in tokenized_text]])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoded_layers = extractor_full.model(inputs_ids, attention_mask=attention_mask) # last_hidden_state, pooler_output, hidden_states, attentions\n",
    "\n",
    "        hidden_states_activations_ = np.vstack(encoded_layers[2]) # retrieve all the hidden states (dimension = layer_count * len(tokenized_text) * feature_count)\n",
    "        \n",
    "        #print(len(encoded_layers[2]))\n",
    "        #print('output shape:', hidden_states_activations_.shape)\n",
    "        \n",
    "        new_activations = []\n",
    "        key = None\n",
    "        \n",
    "        #print('Mapping:')\n",
    "        #for key in mapping.keys():\n",
    "        #    print(batch.split()[key], ''.join([tokenized_text[i] for i in mapping[key]]))\n",
    "        #print('A priori Token of interest:', tokenized_text[indexes_full_tmp[index][0]:indexes_full_tmp[index][1]])\n",
    "            \n",
    "        for key_, value in mapping.items(): \n",
    "            if indexes_full_tmp[index][0] in value:\n",
    "                key = key_\n",
    "                \n",
    "        #print(key)\n",
    "        #print('dimension match:', len(tokenized_text)==hidden_states_activations_.shape[1])\n",
    "        tmp = ' '.join([tokenizer.decode(tokenizer.convert_tokens_to_ids([tokenized_text[word] for word in mapping[index]])) for index in range(key, len(mapping.keys()) - 1)])\n",
    "        tmp = tmp.replace('  ', ' ').strip()\n",
    "        #print('Extracting sentence:')\n",
    "        print(tmp)\n",
    "        output.append(tmp)\n",
    "\n",
    "        for word_index in range(key, len(mapping.keys()) - 1):\n",
    "            word_activation = []\n",
    "            word_activation.append([hidden_states_activations_[:,index, :] for index in mapping[word_index]])\n",
    "            word_activation = np.vstack(word_activation)\n",
    "            new_activations.append(np.mean(word_activation, axis=0).reshape(1,-1))\n",
    "\n",
    "        \n",
    "        #print(np.vstack(new_activations).shape)\n",
    "        #if input()!='':\n",
    "        #    break\n",
    "\n",
    "\n",
    "assert ' '.join(output) == ' '.join(iterator_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once , when I was six years old , I saw a magnificent picture in a book about the primeval forest called ‘ Real - life Stories . ’ It showed a boa constrictor swallowing a wild animal . Here is a copy of the drawing .\n",
      "It said in the book : “ Boa constrictors swallow their prey whole , without chewing .\n",
      "Then they are not able to move , and they sleep for the six months it takes for digestion . ”\n",
      "So I thought a lot about the adventures of the jungle and , in turn , I managed , with a coloured pencil , to make my first drawing .\n",
      "My Drawing Number one .\n",
      "It looked like this : I showed my masterpiece to the grownups and I asked them if my drawing frightened them .\n",
      "They answered me : “ Why would anyone be frightened by a hat ? ”\n",
      "My drawing was not of a hat .\n",
      "It showed a boa constrictor digesting an elephant .\n",
      "I then drew the inside of the boa constrictor , so that the grownups could understand .\n",
      "They always need to have things explained .\n",
      "My Drawing Number two looked like this : The grownups advised me to leave aside drawings of boa constrictors , open or closed , and to apply myself instead to geography , history , arithmetic and grammar .\n",
      "Thus I abandoned , at the age of six , a magnificent career as a painter .\n",
      "I was discouraged by the failure of my Drawing Number one and of my Drawing Number two .\n",
      "Grownups never understand anything by themselves , and it ’ s tiresome for children to always explain things for them again and again .\n",
      "So I had to choose another profession , and I learned to fly airplanes .\n",
      "I flew a little in many places around the world .\n",
      "And geography , it ' s true , has served me well .\n",
      "I could recognize , at first glance , China from Arizona .\n",
      "It ’ s very useful if you get lost at night .\n",
      "I have had , during my life , a lot of contact with many persons of consequence .\n",
      "I have lived a lot amongst the grownups .\n",
      "I have seen them from close up .\n",
      "It hasnt much improved my opinion of them .\n",
      "Whenever I met one of them that seemed a bit more clear - sighted , I tried the experiment of showing them my Drawing Number one , that I ' ve always kept .\n",
      "I wanted to know if they were really a person of true understanding .\n",
      "But they always responded : “ It ' s a hat . ”\n",
      "So I would never speak to them of boa constrictors , nor of primeval forests , nor of the stars .\n",
      "I put myself at their level .\n",
      "I talked to them about bridge , golf , politics and neckties .\n",
      "And the grownup was glad to know such a sensible man .\n",
      "So I lived alone , without anyone I could really talk to , until a breakdown in the Sahara desert , six years ago .\n",
      "Something had broken in my engine .\n",
      "And as I had with me neither a mechanic nor any passengers , I readied myself to try and carry out , all alone , the difficult repairs .\n",
      "For me it was a matter of life or death .\n",
      "I had hardly enough water to drink for a week .\n",
      "The first night I went to sleep on the sand , a thousand miles from any human habitation .\n",
      "I was more isolated than a shipwrecked sailor on a raft in the middle of the ocean .\n",
      "So you can imagine my surprise when at daybreak , a funny little voice woke me up .\n",
      "It said : “ Please ...\n",
      "draw me a sheep ! ”\n",
      "“ What ? ”\n",
      "“ Draw me a sheep ! ”\n",
      "I jumped to my feet as if I ’ d been struck by lightning .\n",
      "I rubbed my eyes .\n",
      "I took a good look around me .\n",
      "And I saw a quite extraordinary little man , who was examining me seriously .\n",
      "Here is the best portrait that , later , I managed to do of him .\n",
      "But my drawing , of course , is much less charming than its model .\n",
      "It ' s not my fault .\n",
      "I was discouraged in my career as a painter by the grownups , at the age of six , and I hadn ' t learned to draw anything except boa constrictors , closed and open .\n",
      "I stared at this sudden apparition wide eyed with astonishment .\n",
      "Remember that I was a thousand miles from any inhabited region .\n",
      "And yet this little fellow seemed neither lost , nor half - dead with fatigue , nor starved or dying of thirst or fear .\n",
      "He looked nothing like a child lost in the middle of the desert , a thousand miles from any inhabited region .\n",
      "When I finally managed to speak , I said : “ But — what are you doing here ? ”\n",
      "And he repeated , very slowly , as if it was something very serious : “ Please ...\n",
      "draw me a sheep ... ”\n",
      "When a mystery is too overpowering , one dare not disobey .\n",
      "Absurd as it seemed to me a thousand miles from any human habitation and in danger of death , I took out of my pocket a sheet of paper and a pen .\n",
      "But then I remembered that I had mainly studied geography , history , arithmetic and grammar , and I told the little fellow ( a little crossly ) that I didn ’ t know how to draw .\n",
      "He replied : “ It doesn ' t matter .\n",
      "Draw me a sheep . ”\n",
      "As I ’ d never drawn a sheep , I redrew for him one of the only two drawings that I was capable of .\n",
      "The one of the closed boa constrictor .\n",
      "And I was astounded to hear the little fellow respond : “ No !\n",
      "No !\n",
      "I don ’ t want an elephant inside a boa constrictor .\n",
      "A boa constrictor is very dangerous , and an elephant is very cumbersome .\n",
      "Where I live everything is very small .\n",
      "I need a sheep .\n",
      "Draw me a sheep . ”\n",
      "So I drew .\n",
      "He looked carefully , then said : “ No !\n",
      "This one ’ s already very sick .\n",
      "Make another one . ”\n",
      "I drew again : My friend smiled gently and indulgently : “ You can see yourself ...\n",
      "this isn ’ t a sheep , it ' s a ram .\n",
      "It has horns ...\n",
      "“ So once again I redid my drawing : But it was rejected , like the previous ones : “ This one ’ s too old .\n",
      "I want a sheep that will live a long time . ”\n",
      "So , getting impatient , as I was eager to start dismantling my engine , I hastily sketched this drawing : And I snapped : “ This here is the box .\n",
      "The sheep you want is inside . ”\n",
      "But I was very surprised to see the face of my young judge light up : “ It ' s exactly the way I wanted !\n",
      "Do you think this sheep needs a lot of grass ? ”\n",
      "“ Why ? ”\n",
      "“ Because where I ' m from everything is very small ... ”\n",
      "“ There will certainly be enough .\n",
      "I gave you a very small sheep . ”\n",
      "He leaned his head towards the drawing : “ Not that small ...\n",
      "Look !\n",
      "He ' s fallen asleep ... ”\n",
      "And that ' s how I met the little prince .\n",
      "It took me a long time to find out where he came from .\n",
      "The little prince , who asked me many questions , never seemed to hear my own .\n",
      "It was the words spoken by chance that , little by little , revealed everything to me .\n",
      "So , when he saw my airplane for the first time ( I won ’ t draw my airplane , it would be a drawing far too complicated for me ) , he asked me : “ What ' s that thing there ? ”\n",
      "“ It ' s not a thing .\n",
      "It flies .\n",
      "It ' s an airplane .\n",
      "It ’ s my airplane . ”\n",
      "And I was proud to have him know that I could fly .\n",
      "Then he cried : “ What ?\n",
      "You fell from the sky ! ”\n",
      "“ Yes , ” I said modestly .\n",
      "“ Oh !\n",
      "That ' s funny !\n",
      "... ”\n",
      "And the little prince broke into a lovely peal of laughter , which irritated me very much .\n",
      "I prefer people to take my misfortunes seriously .\n",
      "Then he added : “ So , you also come from the sky !\n",
      "What planet are you from ? ”\n",
      "I caught a glimpse into the mystery of his presence , and I asked abruptly : “ So you come from another planet then ? ”\n",
      "But he didn ’ t answer .\n",
      "He shook his head slowly whilst looking at my airplane : “ It ' s true that you can ' t have come from far away in that thing ... ”\n",
      "And he drifted into a daydream which lasted a long while .\n",
      "Then , taking my sheep out of his pocket , he sank himself into the contemplation of his treasure .\n",
      "You can imagine how my curiosity was aroused by this small disclosure about ‘ the other planets . ’\n",
      "So I tried to find out more : “ Where are you from my little fellow ?\n",
      "Where ’ s this ‘ where I live ’ of yours ?\n",
      "Where do you take my sheep off to ? ”\n",
      "After a reflective silence he answered : “ What ' s good about the box you ’ ve given me is that at night , he can use it as a house . ”\n",
      "“ That ’ s right .\n",
      "And if you ’ re good , I ' ll give you a rope to tie him up with during the day .\n",
      "And a stake . ”\n",
      "The offer seemed to shock the little prince : “ Tie him up ?\n",
      "What a funny idea ! ”\n",
      "“ But if you don ' t tie him up , he ’ ll wander off , and get lost . ”\n",
      "My friend broke into another peal of laughter : “ Where do you think he ’ d go ! ”\n",
      "“ Anywhere .\n",
      "Straight ahead ... ”\n",
      "Then the little prince said gravely : “ That doesn ’ t matter ; where I live , everything is so small ! ”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And perhaps with a hint of sadness , he added : “ Straight ahead you can ' t go far ... ”\n"
     ]
    }
   ],
   "source": [
    "# activation generation masked\n",
    "output = []\n",
    "for index_batch, batch in enumerate(batches_masked):\n",
    "    batch = batch.strip() # Remove trailing character\n",
    "    batch = '<|endoftext|> ' + batch + ' <|endoftext|>'\n",
    "\n",
    "    tokenized_text = tokenizer.tokenize(batch, add_prefix_space=False)\n",
    "    mapping = utils.match_tokenized_to_untokenized(tokenized_text, batch)\n",
    "\n",
    "    beg = indexes_masked_tmp[index_batch][0] \n",
    "    end = indexes_masked_tmp[index_batch][1] \n",
    "\n",
    "    inputs_ids = torch.tensor([tokenizer.convert_tokens_to_ids(tokenized_text)])\n",
    "    inputs_ids = torch.cat(inputs_ids.size(1) * [inputs_ids])\n",
    "    inputs_ids = inputs_ids[beg:end, :]\n",
    "\n",
    "    attention_mask =  torch.diag_embed(torch.tensor([0 for x in tokenized_text]))\n",
    "    for i in range(min(len(tokenized_text), config['attention_length_before'])):\n",
    "        attention_mask = torch.add(attention_mask, torch.diag_embed(torch.tensor([1 for x in range(len(tokenized_text) - i)]), offset=-i))\n",
    "    attention_mask = attention_mask[beg:end, :]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoded_layers = extractor_masked.model(inputs_ids, attention_mask=attention_mask) # last_hidden_state, pooler_output, hidden_states, attentions\n",
    "\n",
    "        hidden_states_activations_ = np.vstack([torch.cat([encoded_layers[2][layer][i,len(tokenized_text) - encoded_layers[2][layer].size(0) + i - 1,:].unsqueeze(0) for i in range(encoded_layers[2][layer].size(0))], dim=0).unsqueeze(0).detach().numpy() for layer in range(len(encoded_layers[2]))])\n",
    "        hidden_states_activations_ = np.concatenate([np.zeros((hidden_states_activations_.shape[0], indexes_masked_tmp[index_batch][0] , hidden_states_activations_.shape[-1])), hidden_states_activations_, np.zeros((hidden_states_activations_.shape[0], len(tokenized_text) - indexes_masked_tmp[index_batch][1], hidden_states_activations_.shape[-1]))], axis=1)\n",
    "        # retrieve all the hidden states (dimension = layer_count * len(tokenized_text) * feature_count)\n",
    "            \n",
    "        #print(len(encoded_layers[2]))\n",
    "        #print('output shape:', hidden_states_activations_.shape)\n",
    "        \n",
    "        new_activations = []\n",
    "        key = None\n",
    "        \n",
    "        #print('Mapping:')\n",
    "        #for key in mapping.keys():\n",
    "        #    print(batch.split()[key], ''.join([tokenized_text[i] for i in mapping[key]]))\n",
    "        #print('A priori Token of interest:', tokenized_text[indexes_masked_tmp[index_batch][0]:indexes_masked_tmp[index_batch][1]])\n",
    "            \n",
    "        for key_, value in mapping.items(): \n",
    "            if indexes_masked_tmp[index_batch][0] in value:\n",
    "                key = key_\n",
    "                \n",
    "        #print(key)\n",
    "        #print('dimension match:', len(tokenized_text)==hidden_states_activations_.shape[1])\n",
    "        tmp = ' '.join([tokenizer.decode(tokenizer.convert_tokens_to_ids([tokenized_text[word] for word in mapping[index]])) for index in range(key, len(mapping.keys()) - 1)])\n",
    "        tmp = tmp.replace('  ', ' ').strip()\n",
    "        #print('Extracting sentence:')\n",
    "        print(tmp)\n",
    "        output.append(tmp)\n",
    "\n",
    "        for word_index in range(key, len(mapping.keys()) - 1):\n",
    "            word_activation = []\n",
    "            word_activation.append([hidden_states_activations_[:,index, :] for index in mapping[word_index]])\n",
    "            word_activation = np.vstack(word_activation)\n",
    "            new_activations.append(np.mean(word_activation, axis=0).reshape(1,-1))\n",
    "\n",
    "        \n",
    "        #print(np.vstack(new_activations).shape)\n",
    "        #if input()!='':\n",
    "        #    break\n",
    "\n",
    "\n",
    "assert ' '.join(output) == ' '.join(iterator_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once\n",
      ",\n",
      "when\n",
      "I\n",
      "was\n",
      "six\n",
      "years\n",
      "old\n",
      ",\n",
      "I\n",
      "saw\n",
      "a\n",
      "magnificent\n",
      "picture\n",
      "in\n",
      "a\n",
      "book\n",
      "about\n",
      "the\n",
      "primeval\n",
      "forest\n",
      "called\n",
      "‘\n",
      "Real\n",
      "-\n",
      "life\n",
      "Stories\n",
      ".\n",
      "’\n",
      "It\n",
      "showed\n",
      "a\n",
      "boa\n",
      "constrictor\n",
      "swallowing\n",
      "a\n",
      "wild\n",
      "animal\n",
      ".\n",
      "Here\n",
      "is\n",
      "a\n",
      "copy\n",
      "of\n",
      "the\n",
      "drawing\n",
      ".\n",
      "It\n",
      "said\n",
      "in\n",
      "the\n",
      "book\n",
      ":\n",
      "“\n",
      "Boa\n",
      "constrictors\n",
      "swallow\n",
      "their\n",
      "prey\n",
      "whole\n",
      ",\n",
      "without\n",
      "chewing\n",
      ".\n",
      "Then\n",
      "they\n",
      "are\n",
      "not\n",
      "able\n",
      "to\n",
      "move\n",
      ",\n",
      "and\n",
      "they\n",
      "sleep\n",
      "for\n",
      "the\n",
      "six\n",
      "months\n",
      "it\n",
      "takes\n",
      "for\n",
      "digestion\n",
      ".\n",
      "”\n",
      "So\n",
      "I\n",
      "thought\n",
      "a\n",
      "lot\n",
      "about\n",
      "the\n",
      "adventures\n",
      "of\n",
      "the\n",
      "jungle\n",
      "and\n",
      ",\n",
      "in\n",
      "turn\n",
      ",\n",
      "I\n",
      "managed\n",
      ",\n",
      "with\n",
      "a\n",
      "coloured\n",
      "pencil\n",
      ",\n",
      "to\n",
      "make\n",
      "my\n",
      "first\n",
      "drawing\n",
      ".\n",
      "My\n",
      "Drawing\n",
      "Number\n",
      "one\n",
      ".\n",
      "It\n",
      "looked\n",
      "like\n",
      "this\n",
      ":\n",
      "I\n",
      "showed\n",
      "my\n",
      "masterpiece\n",
      "to\n",
      "the\n",
      "grownups\n",
      "and\n",
      "I\n",
      "asked\n",
      "them\n",
      "if\n",
      "my\n",
      "drawing\n",
      "frightened\n",
      "them\n",
      ".\n",
      "They\n",
      "answered\n",
      "me\n",
      ":\n",
      "“\n",
      "Why\n",
      "would\n",
      "anyone\n",
      "be\n",
      "frightened\n",
      "by\n",
      "a\n",
      "hat\n",
      "?\n",
      "”\n",
      "My\n",
      "drawing\n",
      "was\n",
      "not\n",
      "of\n",
      "a\n",
      "hat\n",
      ".\n",
      "It\n",
      "showed\n",
      "a\n",
      "boa\n",
      "constrictor\n",
      "digesting\n",
      "an\n",
      "elephant\n",
      ".\n",
      "I\n",
      "then\n",
      "drew\n",
      "the\n",
      "inside\n",
      "of\n",
      "the\n",
      "boa\n",
      "constrictor\n",
      ",\n",
      "so\n",
      "that\n",
      "the\n",
      "grownups\n",
      "could\n",
      "understand\n",
      ".\n",
      "They\n",
      "always\n",
      "need\n",
      "to\n",
      "have\n",
      "things\n",
      "explained\n",
      ".\n",
      "My\n",
      "Drawing\n",
      "Number\n",
      "two\n",
      "looked\n",
      "like\n",
      "this\n",
      ":\n",
      "The\n",
      "grownups\n",
      "advised\n",
      "me\n",
      "to\n",
      "leave\n",
      "aside\n",
      "drawings\n",
      "of\n",
      "boa\n",
      "constrictors\n",
      ",\n",
      "open\n",
      "or\n",
      "closed\n",
      ",\n",
      "and\n",
      "to\n",
      "apply\n",
      "myself\n",
      "instead\n",
      "to\n",
      "geography\n",
      ",\n",
      "history\n",
      ",\n",
      "arithmetic\n",
      "and\n",
      "grammar\n",
      ".\n",
      "Thus\n",
      "I\n",
      "abandoned\n",
      ",\n",
      "at\n",
      "the\n",
      "age\n",
      "of\n",
      "six\n",
      ",\n",
      "a\n",
      "magnificent\n",
      "career\n",
      "as\n",
      "a\n",
      "painter\n",
      ".\n",
      "I\n",
      "was\n",
      "discouraged\n",
      "by\n",
      "the\n",
      "failure\n",
      "of\n",
      "my\n",
      "Drawing\n",
      "Number\n",
      "one\n",
      "and\n",
      "of\n",
      "my\n",
      "Drawing\n",
      "Number\n",
      "two\n",
      ".\n",
      "Grownups\n",
      "never\n",
      "understand\n",
      "anything\n",
      "by\n",
      "themselves\n",
      ",\n",
      "and\n",
      "it\n",
      "’\n",
      "s\n",
      "tiresome\n",
      "for\n",
      "children\n",
      "to\n",
      "always\n",
      "explain\n",
      "things\n",
      "for\n",
      "them\n",
      "again\n",
      "and\n",
      "again\n",
      ".\n",
      "So\n",
      "I\n",
      "had\n",
      "to\n",
      "choose\n",
      "another\n",
      "profession\n",
      ",\n",
      "and\n",
      "I\n",
      "learned\n",
      "to\n",
      "fly\n",
      "airplanes\n",
      ".\n",
      "I\n",
      "flew\n",
      "a\n",
      "little\n",
      "in\n",
      "many\n",
      "places\n",
      "around\n",
      "the\n",
      "world\n",
      ".\n",
      "And\n",
      "geography\n",
      ",\n",
      "it\n",
      "'\n",
      "s\n",
      "true\n",
      ",\n",
      "has\n",
      "served\n",
      "me\n",
      "well\n",
      ".\n",
      "I\n",
      "could\n",
      "recognize\n",
      ",\n",
      "at\n",
      "first\n",
      "glance\n",
      ",\n",
      "China\n",
      "from\n",
      "Arizona\n",
      ".\n",
      "It\n",
      "’\n",
      "s\n",
      "very\n",
      "useful\n",
      "if\n",
      "you\n",
      "get\n",
      "lost\n",
      "at\n",
      "night\n",
      ".\n",
      "I\n",
      "have\n",
      "had\n",
      ",\n",
      "during\n",
      "my\n",
      "life\n",
      ",\n",
      "a\n",
      "lot\n",
      "of\n",
      "contact\n",
      "with\n",
      "many\n",
      "persons\n",
      "of\n",
      "consequence\n",
      ".\n",
      "I\n",
      "have\n",
      "lived\n",
      "a\n",
      "lot\n",
      "amongst\n",
      "the\n",
      "grownups\n",
      ".\n",
      "I\n",
      "have\n",
      "seen\n",
      "them\n",
      "from\n",
      "close\n",
      "up\n",
      ".\n",
      "It\n",
      "hasnt\n",
      "much\n",
      "improved\n",
      "my\n",
      "opinion\n",
      "of\n",
      "them\n",
      ".\n",
      "Whenever\n",
      "I\n",
      "met\n",
      "one\n",
      "of\n",
      "them\n",
      "that\n",
      "seemed\n",
      "a\n",
      "bit\n",
      "more\n",
      "clear\n",
      "-\n",
      "sighted\n",
      ",\n",
      "I\n",
      "tried\n",
      "the\n",
      "experiment\n",
      "of\n",
      "showing\n",
      "them\n",
      "my\n",
      "Drawing\n",
      "Number\n",
      "one\n",
      ",\n",
      "that\n",
      "I\n",
      "'\n",
      "ve\n",
      "always\n",
      "kept\n",
      ".\n",
      "I\n",
      "wanted\n",
      "to\n",
      "know\n",
      "if\n",
      "they\n",
      "were\n",
      "really\n",
      "a\n",
      "person\n",
      "of\n",
      "true\n",
      "understanding\n",
      ".\n",
      "But\n",
      "they\n",
      "always\n",
      "responded\n",
      ":\n",
      "“\n",
      "It\n",
      "'\n",
      "s\n",
      "a\n",
      "hat\n",
      ".\n",
      "”\n",
      "So\n",
      "I\n",
      "would\n",
      "never\n",
      "speak\n",
      "to\n",
      "them\n",
      "of\n",
      "boa\n",
      "constrictors\n",
      ",\n",
      "nor\n",
      "of\n",
      "primeval\n",
      "forests\n",
      ",\n",
      "nor\n",
      "of\n",
      "the\n",
      "stars\n",
      ".\n",
      "I\n",
      "put\n",
      "myself\n",
      "at\n",
      "their\n",
      "level\n",
      ".\n",
      "I\n",
      "talked\n",
      "to\n",
      "them\n",
      "about\n",
      "bridge\n",
      ",\n",
      "golf\n",
      ",\n",
      "politics\n",
      "and\n",
      "neckties\n",
      ".\n",
      "And\n",
      "the\n",
      "grownup\n",
      "was\n",
      "glad\n",
      "to\n",
      "know\n",
      "such\n",
      "a\n",
      "sensible\n",
      "man\n",
      ".\n",
      "So\n",
      "I\n",
      "lived\n",
      "alone\n",
      ",\n",
      "without\n",
      "anyone\n",
      "I\n",
      "could\n",
      "really\n",
      "talk\n",
      "to\n",
      ",\n",
      "until\n",
      "a\n",
      "breakdown\n",
      "in\n",
      "the\n",
      "Sahara\n",
      "desert\n",
      ",\n",
      "six\n",
      "years\n",
      "ago\n",
      ".\n",
      "Something\n",
      "had\n",
      "broken\n",
      "in\n",
      "my\n",
      "engine\n",
      ".\n",
      "And\n",
      "as\n",
      "I\n",
      "had\n",
      "with\n",
      "me\n",
      "neither\n",
      "a\n",
      "mechanic\n",
      "nor\n",
      "any\n",
      "passengers\n",
      ",\n",
      "I\n",
      "readied\n",
      "myself\n",
      "to\n",
      "try\n",
      "and\n",
      "carry\n",
      "out\n",
      ",\n",
      "all\n",
      "alone\n",
      ",\n",
      "the\n",
      "difficult\n",
      "repairs\n",
      ".\n",
      "For\n",
      "me\n",
      "it\n",
      "was\n",
      "a\n",
      "matter\n",
      "of\n",
      "life\n",
      "or\n",
      "death\n",
      ".\n",
      "I\n",
      "had\n",
      "hardly\n",
      "enough\n",
      "water\n",
      "to\n",
      "drink\n",
      "for\n",
      "a\n",
      "week\n",
      ".\n",
      "The\n",
      "first\n",
      "night\n",
      "I\n",
      "went\n",
      "to\n",
      "sleep\n",
      "on\n",
      "the\n",
      "sand\n",
      ",\n",
      "a\n",
      "thousand\n",
      "miles\n",
      "from\n",
      "any\n",
      "human\n",
      "habitation\n",
      ".\n",
      "I\n",
      "was\n",
      "more\n",
      "isolated\n",
      "than\n",
      "a\n",
      "shipwrecked\n",
      "sailor\n",
      "on\n",
      "a\n",
      "raft\n",
      "in\n",
      "the\n",
      "middle\n",
      "of\n",
      "the\n",
      "ocean\n",
      ".\n",
      "So\n",
      "you\n",
      "can\n",
      "imagine\n",
      "my\n",
      "surprise\n",
      "when\n",
      "at\n",
      "daybreak\n",
      ",\n",
      "a\n",
      "funny\n",
      "little\n",
      "voice\n",
      "woke\n",
      "me\n",
      "up\n",
      ".\n",
      "It\n",
      "said\n",
      ":\n",
      "“\n",
      "Please\n",
      "...\n",
      "draw\n",
      "me\n",
      "a\n",
      "sheep\n",
      "!\n",
      "”\n",
      "“\n",
      "What\n",
      "?\n",
      "”\n",
      "“\n",
      "Draw\n",
      "me\n",
      "a\n",
      "sheep\n",
      "!\n",
      "”\n",
      "I\n",
      "jumped\n",
      "to\n",
      "my\n",
      "feet\n",
      "as\n",
      "if\n",
      "I\n",
      "’\n",
      "d\n",
      "been\n",
      "struck\n",
      "by\n",
      "lightning\n",
      ".\n",
      "I\n",
      "rubbed\n",
      "my\n",
      "eyes\n",
      ".\n",
      "I\n",
      "took\n",
      "a\n",
      "good\n",
      "look\n",
      "around\n",
      "me\n",
      ".\n",
      "And\n",
      "I\n",
      "saw\n",
      "a\n",
      "quite\n",
      "extraordinary\n",
      "little\n",
      "man\n",
      ",\n",
      "who\n",
      "was\n",
      "examining\n",
      "me\n",
      "seriously\n",
      ".\n",
      "Here\n",
      "is\n",
      "the\n",
      "best\n",
      "portrait\n",
      "that\n",
      ",\n",
      "later\n",
      ",\n",
      "I\n",
      "managed\n",
      "to\n",
      "do\n",
      "of\n",
      "him\n",
      ".\n",
      "But\n",
      "my\n",
      "drawing\n",
      ",\n",
      "of\n",
      "course\n",
      ",\n",
      "is\n",
      "much\n",
      "less\n",
      "charming\n",
      "than\n",
      "its\n",
      "model\n",
      ".\n",
      "It\n",
      "'\n",
      "s\n",
      "not\n",
      "my\n",
      "fault\n",
      ".\n",
      "I\n",
      "was\n",
      "discouraged\n",
      "in\n",
      "my\n",
      "career\n",
      "as\n",
      "a\n",
      "painter\n",
      "by\n",
      "the\n",
      "grownups\n",
      ",\n",
      "at\n",
      "the\n",
      "age\n",
      "of\n",
      "six\n",
      ",\n",
      "and\n",
      "I\n",
      "hadn\n",
      "'\n",
      "t\n",
      "learned\n",
      "to\n",
      "draw\n",
      "anything\n",
      "except\n",
      "boa\n",
      "constrictors\n",
      ",\n",
      "closed\n",
      "and\n",
      "open\n",
      ".\n",
      "I\n",
      "stared\n",
      "at\n",
      "this\n",
      "sudden\n",
      "apparition\n",
      "wide\n",
      "eyed\n",
      "with\n",
      "astonishment\n",
      ".\n",
      "Remember\n",
      "that\n",
      "I\n",
      "was\n",
      "a\n",
      "thousand\n",
      "miles\n",
      "from\n",
      "any\n",
      "inhabited\n",
      "region\n",
      ".\n",
      "And\n",
      "yet\n",
      "this\n",
      "little\n",
      "fellow\n",
      "seemed\n",
      "neither\n",
      "lost\n",
      ",\n",
      "nor\n",
      "half\n",
      "-\n",
      "dead\n",
      "with\n",
      "fatigue\n",
      ",\n",
      "nor\n",
      "starved\n",
      "or\n",
      "dying\n",
      "of\n",
      "thirst\n",
      "or\n",
      "fear\n",
      ".\n",
      "He\n",
      "looked\n",
      "nothing\n",
      "like\n",
      "a\n",
      "child\n",
      "lost\n",
      "in\n",
      "the\n",
      "middle\n",
      "of\n",
      "the\n",
      "desert\n",
      ",\n",
      "a\n",
      "thousand\n",
      "miles\n",
      "from\n",
      "any\n",
      "inhabited\n",
      "region\n",
      ".\n",
      "When\n",
      "I\n",
      "finally\n",
      "managed\n",
      "to\n",
      "speak\n",
      ",\n",
      "I\n",
      "said\n",
      ":\n",
      "“\n",
      "But\n",
      "—\n",
      "what\n",
      "are\n",
      "you\n",
      "doing\n",
      "here\n",
      "?\n",
      "”\n",
      "And\n",
      "he\n",
      "repeated\n",
      ",\n",
      "very\n",
      "slowly\n",
      ",\n",
      "as\n",
      "if\n",
      "it\n",
      "was\n",
      "something\n",
      "very\n",
      "serious\n",
      ":\n",
      "“\n",
      "Please\n",
      "...\n",
      "draw\n",
      "me\n",
      "a\n",
      "sheep\n",
      "...\n",
      "”\n",
      "When\n",
      "a\n",
      "mystery\n",
      "is\n",
      "too\n",
      "overpowering\n",
      ",\n",
      "one\n",
      "dare\n",
      "not\n",
      "disobey\n",
      ".\n",
      "Absurd\n",
      "as\n",
      "it\n",
      "seemed\n",
      "to\n",
      "me\n",
      "a\n",
      "thousand\n",
      "miles\n",
      "from\n",
      "any\n",
      "human\n",
      "habitation\n",
      "and\n",
      "in\n",
      "danger\n",
      "of\n",
      "death\n",
      ",\n",
      "I\n",
      "took\n",
      "out\n",
      "of\n",
      "my\n",
      "pocket\n",
      "a\n",
      "sheet\n",
      "of\n",
      "paper\n",
      "and\n",
      "a\n",
      "pen\n",
      ".\n",
      "But\n",
      "then\n",
      "I\n",
      "remembered\n",
      "that\n",
      "I\n",
      "had\n",
      "mainly\n",
      "studied\n",
      "geography\n",
      ",\n",
      "history\n",
      ",\n",
      "arithmetic\n",
      "and\n",
      "grammar\n",
      ",\n",
      "and\n",
      "I\n",
      "told\n",
      "the\n",
      "little\n",
      "fellow\n",
      "(\n",
      "a\n",
      "little\n",
      "crossly\n",
      ")\n",
      "that\n",
      "I\n",
      "didn\n",
      "’\n",
      "t\n",
      "know\n",
      "how\n",
      "to\n",
      "draw\n",
      ".\n",
      "He\n",
      "replied\n",
      ":\n",
      "“\n",
      "It\n",
      "doesn\n",
      "'\n",
      "t\n",
      "matter\n",
      ".\n",
      "Draw\n",
      "me\n",
      "a\n",
      "sheep\n",
      ".\n",
      "”\n",
      "As\n",
      "I\n",
      "’\n",
      "d\n",
      "never\n",
      "drawn\n",
      "a\n",
      "sheep\n",
      ",\n",
      "I\n",
      "redrew\n",
      "for\n",
      "him\n",
      "one\n",
      "of\n",
      "the\n",
      "only\n",
      "two\n",
      "drawings\n",
      "that\n",
      "I\n",
      "was\n",
      "capable\n",
      "of\n",
      ".\n",
      "The\n",
      "one\n",
      "of\n",
      "the\n",
      "closed\n",
      "boa\n",
      "constrictor\n",
      ".\n",
      "And\n",
      "I\n",
      "was\n",
      "astounded\n",
      "to\n",
      "hear\n",
      "the\n",
      "little\n",
      "fellow\n",
      "respond\n",
      ":\n",
      "“\n",
      "No\n",
      "!\n",
      "No\n",
      "!\n",
      "I\n",
      "don\n",
      "’\n",
      "t\n",
      "want\n",
      "an\n",
      "elephant\n",
      "inside\n",
      "a\n",
      "boa\n",
      "constrictor\n",
      ".\n",
      "A\n",
      "boa\n",
      "constrictor\n",
      "is\n",
      "very\n",
      "dangerous\n",
      ",\n",
      "and\n",
      "an\n",
      "elephant\n",
      "is\n",
      "very\n",
      "cumbersome\n",
      ".\n",
      "Where\n",
      "I\n",
      "live\n",
      "everything\n",
      "is\n",
      "very\n",
      "small\n",
      ".\n",
      "I\n",
      "need\n",
      "a\n",
      "sheep\n",
      ".\n",
      "Draw\n",
      "me\n",
      "a\n",
      "sheep\n",
      ".\n",
      "”\n",
      "So\n",
      "I\n",
      "drew\n",
      ".\n",
      "He\n",
      "looked\n",
      "carefully\n",
      ",\n",
      "then\n",
      "said\n",
      ":\n",
      "“\n",
      "No\n",
      "!\n",
      "This\n",
      "one\n",
      "’\n",
      "s\n",
      "already\n",
      "very\n",
      "sick\n",
      ".\n",
      "Make\n",
      "another\n",
      "one\n",
      ".\n",
      "”\n",
      "I\n",
      "drew\n",
      "again\n",
      ":\n",
      "My\n",
      "friend\n",
      "smiled\n",
      "gently\n",
      "and\n",
      "indulgently\n",
      ":\n",
      "“\n",
      "You\n",
      "can\n",
      "see\n",
      "yourself\n",
      "...\n",
      "this\n",
      "isn\n",
      "’\n",
      "t\n",
      "a\n",
      "sheep\n",
      ",\n",
      "it\n",
      "'\n",
      "s\n",
      "a\n",
      "ram\n",
      ".\n",
      "It\n",
      "has\n",
      "horns\n",
      "...\n",
      "“\n",
      "So\n",
      "once\n",
      "again\n",
      "I\n",
      "redid\n",
      "my\n",
      "drawing\n",
      ":\n",
      "But\n",
      "it\n",
      "was\n",
      "rejected\n",
      ",\n",
      "like\n",
      "the\n",
      "previous\n",
      "ones\n",
      ":\n",
      "“\n",
      "This\n",
      "one\n",
      "’\n",
      "s\n",
      "too\n",
      "old\n",
      ".\n",
      "I\n",
      "want\n",
      "a\n",
      "sheep\n",
      "that\n",
      "will\n",
      "live\n",
      "a\n",
      "long\n",
      "time\n",
      ".\n",
      "”\n",
      "So\n",
      ",\n",
      "getting\n",
      "impatient\n",
      ",\n",
      "as\n",
      "I\n",
      "was\n",
      "eager\n",
      "to\n",
      "start\n",
      "dismantling\n",
      "my\n",
      "engine\n",
      ",\n",
      "I\n",
      "hastily\n",
      "sketched\n",
      "this\n",
      "drawing\n",
      ":\n",
      "And\n",
      "I\n",
      "snapped\n",
      ":\n",
      "“\n",
      "This\n",
      "here\n",
      "is\n",
      "the\n",
      "box\n",
      ".\n",
      "The\n",
      "sheep\n",
      "you\n",
      "want\n",
      "is\n",
      "inside\n",
      ".\n",
      "”\n",
      "But\n",
      "I\n",
      "was\n",
      "very\n",
      "surprised\n",
      "to\n",
      "see\n",
      "the\n",
      "face\n",
      "of\n",
      "my\n",
      "young\n",
      "judge\n",
      "light\n",
      "up\n",
      ":\n",
      "“\n",
      "It\n",
      "'\n",
      "s\n",
      "exactly\n",
      "the\n",
      "way\n",
      "I\n",
      "wanted\n",
      "!\n",
      "Do\n",
      "you\n",
      "think\n",
      "this\n",
      "sheep\n",
      "needs\n",
      "a\n",
      "lot\n",
      "of\n",
      "grass\n",
      "?\n",
      "”\n",
      "“\n",
      "Why\n",
      "?\n",
      "”\n",
      "“\n",
      "Because\n",
      "where\n",
      "I\n",
      "'\n",
      "m\n",
      "from\n",
      "everything\n",
      "is\n",
      "very\n",
      "small\n",
      "...\n",
      "”\n",
      "“\n",
      "There\n",
      "will\n",
      "certainly\n",
      "be\n",
      "enough\n",
      ".\n",
      "I\n",
      "gave\n",
      "you\n",
      "a\n",
      "very\n",
      "small\n",
      "sheep\n",
      ".\n",
      "”\n",
      "He\n",
      "leaned\n",
      "his\n",
      "head\n",
      "towards\n",
      "the\n",
      "drawing\n",
      ":\n",
      "“\n",
      "Not\n",
      "that\n",
      "small\n",
      "...\n",
      "Look\n",
      "!\n",
      "He\n",
      "'\n",
      "s\n",
      "fallen\n",
      "asleep\n",
      "...\n",
      "”\n",
      "And\n",
      "that\n",
      "'\n",
      "s\n",
      "how\n",
      "I\n",
      "met\n",
      "the\n",
      "little\n",
      "prince\n",
      ".\n",
      "It\n",
      "took\n",
      "me\n",
      "a\n",
      "long\n",
      "time\n",
      "to\n",
      "find\n",
      "out\n",
      "where\n",
      "he\n",
      "came\n",
      "from\n",
      ".\n",
      "The\n",
      "little\n",
      "prince\n",
      ",\n",
      "who\n",
      "asked\n",
      "me\n",
      "many\n",
      "questions\n",
      ",\n",
      "never\n",
      "seemed\n",
      "to\n",
      "hear\n",
      "my\n",
      "own\n",
      ".\n",
      "It\n",
      "was\n",
      "the\n",
      "words\n",
      "spoken\n",
      "by\n",
      "chance\n",
      "that\n",
      ",\n",
      "little\n",
      "by\n",
      "little\n",
      ",\n",
      "revealed\n",
      "everything\n",
      "to\n",
      "me\n",
      ".\n",
      "So\n",
      ",\n",
      "when\n",
      "he\n",
      "saw\n",
      "my\n",
      "airplane\n",
      "for\n",
      "the\n",
      "first\n",
      "time\n",
      "(\n",
      "I\n",
      "won\n",
      "’\n",
      "t\n",
      "draw\n",
      "my\n",
      "airplane\n",
      ",\n",
      "it\n",
      "would\n",
      "be\n",
      "a\n",
      "drawing\n",
      "far\n",
      "too\n",
      "complicated\n",
      "for\n",
      "me\n",
      ")\n",
      ",\n",
      "he\n",
      "asked\n",
      "me\n",
      ":\n",
      "“\n",
      "What\n",
      "'\n",
      "s\n",
      "that\n",
      "thing\n",
      "there\n",
      "?\n",
      "”\n",
      "“\n",
      "It\n",
      "'\n",
      "s\n",
      "not\n",
      "a\n",
      "thing\n",
      ".\n",
      "It\n",
      "flies\n",
      ".\n",
      "It\n",
      "'\n",
      "s\n",
      "an\n",
      "airplane\n",
      ".\n",
      "It\n",
      "’\n",
      "s\n",
      "my\n",
      "airplane\n",
      ".\n",
      "”\n",
      "And\n",
      "I\n",
      "was\n",
      "proud\n",
      "to\n",
      "have\n",
      "him\n",
      "know\n",
      "that\n",
      "I\n",
      "could\n",
      "fly\n",
      ".\n",
      "Then\n",
      "he\n",
      "cried\n",
      ":\n",
      "“\n",
      "What\n",
      "?\n",
      "You\n",
      "fell\n",
      "from\n",
      "the\n",
      "sky\n",
      "!\n",
      "”\n",
      "“\n",
      "Yes\n",
      ",\n",
      "”\n",
      "I\n",
      "said\n",
      "modestly\n",
      ".\n",
      "“\n",
      "Oh\n",
      "!\n",
      "That\n",
      "'\n",
      "s\n",
      "funny\n",
      "!\n",
      "...\n",
      "”\n",
      "And\n",
      "the\n",
      "little\n",
      "prince\n",
      "broke\n",
      "into\n",
      "a\n",
      "lovely\n",
      "peal\n",
      "of\n",
      "laughter\n",
      ",\n",
      "which\n",
      "irritated\n",
      "me\n",
      "very\n",
      "much\n",
      ".\n",
      "I\n",
      "prefer\n",
      "people\n",
      "to\n",
      "take\n",
      "my\n",
      "misfortunes\n",
      "seriously\n",
      ".\n",
      "Then\n",
      "he\n",
      "added\n",
      ":\n",
      "“\n",
      "So\n",
      ",\n",
      "you\n",
      "also\n",
      "come\n",
      "from\n",
      "the\n",
      "sky\n",
      "!\n",
      "What\n",
      "planet\n",
      "are\n",
      "you\n",
      "from\n",
      "?\n",
      "”\n",
      "I\n",
      "caught\n",
      "a\n",
      "glimpse\n",
      "into\n",
      "the\n",
      "mystery\n",
      "of\n",
      "his\n",
      "presence\n",
      ",\n",
      "and\n",
      "I\n",
      "asked\n",
      "abruptly\n",
      ":\n",
      "“\n",
      "So\n",
      "you\n",
      "come\n",
      "from\n",
      "another\n",
      "planet\n",
      "then\n",
      "?\n",
      "”\n",
      "But\n",
      "he\n",
      "didn\n",
      "’\n",
      "t\n",
      "answer\n",
      ".\n",
      "He\n",
      "shook\n",
      "his\n",
      "head\n",
      "slowly\n",
      "whilst\n",
      "looking\n",
      "at\n",
      "my\n",
      "airplane\n",
      ":\n",
      "“\n",
      "It\n",
      "'\n",
      "s\n",
      "true\n",
      "that\n",
      "you\n",
      "can\n",
      "'\n",
      "t\n",
      "have\n",
      "come\n",
      "from\n",
      "far\n",
      "away\n",
      "in\n",
      "that\n",
      "thing\n",
      "...\n",
      "”\n",
      "And\n",
      "he\n",
      "drifted\n",
      "into\n",
      "a\n",
      "daydream\n",
      "which\n",
      "lasted\n",
      "a\n",
      "long\n",
      "while\n",
      ".\n",
      "Then\n",
      ",\n",
      "taking\n",
      "my\n",
      "sheep\n",
      "out\n",
      "of\n",
      "his\n",
      "pocket\n",
      ",\n",
      "he\n",
      "sank\n",
      "himself\n",
      "into\n",
      "the\n",
      "contemplation\n",
      "of\n",
      "his\n",
      "treasure\n",
      ".\n",
      "You\n",
      "can\n",
      "imagine\n",
      "how\n",
      "my\n",
      "curiosity\n",
      "was\n",
      "aroused\n",
      "by\n",
      "this\n",
      "small\n",
      "disclosure\n",
      "about\n",
      "‘\n",
      "the\n",
      "other\n",
      "planets\n",
      ".\n",
      "’\n",
      "So\n",
      "I\n",
      "tried\n",
      "to\n",
      "find\n",
      "out\n",
      "more\n",
      ":\n",
      "“\n",
      "Where\n",
      "are\n",
      "you\n",
      "from\n",
      "my\n",
      "little\n",
      "fellow\n",
      "?\n",
      "Where\n",
      "’\n",
      "s\n",
      "this\n",
      "‘\n",
      "where\n",
      "I\n",
      "live\n",
      "’\n",
      "of\n",
      "yours\n",
      "?\n",
      "Where\n",
      "do\n",
      "you\n",
      "take\n",
      "my\n",
      "sheep\n",
      "off\n",
      "to\n",
      "?\n",
      "”\n",
      "After\n",
      "a\n",
      "reflective\n",
      "silence\n",
      "he\n",
      "answered\n",
      ":\n",
      "“\n",
      "What\n",
      "'\n",
      "s\n",
      "good\n",
      "about\n",
      "the\n",
      "box\n",
      "you\n",
      "’\n",
      "ve\n",
      "given\n",
      "me\n",
      "is\n",
      "that\n",
      "at\n",
      "night\n",
      ",\n",
      "he\n",
      "can\n",
      "use\n",
      "it\n",
      "as\n",
      "a\n",
      "house\n",
      ".\n",
      "”\n",
      "“\n",
      "That\n",
      "’\n",
      "s\n",
      "right\n",
      ".\n",
      "And\n",
      "if\n",
      "you\n",
      "’\n",
      "re\n",
      "good\n",
      ",\n",
      "I\n",
      "'\n",
      "ll\n",
      "give\n",
      "you\n",
      "a\n",
      "rope\n",
      "to\n",
      "tie\n",
      "him\n",
      "up\n",
      "with\n",
      "during\n",
      "the\n",
      "day\n",
      ".\n",
      "And\n",
      "a\n",
      "stake\n",
      ".\n",
      "”\n",
      "The\n",
      "offer\n",
      "seemed\n",
      "to\n",
      "shock\n",
      "the\n",
      "little\n",
      "prince\n",
      ":\n",
      "“\n",
      "Tie\n",
      "him\n",
      "up\n",
      "?\n",
      "What\n",
      "a\n",
      "funny\n",
      "idea\n",
      "!\n",
      "”\n",
      "“\n",
      "But\n",
      "if\n",
      "you\n",
      "don\n",
      "'\n",
      "t\n",
      "tie\n",
      "him\n",
      "up\n",
      ",\n",
      "he\n",
      "’\n",
      "ll\n",
      "wander\n",
      "off\n",
      ",\n",
      "and\n",
      "get\n",
      "lost\n",
      ".\n",
      "”\n",
      "My\n",
      "friend\n",
      "broke\n",
      "into\n",
      "another\n",
      "peal\n",
      "of\n",
      "laughter\n",
      ":\n",
      "“\n",
      "Where\n",
      "do\n",
      "you\n",
      "think\n",
      "he\n",
      "’\n",
      "d\n",
      "go\n",
      "!\n",
      "”\n",
      "“\n",
      "Anywhere\n",
      ".\n",
      "Straight\n",
      "ahead\n",
      "...\n",
      "”\n",
      "Then\n",
      "the\n",
      "little\n",
      "prince\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said\n",
      "gravely\n",
      ":\n",
      "“\n",
      "That\n",
      "doesn\n",
      "’\n",
      "t\n",
      "matter\n",
      ";\n",
      "where\n",
      "I\n",
      "live\n",
      ",\n",
      "everything\n",
      "is\n",
      "so\n",
      "small\n",
      "!\n",
      "”\n",
      "And\n",
      "perhaps\n",
      "with\n",
      "a\n",
      "hint\n",
      "of\n",
      "sadness\n",
      ",\n",
      "he\n",
      "added\n",
      ":\n",
      "“\n",
      "Straight\n",
      "ahead\n",
      "you\n",
      "can\n",
      "'\n",
      "t\n",
      "go\n",
      "far\n",
      "...\n",
      "”\n"
     ]
    }
   ],
   "source": [
    "# activation generation shuffle\n",
    "output = []\n",
    "for index_batch, batch in enumerate(batches_shuffle):\n",
    "    batch = batch.strip() # Remove trailing character\n",
    "\n",
    "    batch = '<|endoftext|> ' + batch + ' <|endoftext|>'\n",
    "    tokenized_text = tokenizer.tokenize(batch, add_prefix_space=False)\n",
    "    inputs_ids = torch.tensor([tokenizer.convert_tokens_to_ids(tokenized_text)])\n",
    "\n",
    "    #print('input shape: ', inputs_ids.shape)\n",
    "    #print(batch)\n",
    "    #print(tokenized_text)\n",
    "    \n",
    "    mapping = utils.match_tokenized_to_untokenized(tokenized_text, batch)\n",
    "\n",
    "    #print('Mapping:')\n",
    "    #for key in mapping.keys():\n",
    "    #    print(batch.split()[key], ''.join([tokenized_text[i] for i in mapping[key]]))\n",
    "    #print('A priori Token of interest:', tokenized_text[indexes_shuffle[index_batch][0]:indexes_shuffle[index_batch][1]])\n",
    "         \n",
    "    with torch.no_grad():\n",
    "        encoded_layers = extractor_shuffle.model(inputs_ids) # last_hidden_state, pooler_output, hidden_states, attentions\n",
    "\n",
    "        hidden_states_activations_ = np.vstack(encoded_layers[2]) # retrieve all the hidden states (dimension = layer_count * len(tokenized_text) * feature_count)\n",
    "\n",
    "        #print('nb of layer:', len(encoded_layers[2]))\n",
    "        #print('output shape:', hidden_states_activations_.shape)\n",
    "        \n",
    "        new_activations = []\n",
    "        key_start = None\n",
    "        key_stop = None\n",
    "        \n",
    "        #print('Mapping:')\n",
    "        #for key in mapping.keys():\n",
    "        #    print(batch.split()[key], ''.join([tokenized_text[i] for i in mapping[key]]))\n",
    "        #print('A priori Token of interest:', tokenized_text[indexes_masked_tmp[index_batch][0]:indexes_masked_tmp[index_batch][1]])\n",
    "            \n",
    "        for key_, value in mapping.items(): \n",
    "            if (value[0] - 1) == (indexes_shuffle[index_batch][0]): #because we added [CLS] token at the beginning\n",
    "                key_start = key_\n",
    "        for key_, value in mapping.items(): \n",
    "            if value[-1] == (indexes_shuffle[index_batch][1]): #because we added [CLS] token at the beginning\n",
    "                key_stop = key_\n",
    "\n",
    "        #print(key)\n",
    "        #print('dimension match:', len(tokenized_text)==hidden_states_activations_.shape[1])\n",
    "        tmp = ' '.join([tokenizer.decode(tokenizer.convert_tokens_to_ids([tokenized_text[word] for word in mapping[index]])) for index in range(key_start, key_stop + 1)])\n",
    "        tmp = tmp.replace('  ', ' ').strip()\n",
    "        #print('Extracting sentence:')\n",
    "        print(tmp)\n",
    "        output.append(tmp)\n",
    "        \n",
    "        for word_index in range(key_start, key_stop + 1):\n",
    "            word_activation = []\n",
    "            word_activation.append([hidden_states_activations_[:,index, :] for index in mapping[word_index]])\n",
    "            word_activation = np.vstack(word_activation)\n",
    "            new_activations.append(np.mean(word_activation, axis=0).reshape(1,-1))\n",
    "\n",
    "        \n",
    "        #print(np.vstack(new_activations).shape)\n",
    "        #if input()!='':\n",
    "        #    break\n",
    "\n",
    "\n",
    "assert ' '.join(output) == ' '.join(iterator_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sentence_and_context(\n",
    "    iterator, \n",
    "    past_context_size, \n",
    "    pretrained_model,\n",
    "    transformation='shuffle',\n",
    "    vocabulary=None,\n",
    "    dictionary=None,\n",
    "    select=None,\n",
    "    seed=1111,\n",
    "    add_prefix_space=True):\n",
    "    \"\"\" Given a list of sentences, for each word, we transform its context outside a certain context window.\n",
    "    Args:\n",
    "        - iterator: list (of str)\n",
    "        - context_size: int\n",
    "        - pretrained_model: str\n",
    "        - vocabulary: list\n",
    "        - dictionary: dict\n",
    "        - seed: int\n",
    "        - add_prefix_space: bool\n",
    "    Returns:\n",
    "        - batch_tmp: list (of str)\n",
    "        - index_tmp: list (of tuple of int)\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    punctuation = ['.', '!', '?', '...', '\\'', ',', ';', ':', '/', '-', '\"', '‘', '’', '(', ')', '{', '}', '[', ']', '`', '“', '”', '—']\n",
    "    if select is None:\n",
    "        words = ' '.join(iterator).split()\n",
    "    else:\n",
    "        words = iterator[select].split()\n",
    "        \n",
    "    all_words = ' '.join(iterator).split()\n",
    "    words_before = [] if select is None else ' '.join(iterator[:select]).split()\n",
    "    supp_before = [len([word for word in all_words[max(j+len(words_before)+1-past_context_size, 0):j+len(words_before)+1] if word in punctuation]) for j in range(len(words))] # we do not count punctuation in the number of words to shuffle\n",
    "\n",
    "    # For each word, we compute the index of the other words to transform\n",
    "    # We transform past context. Change conditions \"i<j\" and ... to something else if needed\n",
    "    index_words_list_before = [[i for i, item in enumerate(all_words) if item not in punctuation if ((i!=(j+len(words_before))) and  (i <= j+len(words_before)-past_context_size-supp_before[j]))] for j in range(len(words))] # '<=' because context_size of 1 is the current word\n",
    "\n",
    "    # Create the new array of sentences with original words \n",
    "    new_words = np.tile(np.array(all_words.copy()), (len(words), 1))\n",
    "\n",
    "    for i in range(len(new_words)):\n",
    "        if len(index_words_list_before[i])>0: # if there are words to change...\n",
    "            if transformation=='shuffle':\n",
    "                # Replace words that need to be shuffled by the random sampling (except fix point and punctuation)\n",
    "                new_order = random.sample(index_words_list_before[i], len(index_words_list_before[i]))\n",
    "                if len(index_words_list_before[i])>1:\n",
    "                    while new_order==index_words_list_before[i]:\n",
    "                        new_order = random.sample(index_words_list_before[i], len(index_words_list_before[i]))\n",
    "                new_words[i, index_words_list_before[i]] = new_words[i, new_order]\n",
    "            elif transformation=='pos_replacement':\n",
    "                # Replace words that need to be replaced by words with same POS (except fix point and punctuation)\n",
    "                new_words[i, index_words_list_before[i]] = pick_pos_word(new_words[i, index_words_list_before[i]], dictionary)\n",
    "            elif transformation=='random_replacement':\n",
    "                # Replace words that need to be replaced by random words (except fix point and punctuation)\n",
    "                new_words[i, index_words_list_before[i]] = pick_random_word(new_words[i, index_words_list_before[i]], vocabulary)\n",
    "\n",
    "    # Convert array to list\n",
    "    new_words = list(new_words)\n",
    "    new_words = [list(item) for item in new_words]\n",
    "    batch_tmp = []\n",
    "    index_tmp = []\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model) # to replace with tokenizer of interest\n",
    "    # adding transformed context to each sentence\n",
    "    for i, sentence in enumerate(new_words):\n",
    "        batch_tmp.append(' '.join(sentence).strip())\n",
    "        # Determining associated indexes\n",
    "        tmp1 = ' '.join(sentence[:i+len(words_before)])\n",
    "        tmp2 = ' '.join(sentence[:i+len(words_before)+1])\n",
    "        index_tmp.append((len(tokenizer.tokenize(tmp1.strip(), add_prefix_space=add_prefix_space)), \n",
    "                     len(tokenizer.tokenize(tmp2.strip(), add_prefix_space=add_prefix_space))\n",
    "                    )) # to replace with tokenizer of interest and arguments\n",
    "    return batch_tmp, index_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entering while loop...\n",
      "1\n",
      "['Once , when I was six years old , I saw a magnificent picture in a book about the primeval forest called ‘ Real - life Stories . ’']\n",
      "result:\n",
      "Once , when I was six years old , I saw a magnificent picture in a book about the primeval forest called ‘ Real - life Stories . ’\n",
      "Once , when I was six years old , I saw a magnificent picture in a book about the primeval forest called ‘ Real - life Stories . ’\n",
      "Once , when I was six years old , I saw a magnificent picture in a book about the primeval forest called ‘ Real - life Stories . ’\n",
      "Once , when I was six years old , I saw a magnificent picture in a book about the primeval forest called ‘ Real - life Stories . ’\n",
      "when , Once I was six years old , I saw a magnificent picture in a book about the primeval forest called ‘ Real - life Stories . ’\n",
      "I , Once when was six years old , I saw a magnificent picture in a book about the primeval forest called ‘ Real - life Stories . ’\n",
      "I , was Once when six years old , I saw a magnificent picture in a book about the primeval forest called ‘ Real - life Stories . ’\n",
      "when , six was Once I years old , I saw a magnificent picture in a book about the primeval forest called ‘ Real - life Stories . ’\n",
      "was , I when six Once years old , I saw a magnificent picture in a book about the primeval forest called ‘ Real - life Stories . ’\n",
      "when , six was years I Once old , I saw a magnificent picture in a book about the primeval forest called ‘ Real - life Stories . ’\n",
      "was , six when Once years I old , I saw a magnificent picture in a book about the primeval forest called ‘ Real - life Stories . ’\n",
      "I , was when six Once I old , years saw a magnificent picture in a book about the primeval forest called ‘ Real - life Stories . ’\n",
      "was , old I when I years six , Once saw a magnificent picture in a book about the primeval forest called ‘ Real - life Stories . ’\n",
      "I , I saw was old six years , Once when a magnificent picture in a book about the primeval forest called ‘ Real - life Stories . ’\n",
      "old , was I a six I saw , when Once magnificent years picture in a book about the primeval forest called ‘ Real - life Stories . ’\n",
      "magnificent , picture when saw years six old , I Once a I was in a book about the primeval forest called ‘ Real - life Stories . ’\n",
      "years , saw I a when Once picture , six I in magnificent was old a book about the primeval forest called ‘ Real - life Stories . ’\n",
      "saw , a I was six a Once , when years picture in magnificent old I book about the primeval forest called ‘ Real - life Stories . ’\n",
      "I , in six was picture Once book , years magnificent I saw a when a old about the primeval forest called ‘ Real - life Stories . ’\n",
      "old , years a about book when saw , six a I I was magnificent picture Once in the primeval forest called ‘ Real - life Stories . ’\n",
      "magnificent , picture about I when six in , old years the I was Once book a saw a primeval forest called ‘ Real - life Stories . ’\n",
      "six , the about in years primeval old , when picture book Once a I a was saw I magnificent forest called ‘ Real - life Stories . ’\n",
      "magnificent , six years was book the I , I Once picture a in a old about primeval saw when forest called ‘ Real - life Stories . ’\n",
      "forest , six was in picture the primeval , I a magnificent about a saw book old Once I years when called ‘ Real - life Stories . ’\n",
      "saw , a a Once in picture I , when primeval magnificent about forest called six book I years the was old ‘ Real - life Stories . ’\n",
      "in , saw magnificent old was about the , called I six a Once book picture when primeval I a forest years ‘ Real - life Stories . ’\n",
      "six , in picture about a Real Once , old a was I saw magnificent book primeval I called years the forest ‘ when - life Stories . ’\n",
      "picture , saw a six years book old , I when was in forest about the I magnificent primeval Real Once a ‘ called - life Stories . ’\n",
      "I , a called saw Once I old , book was picture in the forest six Real when about magnificent primeval years ‘ a - life Stories . ’\n",
      "0\n",
      "\n",
      "2\n",
      "['Once , when I was six years old , I saw a magnificent picture in a book about the primeval forest called ‘ Real - life Stories . ’', 'It showed a boa constrictor swallowing a wild animal .']\n",
      "result:\n",
      "old , Stories picture six Real book I , years when a about magnificent forest primeval I the in called a saw ‘ Once - life was . ’ It showed a boa constrictor swallowing a wild animal .\n",
      "a , picture Stories old six the book , life primeval called when I Real I Once about magnificent in years was ‘ saw - a forest . ’ It showed a boa constrictor swallowing a wild animal .\n",
      "years , called old in magnificent It picture , the I Stories I book Real six a about saw was life primeval ‘ forest - Once when . ’ a showed a boa constrictor swallowing a wild animal .\n",
      "in , Stories I six It called showed , a life Real forest I a magnificent picture when saw years the old ‘ book - Once primeval . ’ about was a boa constrictor swallowing a wild animal .\n",
      "picture , the about life forest was when , years primeval saw showed I Real a It a book in six I ‘ old - Once called . ’ magnificent a Stories boa constrictor swallowing a wild animal .\n",
      "called , magnificent a years a I life , when six picture showed saw boa It old forest was the in Stories ‘ about - Once primeval . ’ I book a Real constrictor swallowing a wild animal .\n",
      "six , saw called the I primeval I , Stories magnificent picture about life boa a forest when old years Real in ‘ constrictor - was Once . ’ It a showed book a swallowing a wild animal .\n",
      "constrictor , Real life six called the showed , about picture saw in was forest a a Once magnificent a It years ‘ Stories - old boa . ’ I when primeval I swallowing book a wild animal .\n",
      "years , old I a book a Once , about swallowing primeval It Real called forest when boa I Stories life was ‘ six - constrictor magnificent . ’ in a a picture showed saw the wild animal .\n",
      "the , swallowing showed Once a saw Real , a a boa in years I primeval six a constrictor book forest was ‘ It - Stories picture . ’ called magnificent old about life I when wild animal .\n",
      "1\n",
      "\n",
      "3\n",
      "['Once , when I was six years old , I saw a magnificent picture in a book about the primeval forest called ‘ Real - life Stories . ’', 'It showed a boa constrictor swallowing a wild animal .', 'Here is a copy of the drawing .']\n",
      "result:\n",
      "old , wild picture showed Real six swallowing , book I the years Stories magnificent was It a forest a called about ‘ Once - boa I . ’ constrictor in a when saw a primeval life animal . Here is a copy of the drawing .\n",
      "constrictor , was about called six Once book , a I animal a It the when boa years old a swallowing I ‘ a - showed magnificent . ’ saw wild picture forest in life Real Stories primeval . Here is a copy of the drawing .\n",
      "boa , a I saw was in Stories , wild six a called constrictor a Here showed It years forest magnificent book ‘ when - Real animal . ’ the a about primeval Once swallowing I picture life . old is a copy of the drawing .\n",
      "old , I magnificent primeval saw a years , I is the forest six a a called wild when boa picture a ‘ Stories - Real constrictor . ’ life Here book was It Once animal in swallowing . showed about a copy of the drawing .\n",
      "It , showed primeval in magnificent forest animal , wild boa I Here Real a Stories constrictor called a six saw I ‘ book - the when . ’ a years a about swallowing old a Once was . picture is life copy of the drawing .\n",
      "wild , old Once book I constrictor Real , life six called the boa about picture saw in was a a is ‘ swallowing - magnificent years . ’ It a Here showed Stories a copy when I . forest a primeval animal of the drawing .\n",
      "picture , Once wild in when copy boa , a a showed life animal Here forest six was is I a about ‘ old - called It . ’ Stories swallowing saw primeval of constrictor a I the . Real years magnificent book a the drawing .\n",
      "six , years saw magnificent picture Real wild , I boa a showed Once animal forest It the Here swallowing in a ‘ is - old when . ’ was about I of Stories constrictor a called book . life a a primeval copy the drawing .\n",
      "2\n",
      "\n",
      "4\n",
      "['Once , when I was six years old , I saw a magnificent picture in a book about the primeval forest called ‘ Real - life Stories . ’', 'It showed a boa constrictor swallowing a wild animal .', 'Here is a copy of the drawing .', 'It said in the book : “ Boa constrictors swallow their prey whole , without chewing .']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:\n",
      "a , in Stories a It swallowing years , is the animal I called magnificent life showed was book six forest Here ‘ saw - when picture . ’ I Real boa the copy constrictor primeval wild a . a Once of old about a drawing . It said in the book : “ Boa constrictors swallow their prey whole , without chewing .\n",
      "swallowing , forest book saw drawing about I , when wild years boa called Stories old in magnificent a picture constrictor Real ‘ the - primeval I . ’ a It of six Here the a a a . showed was animal life Once copy is . It said in the book : “ Boa constrictors swallow their prey whole , without chewing .\n",
      "saw , about showed called a drawing magnificent , copy years of the picture wild a Real Here boa I a is ‘ animal - swallowing It . ’ a was Once Stories It six forest life primeval . in a old when constrictor the I . book said in the book : “ Boa constrictors swallow their prey whole , without chewing .\n",
      "about , It forest Real constrictor magnificent called , said is I six picture a the saw was a in boa old ‘ copy - drawing It . ’ of animal a wild years Here showed life the . book a Stories Once primeval a when . I swallowing in the book : “ Boa constrictors swallow their prey whole , without chewing .\n",
      "primeval , old showed It copy said the , wild of Once book when constrictor Real life six called the a about ‘ picture - saw in . ’ was drawing a Here magnificent boa animal years swallowing . is forest It a a I a . Stories I in the book : “ Boa constrictors swallow their prey whole , without chewing .\n",
      "forest , swallowing picture Once wild in copy , when drawing a was in six animal showed of Stories I a about ‘ old - constrictor called . ’ It Real book I saw boa is a It . life magnificent a the said Here primeval . a years the the book : “ Boa constrictors swallow their prey whole , without chewing .\n",
      "magnificent , is picture Real about when Here , Stories a a the boa in old was swallowing in years a a ‘ of - primeval I . ’ saw called said wild Once I the showed drawing . copy animal book constrictor six life It . forest a It the book : “ Boa constrictors swallow their prey whole , without chewing .\n",
      "of , was It six book drawing picture , copy primeval in It forest saw years book a said I I Stories ‘ the - Here swallowing . ’ magnificent wild is life when a showed about the . Real a constrictor a animal called Once . old in the a boa : “ Boa constrictors swallow their prey whole , without chewing .\n",
      "I , Once forest It book Stories constrictor , picture in swallowing magnificent six the drawing I the about called saw a ‘ Here - wild It . ’ a boa of animal a showed a book years . is Real copy old when primeval said . the in was a life : “ Boa constrictors swallow their prey whole , without chewing .\n",
      "years , is book the a a I , life magnificent the Here Real Boa of swallowing old in primeval was when ‘ drawing - called six . ’ in the It animal book constrictor It forest said . Stories copy a Once I a showed . a picture boa saw about : “ wild constrictors swallow their prey whole , without chewing .\n",
      "forest , animal life six the a a , I years picture in Boa a called Once showed It constrictors primeval copy ‘ saw - It said . ’ book Stories Real was wild is about swallowing old . of the magnificent constrictor boa a drawing . I the a in when : “ Here book swallow their prey whole , without chewing .\n",
      "saw , swallow copy life was when boa , the a said animal wild Once constrictors showed in book called in old ‘ picture - forest of . ’ is about I Here It swallowing a Real a . years a It a the constrictor the . drawing Boa book primeval I : “ magnificent six Stories their prey whole , without chewing .\n",
      "It , wild saw constrictors Stories Here a , a I picture swallow is in about I constrictor six the a a ‘ magnificent - was swallowing . ’ copy boa showed the Once their when the in . drawing old book Real years Boa animal . called a primeval life It : “ book of said forest prey whole , without chewing .\n",
      "called , drawing constrictors in Once years Real , wild Stories It I the a life a book copy saw a old ‘ the - a animal . ’ constrictor of primeval boa book swallow Here the a . picture six about magnificent their showed forest . I is Boa said when : “ It swallowing in was prey whole , without chewing .\n",
      "in , It magnificent drawing Stories It picture , book a Boa in old was life of a primeval forest called their ‘ swallow - prey years . ’ I a swallowing boa when saw six animal the . the Once book is Real said the . wild constrictors showed copy about : “ a Here a I constrictor whole , without chewing .\n",
      "swallow , the their the It boa Real , wild It magnificent about a six the Boa said Once book constrictor prey ‘ animal - picture of . ’ forest I a Here life copy in book drawing . a is primeval when swallowing saw called . Stories showed old a in : “ a years was constrictors whole I , without chewing .\n",
      "the , six swallowing showed primeval Real boa , constrictor life in said a drawing I Boa Here the animal was called ‘ years - magnificent copy . ’ book of a It the picture prey in wild . a swallow forest whole is constrictors their . old when book Stories a : “ I a saw It Once about , without chewing .\n",
      "3\n",
      "p\n"
     ]
    }
   ],
   "source": [
    "iterator= iterator_list[0]\n",
    "number_of_sentence=config['number_of_sentence']\n",
    "number_sentence_before=config['number_of_sentence_before']\n",
    "pretrained_model='gpt2'\n",
    "past_context_size=config['attention_length_before']\n",
    "transformation='shuffle'\n",
    "vocabulary=None\n",
    "dictionary=None\n",
    "seed=1111\n",
    "max_length=512\n",
    "add_prefix_space=True\n",
    "\n",
    "\n",
    "iterator = [item.strip() for item in iterator]\n",
    "max_length -= 3 # for special tokens, because there is a G (dot) before last special token\n",
    "assert number_of_sentence > 0\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model)\n",
    "\n",
    "batch = []\n",
    "indexes = []\n",
    "sentence_count = 0\n",
    "n = len(iterator)\n",
    "\n",
    "\n",
    "print('entering while loop...')\n",
    "# rest of the iterator + context \n",
    "while sentence_count < n:\n",
    "    start = max(sentence_count - number_sentence_before, 0)\n",
    "    stop = min(sentence_count + number_of_sentence, n)\n",
    "    token_count = len(tokenizer.tokenize(' '.join(iterator[start:stop]), add_prefix_space=add_prefix_space)) # to replace with tokenizer of interest and arguments\n",
    "    if token_count > max_length:\n",
    "        raise ValueError('Cannot fit context with additional sentence. You should reduce context length.')\n",
    "    # computing batch and indexes\n",
    "    print(len(iterator[start:stop]))\n",
    "    print(iterator[start:stop])\n",
    "    \n",
    "    batch_tmp, index_tmp = transform_sentence_and_context(\n",
    "        iterator[start:stop], \n",
    "        past_context_size=past_context_size,\n",
    "        pretrained_model=pretrained_model,\n",
    "        transformation=transformation,\n",
    "        vocabulary=vocabulary,\n",
    "        dictionary=dictionary,\n",
    "        select=stop-start-1,\n",
    "        seed=seed,\n",
    "        add_prefix_space=add_prefix_space\n",
    "    )\n",
    "    print('result:')\n",
    "    for b in batch_tmp:\n",
    "        print(b)\n",
    "    batch += batch_tmp\n",
    "    indexes += index_tmp\n",
    "    sentence_count = stop\n",
    "    print(stop-start-1)\n",
    "    if input()!='':\n",
    "        break\n",
    "\n",
    "#for b in batch:\n",
    "#    print(b)\n",
    "#print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
