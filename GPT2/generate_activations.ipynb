{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to extract hidden-states and attention heads activations from gpt2 model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from model import GPT2Extractor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tokenizer import tokenize\n",
    "from utils import set_seed\n",
    "from numpy import linalg as la\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_folder(path):\n",
    "    \"\"\"Create adequate folders if necessary.\"\"\"\n",
    "    try:\n",
    "        if not os.path.isdir(path):\n",
    "            check_folder(os.path.dirname(path))\n",
    "            os.mkdir(path)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(activations, path, name, run_index, n_layers_hidden=13, n_layers_attention=12, hidden_size=768):\n",
    "    assert activations.values.shape[1] == (n_layers_hidden + n_layers_attention) * hidden_size\n",
    "    indexes = [[index*hidden_size, (index+1)*hidden_size] for index in range(n_layers_hidden + n_layers_attention)]\n",
    "    for order in [2]: # np.inf\n",
    "        matrices = []\n",
    "        for i, index in enumerate(indexes):\n",
    "            matrix = activations.values[:, index[0]:index[1]]\n",
    "            #with_std = True if order=='std' else False\n",
    "            #scaler = StandardScaler(with_mean=True, with_std=with_std)\n",
    "            #scaler.fit(matrix)\n",
    "            #matrix = scaler.transform(matrix)\n",
    "            if order is not None and order != 'std':\n",
    "                matrix = matrix / np.mean(la.norm(matrix, ord=order, axis=1))\n",
    "            matrices.append(matrix)\n",
    "        matrices = np.hstack(matrices)\n",
    "        new_data = pd.DataFrame(matrices, columns=activations.columns)\n",
    "        new_path = path + '_norm-' + str(order).replace('np.', '')\n",
    "        check_folder(new_path)\n",
    "        new_data.to_csv(os.path.join(new_path, name + '_run{}.csv'.format(run_index + 1)), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/english/text_english_run*.txt' # path to text input\n",
    "language = 'english'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating iterator for each run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '/Users/alexpsq/Code/Parietal/data/text_english_run*.txt' # path to text input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = sorted(glob.glob(template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:00<00:00, 730620.70it/s]\n",
      "100%|██████████| 135/135 [00:00<00:00, 546555.06it/s]\n",
      "100%|██████████| 176/176 [00:00<00:00, 972592.23it/s]\n",
      "100%|██████████| 173/173 [00:00<00:00, 993992.59it/s]\n",
      "100%|██████████| 177/177 [00:00<00:00, 972990.57it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1235974.98it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 1174405.12it/s]\n",
      "100%|██████████| 145/145 [00:00<00:00, 393895.13it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 631433.40it/s]\n"
     ]
    }
   ],
   "source": [
    "iterator_list = [tokenize(path, language, train=False) for path in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import utils\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "#from transformers import GPT2Tokenizer\n",
    "#\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "#\n",
    "#lengths = []\n",
    "#\n",
    "#for index in range(9):\n",
    "#    batches, indexes = utils.batchify_per_sentence_with_context(\n",
    "#                iterator_list[index], \n",
    "#                1, \n",
    "#                5, \n",
    "#                'gpt2', \n",
    "#                max_length=512)\n",
    "#    #lengths.append(np.array(sorted([len(item.split()) for item in batches])))\n",
    "#    lengths.append(np.array(sorted([len(tokenizer.tokenize(item)) for item in batches])))\n",
    "#\n",
    "#    sns.boxplot(lengths[-1])\n",
    "#    plt.show()\n",
    "#    print()\n",
    "#\n",
    "#print(np.mean(np.array([np.mean(item) for item in lengths])))\n",
    "#print(np.median(np.array([np.median(item) for item in lengths])))\n",
    "#print(np.mean(np.array([np.median(item) for item in lengths])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import GPT2Model\n",
    "#GPT2Model.from_pretrained('gpt2').config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import utils\n",
    "#config = {'stop_attention_at_sent': None,\n",
    "#          'number_of_sentence': 1,\n",
    "#          'stop_attention_before_sent': 0\n",
    "#         }\n",
    "#\n",
    "#batches, indexes = utils.batchify_with_detailed_indexes(\n",
    "#            iterator_list[0], \n",
    "#            config['number_of_sentence'], \n",
    "#            20, \n",
    "#            'gpt2', \n",
    "#            1024,\n",
    "#            config['stop_attention_at_sent'],\n",
    "#            config['stop_attention_before_sent'],\n",
    "#    True\n",
    "#        )\n",
    "#\n",
    "#\n",
    "#\n",
    "#indexes_tmp = [(indexes[i][-config['number_of_sentence']][0], indexes[i][-1][1]) for i in range(len(indexes))]\n",
    "#indexes_tmp[0] = (indexes[0][0][0], indexes[0][-1][1])\n",
    "#print(indexes_tmp)\n",
    "#for i in indexes_tmp:\n",
    "#    print(i[1]-i[0])\n",
    "#\n",
    "#\n",
    "#\n",
    "#from transformers import GPT2Tokenizer, GPT2Model\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "#model = GPT2Model.from_pretrained('gpt2', output_hidden_states=True)\n",
    "#\n",
    "#m = 0\n",
    "#hidden_states_activations = []\n",
    "#\n",
    "#for index, batch in enumerate(batches):\n",
    "#    batch = batch.strip() # Remove trailing character\n",
    "#    #batch = '<|endoftext|> ' + batch + ' <|endoftext|>'\n",
    "#    \n",
    "#    tokenized_text = tokenizer.tokenize(batch, add_prefix_space=False)\n",
    "#    inputs_ids = torch.tensor([tokenizer.convert_tokens_to_ids(tokenized_text)])\n",
    "#    attention_mask = torch.tensor([[1 for x in tokenized_text]])\n",
    "#\n",
    "#    if (config['stop_attention_at_sent'] is not None) and (index > 0):\n",
    "#        attention_mask[:, :1 + indexes[index][-config['stop_attention_at_sent']-config['number_of_sentence']][0]] = 0\n",
    "#        if self.config['stop_attention_before_sent'] < 0:\n",
    "#            attention_mask[:, 1 + indexes[index][-config['stop_attention_at_sent']-config['number_of_sentence']][0]: 1 + indexes[index][-config['stop_attention_at_sent']-config['number_of_sentence']][0]-config['stop_attention_before_sent']] = 0\n",
    "#        elif self.config['stop_attention_before_sent'] > 0:\n",
    "#            attention_mask[:, 1 + indexes[index][-config['stop_attention_at_sent']-config['number_of_sentence']][0]-config['stop_attention_before_sent']: 1 + indexes[index][-config['stop_attention_at_sent']-config['number_of_sentence']][0]] = 1\n",
    "#    mapping = utils.match_tokenized_to_untokenized(tokenized_text, batch)\n",
    "#    \n",
    "#    with torch.no_grad():\n",
    "#        encoded_layers = model(inputs_ids, attention_mask=attention_mask) \n",
    "#    hidden_states_activations_ = np.vstack(encoded_layers[2]) # retrieve all the hidden states (dimension = layer_count * len(tokenized_text) * feature_count)\n",
    "#    a = utils.extract_activations_from_token_activations(hidden_states_activations_, mapping, indexes_tmp[index])\n",
    "#    #m += len(tokenized_text[indexes_tmp[index][0]:indexes_tmp[index][1]])\n",
    "#    m += len(a)\n",
    "#    #print(len(a), a[0].shape)\n",
    "#    #print(len(tokenized_text), indexes_tmp[index][0], indexes_tmp[index][1])\n",
    "#    #print(tokenized_text[indexes_tmp[index][0]:indexes_tmp[index][1]])\n",
    "#    #print(np.array(tokenized_text)[attention_mask.detach().numpy()[0].astype(bool)])\n",
    "#    #print(mapping)\n",
    "#    key_start = None\n",
    "#    key_stop = None\n",
    "#    for key_, value in mapping.items(): \n",
    "#        if (value[0])== (indexes_tmp[index][0]): #because we added <|endoftext|> token at the beginning\n",
    "#            key_start = key_\n",
    "#\n",
    "#    #print(key_start, len(mapping.keys()))\n",
    "#    #print(tokenized_text)\n",
    "#    for word_index in range(key_start, len(mapping.keys())): # len(mapping.keys()) - 1\n",
    "#        print([index for index in mapping[word_index]])\n",
    "#    #print()\n",
    "#    #a=input()\n",
    "#    #if a!='a':\n",
    "#    #    break\n",
    "#print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_gpt2_models = ['gpt2'] \n",
    "names = [\n",
    "    'gpt2_pre-5_1_token-2'\n",
    "        ]\n",
    "config_paths = [None] * 56\n",
    "saving_path_folders = [\n",
    "    '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/{}/gpt2_pre-5_1_token-2'.format(language)\n",
    "    \n",
    "]\n",
    "prediction_types = ['sentence'] * 56\n",
    "number_of_sentence_list = [1] * 56\n",
    "number_of_sentence_before_list = [2]\n",
    "attention_length_before_list = [2] * 56\n",
    "stop_attention_at_sent_list = [None] * 56\n",
    "stop_attention_before_sent_list = [0] * 56\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_attentions = False\n",
    "output_hidden_states = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2_pre-5_1_token-2  - Extracting activations ...\n",
      "############# Run 0 #############\n",
      " Once ,  when  I  was  six  years  old ,  I  saw  a  magnificent  picture  in  a  book  about  the  primeval  forest  called  ‘  Real  -  life  Stories .  ’  It  showed  a  boa  constrictor  swallowing  a  wild  animal .\n",
      " Here  is  a  copy  of  the  drawing .\n",
      " It  said  in  the  book  :  “  Boa  constrictors  swallow  their  prey  whole ,  without  chewing .\n",
      " Then  they  are  not  able  to  move ,  and  they  sleep  for  the  six  months  it  takes  for  digestion .  ”\n",
      " So  I  thought  a  lot  about  the  adventures  of  the  jungle  and ,  in  turn ,  I  managed ,  with  a  coloured  pencil ,  to  make  my  first  drawing .\n",
      " My  Drawing  Number  one .\n",
      " It  looked  like  this  :  I  showed  my  masterpiece  to  the  grownups  and  I  asked  them  if  my  drawing  frightened  them .\n",
      " They  answered  me  :  “  Why  would  anyone  be  frightened  by  a  hat ?  ”\n",
      " My  drawing  was  not  of  a  hat .\n",
      " It  showed  a  boa  constrictor  digesting  an  elephant .\n",
      " I  then  drew  the  inside  of  the  boa  constrictor ,  so  that  the  grownups  could  understand .\n",
      " They  always  need  to  have  things  explained .\n",
      " My  Drawing  Number  two  looked  like  this  :  The  grownups  advised  me  to  leave  aside  drawings  of  boa  constrictors ,  open  or  closed ,  and  to  apply  myself  instead  to  geography ,  history ,  arithmetic  and  grammar .\n",
      " Thus  I  abandoned ,  at  the  age  of  six ,  a  magnificent  career  as  a  painter .\n",
      " I  was  discouraged  by  the  failure  of  my  Drawing  Number  one  and  of  my  Drawing  Number  two .\n",
      " Grownups  never  understand  anything  by  themselves ,  and  it  ’  s  tiresome  for  children  to  always  explain  things  for  them  again  and  again .\n",
      " So  I  had  to  choose  another  profession ,  and  I  learned  to  fly  airplanes .\n",
      " I  flew  a  little  in  many  places  around  the  world .\n",
      " And  geography ,  it  '  s  true ,  has  served  me  well .\n",
      " I  could  recognize ,  at  first  glance ,  China  from  Arizona .\n",
      " It  ’  s  very  useful  if  you  get  lost  at  night .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:03, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-61d576725776>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"############# Run {} #############\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mactivations\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mhidden_states_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mattention_heads_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/Parietal/NLP_models/GPT2/model.py\u001b[0m in \u001b[0;36mextract_activations\u001b[0;34m(self, iterator, language)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'token-level'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_classic_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m'control-context'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/Parietal/NLP_models/GPT2/model.py\u001b[0m in \u001b[0;36mget_classic_activations\u001b[0;34m(self, iterator, language)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                 \u001b[0mencoded_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# last_hidden_state, pooler_output, hidden_states, attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m                 \u001b[0;31m# last_hidden_state dimension: (batch_size, sequence_length, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0;31m# pooler_output dimension: (batch_size, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/Parietal/NLP_models/GPT2/modeling_hacked_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache)\u001b[0m\n\u001b[1;32m    517\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                 \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m             )\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/Parietal/NLP_models/GPT2/modeling_hacked_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, layer_past, attention_mask, head_mask, use_cache)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/Parietal/NLP_models/GPT2/modeling_hacked_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1609\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m         \u001b[0msize_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1611\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1612\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msize_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for index, gpt2_model in enumerate(pretrained_gpt2_models):\n",
    "    extractor = GPT2Extractor(gpt2_model, \n",
    "                              language, \n",
    "                              names[index], \n",
    "                              prediction_types[index],\n",
    "                              output_hidden_states=output_hidden_states, \n",
    "                              output_attentions=output_attentions,\n",
    "                              attention_length_before=attention_length_before_list[index],\n",
    "                              config_path=config_paths[index],\n",
    "                              max_length=512, \n",
    "                              number_of_sentence=number_of_sentence_list[index], \n",
    "                              number_of_sentence_before=number_of_sentence_before_list[index],\n",
    "                              stop_attention_at_sent=stop_attention_at_sent_list[index],\n",
    "                              stop_attention_before_sent=stop_attention_before_sent_list[index]\n",
    "                             )\n",
    "    print(extractor.name, ' - Extracting activations ...')\n",
    "    for run_index, iterator in tqdm(enumerate(iterator_list)):\n",
    "        gc.collect()\n",
    "        print(\"############# Run {} #############\".format(run_index + 1))\n",
    "        activations  = extractor.extract_activations(iterator, language)\n",
    "        hidden_states_activations = activations[0]\n",
    "        attention_heads_activations = activations[1]\n",
    "        #activations = pd.concat([hidden_states_activations, attention_heads_activations], axis=1)\n",
    "        print(hidden_states_activations.shape)\n",
    "        #transform(\n",
    "        #    hidden_states_activations, \n",
    "        #    saving_path_folders[index], \n",
    "        #    'activations', \n",
    "        #    run_index=run_index,\n",
    "        #    n_layers_hidden=13,\n",
    "        #    n_layers_attention=0, \n",
    "        #    hidden_size=768)\n",
    "        \n",
    "        #transform(cls_activations, saving_path_folders[index], 'cls')\n",
    "        #transform(sep_activations, saving_path_folders[index], 'sep')\n",
    "        \n",
    "        #check_folder(saving_path_folders[index])\n",
    "        #hidden_states_activations.to_csv(os.path.join(saving_path_folders[index], 'activations_run{}.csv'.format(run_index + 1)), index=False)\n",
    "        #activations.to_csv(os.path.join(saving_path_folders[index], 'activations_run{}.csv'.format(run_index + 1)), index=False)\n",
    "        #cls_activations.to_csv(os.path.join(saving_path_folders[index], 'cls_run{}.csv'.format(run_index + 1)), index=False)\n",
    "        #sep_activations.to_csv(os.path.join(saving_path_folders[index], 'sep_run{}.csv'.format(run_index + 1)), index=False)\n",
    "        del activations\n",
    "        del hidden_states_activations\n",
    "        #del attention_heads_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, bert_model in enumerate(pretrained_bert_models):\n",
    "    extractor = BertExtractor(bert_model, \n",
    "                              language, \n",
    "                              names[index], \n",
    "                              prediction_types[index], \n",
    "                              output_hidden_states=True, \n",
    "                              output_attentions=True, \n",
    "                              config_path=config_paths[index], \n",
    "                              max_length=512, \n",
    "                              number_of_sentence=number_of_sentence_list[index], \n",
    "                              number_of_sentence_before=number_of_sentence_before_list[index], \n",
    "                              number_of_sentence_after=number_of_sentence_after_list[index])\n",
    "    print(extractor.name, ' - Extracting activations ...')\n",
    "    for run_index, iterator in tqdm(enumerate(iterator_list)):\n",
    "        print(\"############# Run {} #############\".format(run_index))\n",
    "        activations  = extractor.extract_activations(iterator, language)\n",
    "        hidden_states_activations = activations[0]\n",
    "        attention_heads_activations = activations[1]\n",
    "        (cls_hidden_states_activations, cls_attention_activations) = activations[2]\n",
    "        (sep_hidden_states_activations, sep_attention_activations) = activations[3]\n",
    "        activations = pd.concat([hidden_states_activations, attention_heads_activations], axis=1)\n",
    "        cls_activations = pd.concat([cls_hidden_states_activations, cls_attention_activations], axis=1)\n",
    "        sep_activations = pd.concat([sep_hidden_states_activations, sep_attention_activations], axis=1)\n",
    "        \n",
    "        transform(activations, saving_path_folders[index], 'activations', run_index=run_index)\n",
    "        #transform(cls_activations, saving_path_folders[index], 'cls')\n",
    "        #transform(sep_activations, saving_path_folders[index], 'sep')\n",
    "        \n",
    "        #activations.to_csv(os.path.join(saving_path_folders[index], 'activations_run{}.csv'.format(run_index + 1)), index=False)\n",
    "        #cls_activations.to_csv(os.path.join(saving_path_folders[index], 'cls_run{}.csv'.format(run_index + 1)), index=False)\n",
    "        #sep_activations.to_csv(os.path.join(saving_path_folders[index], 'sep_run{}.csv'.format(run_index + 1)), index=False)\n",
    "        del activations\n",
    "        del cls_activations\n",
    "        del sep_activations\n",
    "        del hidden_states_activations\n",
    "        del attention_heads_activations\n",
    "        del cls_hidden_states_activations\n",
    "        del cls_attention_activations\n",
    "        del sep_hidden_states_activations\n",
    "        del sep_attention_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for p in list(extractor.model.named_parameters()):\n",
    "    print(p[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate control activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = 'gpt2'\n",
    "language = 'english'\n",
    "name = 'gpt2_control_'\n",
    "prediction_type = 'sentence'\n",
    "saving_path_folder = '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/{}'.format(language)\n",
    "seeds = [24, 213, 1111, 61, 183]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_layer(model, layer_nb):\n",
    "    \"\"\"Randomize layer weights and put bias to zero.\n",
    "    The input \"layer_nb\" goes from 1 to 12 to be coherent with the rest of the analysis.\n",
    "    It is then transfomed in the function.\n",
    "    \"\"\"\n",
    "    layer_nb = layer_nb - 1\n",
    "    model.h[layer_nb].ln_1.weight = torch.nn.parameter.Parameter(torch.rand_like(model.h[layer_nb].ln_1.weight))\n",
    "    model.h[layer_nb].ln_1.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.h[layer_nb].ln_1.bias))\n",
    "    model.h[layer_nb].attn.c_attn.weight = torch.nn.parameter.Parameter(torch.rand_like(model.h[layer_nb].attn.c_attn.weight))\n",
    "    model.h[layer_nb].attn.c_attn.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.h[layer_nb].attn.c_attn.bias))\n",
    "    model.h[layer_nb].attn.c_proj.weight = torch.nn.parameter.Parameter(torch.rand_like(model.h[layer_nb].attn.c_proj.weight))\n",
    "    model.h[layer_nb].attn.c_proj.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.h[layer_nb].attn.c_proj.bias))\n",
    "    model.h[layer_nb].ln_2.weight = torch.nn.parameter.Parameter(torch.rand_like(model.h[layer_nb].ln_2.weight))\n",
    "    model.h[layer_nb].ln_2.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.h[layer_nb].ln_2.bias))\n",
    "    model.h[layer_nb].mlp.c_fc.weight = torch.nn.parameter.Parameter(torch.rand_like(model.h[layer_nb].mlp.c_fc.weight))\n",
    "    model.h[layer_nb].mlp.c_fc.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.h[layer_nb].mlp.c_fc.bias))\n",
    "    model.h[layer_nb].mlp.c_proj.weight = torch.nn.parameter.Parameter(torch.rand_like(model.h[layer_nb].mlp.c_proj.weight))\n",
    "    model.h[layer_nb].mlp.c_proj.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.h[layer_nb].mlp.c_proj.bias))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_ln_1(model, layer_nb):\n",
    "    \"\"\"Randomize attention query weights of a given layer and put bias to zero.\n",
    "    \"\"\"\n",
    "    layer_nb = layer_nb - 1\n",
    "    model.h[layer_nb].ln_1.weight = torch.nn.parameter.Parameter(torch.rand_like(model.h[layer_nb].ln_1.weight))\n",
    "    model.h[layer_nb].ln_1.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.h[layer_nb].ln_1.bias))\n",
    "    return model\n",
    "\n",
    "def randomize_attention_c_attn(model, layer_nb):\n",
    "    \"\"\"Randomize attention key weights of a given layer and put bias to zero.\n",
    "    \"\"\"\n",
    "    layer_nb = layer_nb - 1\n",
    "    model.h[layer_nb].attn.c_attn.weight = torch.nn.parameter.Parameter(torch.rand_like(model.h[layer_nb].attn.c_attn.weight))\n",
    "    model.h[layer_nb].attn.c_attn.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.h[layer_nb].attn.c_attn.bias))\n",
    "    return model\n",
    "\n",
    "def randomize_attention_c_proj(model, layer_nb):\n",
    "    \"\"\"Randomize attention value weights of a given layer and put bias to zero.\n",
    "    \"\"\"\n",
    "    layer_nb = layer_nb - 1\n",
    "    model.h[layer_nb].attn.c_proj.weight = torch.nn.parameter.Parameter(torch.rand_like(model.h[layer_nb].attn.c_proj.weight))\n",
    "    model.h[layer_nb].attn.c_proj.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.h[layer_nb].attn.c_proj.bias))\n",
    "    return model\n",
    "\n",
    "def randomize_ln_2(model, layer_nb):\n",
    "    \"\"\"Randomize attention dense network weights of a given layer and put bias to zero.\n",
    "    \"\"\"\n",
    "    layer_nb = layer_nb - 1\n",
    "    model.h[layer_nb].ln_2.weight = torch.nn.parameter.Parameter(torch.rand_like(model.h[layer_nb].ln_2.weight))\n",
    "    model.h[layer_nb].ln_2.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.h[layer_nb].ln_2.bias))\n",
    "    return model\n",
    "\n",
    "\n",
    "def randomize_mlp_c_fc(model, layer_nb):\n",
    "    \"\"\"Randomize intermediate dense network weights of a given layer and put bias to zero.\n",
    "    \"\"\"\n",
    "    layer_nb = layer_nb - 1\n",
    "    model.h[layer_nb].mlp.c_fc.weight = torch.nn.parameter.Parameter(torch.rand_like(model.h[layer_nb].mlp.c_fc.weight))\n",
    "    model.h[layer_nb].mlp.c_fc.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.h[layer_nb].mlp.c_fc.bias))\n",
    "    return model\n",
    "\n",
    "def randomize_mlp_c_proj(model, layer_nb):\n",
    "    \"\"\"Randomize output dense network weights of a given layer and put bias to zero.\n",
    "    \"\"\"\n",
    "    layer_nb = layer_nb - 1\n",
    "    model.h[layer_nb].mlp.c_proj.weight = torch.nn.parameter.Parameter(torch.rand_like(model.h[layer_nb].mlp.c_proj.weight))\n",
    "    model.h[layer_nb].mlp.c_proj.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.h[layer_nb].mlp.c_proj.bias))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_embeddings(model):\n",
    "    \"\"\"Randomize embeddings weights and put bias to zero.\n",
    "    \"\"\"\n",
    "    model.wte.weight = torch.nn.parameter.Parameter(torch.rand_like(model.wte.weight))\n",
    "    model.wpe.weight = torch.nn.parameter.Parameter(torch.rand_like(model.wpe.weight))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "    for layer in range(13):\n",
    "        extractor = GPT2Extractor(gpt2_model, language, name, prediction_type, output_hidden_states=True, output_attentions=True, config_path=None)\n",
    "        if layer==0:\n",
    "            extractor.model = randomize_embeddings(extractor.model)\n",
    "        else:\n",
    "            extractor.model = randomize_layer(extractor.model, layer)\n",
    "        print(extractor.name + str(seed), ' - Extracting activations for layer {}...'.format(layer))\n",
    "        for run_index, iterator in tqdm(enumerate(iterator_list)):\n",
    "            print(\"############# Run {} #############\".format(run_index))\n",
    "            activations  = extractor.extract_activations(iterator, language)\n",
    "            hidden_states_activations = activations[0]\n",
    "            attention_heads_activations = activations[1]\n",
    "            activations = pd.concat([hidden_states_activations, attention_heads_activations], axis=1)\n",
    "\n",
    "            # activations\n",
    "            heads = np.arange(1, 13)\n",
    "            columns_to_retrieve = ['hidden_state-layer-{}-{}'.format(layer, i) for i in range(1, 769)]\n",
    "            if layer > 0:\n",
    "                columns_to_retrieve += ['attention-layer-{}-head-{}-{}'.format(layer, head, i) for head in heads for i in range(1, 65)]\n",
    "            activations = activations[columns_to_retrieve]\n",
    "\n",
    "            save_path = os.path.join(saving_path_folder, name + str(seed) + '_layer-{}'.format(layer))\n",
    "            check_folder(save_path)\n",
    "            print('\\tSaving in {}.'.format(save_path))\n",
    "            activations.to_csv(os.path.join(save_path, 'activations_run{}.csv'.format(run_index + 1)), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
