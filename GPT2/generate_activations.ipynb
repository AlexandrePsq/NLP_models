{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to extract hidden-states and attention heads activations from bert model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from model import GPT2Extractor\n",
    "from tokenizer import tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_folder(path):\n",
    "    \"\"\"Create adequate folders if necessary.\"\"\"\n",
    "    try:\n",
    "        if not os.path.isdir(path):\n",
    "            check_folder(os.path.dirname(path))\n",
    "            os.mkdir(path)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/english/text_english_run*.txt' # path to text input\n",
    "language = 'english'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating iterator for each run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_gpt2_models = ['gpt2']\n",
    "names = ['gpt2']\n",
    "config_paths = [None]\n",
    "saving_path_folders = [\n",
    "    '/Users/alexpsq/Code/Parietal/data/stimuli-representations/{}/gpt2'.format(language)]\n",
    "#saving_path_folders = [\n",
    "#    '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/{}/gpt2'.format(language)]\n",
    "prediction_types = ['sentence']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt2']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '/Users/alexpsq/Code/Parietal/data/text_english_run*.txt' # path to text input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = sorted(glob.glob(template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:00<00:00, 569648.93it/s]\n",
      "100%|██████████| 135/135 [00:00<00:00, 813550.34it/s]\n",
      "100%|██████████| 176/176 [00:00<00:00, 994875.34it/s]\n",
      "100%|██████████| 173/173 [00:00<00:00, 418946.07it/s]\n",
      "100%|██████████| 177/177 [00:00<00:00, 912029.25it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 652245.98it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 690246.50it/s]\n",
      "100%|██████████| 145/145 [00:00<00:00, 457617.82it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 1087996.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iterator_list = [tokenize(path, language, train=False) for path in paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_attentions = False\n",
    "output_hidden_states = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2  - Extracting activations ...\n",
      "############# Run 0 #############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:05, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate object of type '<class 'list'>'; only Series and DataFrame objs are valid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b9e5795620ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mhidden_states_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mattention_heads_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhidden_states_activations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_heads_activations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_path_folders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'activations_run{}.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \u001b[0;34m\"only Series and DataFrame objs are valid\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m                 )\n\u001b[0;32m--> 357\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;31m# consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot concatenate object of type '<class 'list'>'; only Series and DataFrame objs are valid"
     ]
    }
   ],
   "source": [
    "for index, gpt2_model in enumerate(pretrained_gpt2_models):\n",
    "    extractor = GPT2Extractor(gpt2_model, language, names[index], prediction_types[index], output_hidden_states=output_hidden_states, output_attentions=output_attentions, config_path=config_paths[index])\n",
    "    print(extractor.name, ' - Extracting activations ...')\n",
    "    for run_index, iterator in tqdm(enumerate(iterator_list)):\n",
    "        print(\"############# Run {} #############\".format(run_index))\n",
    "        check_folder(saving_path_folders[index])\n",
    "        activations  = extractor.extract_activations(iterator, language)\n",
    "        hidden_states_activations = activations[0]\n",
    "        attention_heads_activations = activations[1]\n",
    "        activations = pd.concat([hidden_states_activations, attention_heads_activations], axis=1)\n",
    "        \n",
    "        activations.to_csv(os.path.join(saving_path_folders[index], 'activations_run{}.csv'.format(run_index + 1)), index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_state-layer-0-1</th>\n",
       "      <th>hidden_state-layer-0-2</th>\n",
       "      <th>hidden_state-layer-0-3</th>\n",
       "      <th>hidden_state-layer-0-4</th>\n",
       "      <th>hidden_state-layer-0-5</th>\n",
       "      <th>hidden_state-layer-0-6</th>\n",
       "      <th>hidden_state-layer-0-7</th>\n",
       "      <th>hidden_state-layer-0-8</th>\n",
       "      <th>hidden_state-layer-0-9</th>\n",
       "      <th>hidden_state-layer-0-10</th>\n",
       "      <th>...</th>\n",
       "      <th>hidden_state-layer-12-759</th>\n",
       "      <th>hidden_state-layer-12-760</th>\n",
       "      <th>hidden_state-layer-12-761</th>\n",
       "      <th>hidden_state-layer-12-762</th>\n",
       "      <th>hidden_state-layer-12-763</th>\n",
       "      <th>hidden_state-layer-12-764</th>\n",
       "      <th>hidden_state-layer-12-765</th>\n",
       "      <th>hidden_state-layer-12-766</th>\n",
       "      <th>hidden_state-layer-12-767</th>\n",
       "      <th>hidden_state-layer-12-768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.222231</td>\n",
       "      <td>-0.199346</td>\n",
       "      <td>0.102250</td>\n",
       "      <td>0.072660</td>\n",
       "      <td>-0.109572</td>\n",
       "      <td>-0.251374</td>\n",
       "      <td>-0.427017</td>\n",
       "      <td>-0.192565</td>\n",
       "      <td>-0.090373</td>\n",
       "      <td>-0.170221</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080227</td>\n",
       "      <td>0.069681</td>\n",
       "      <td>-0.214584</td>\n",
       "      <td>-0.155730</td>\n",
       "      <td>0.703937</td>\n",
       "      <td>-0.156755</td>\n",
       "      <td>0.002902</td>\n",
       "      <td>-0.229858</td>\n",
       "      <td>-0.056037</td>\n",
       "      <td>-0.035686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.046362</td>\n",
       "      <td>-0.132434</td>\n",
       "      <td>0.061459</td>\n",
       "      <td>0.199972</td>\n",
       "      <td>0.023228</td>\n",
       "      <td>0.044123</td>\n",
       "      <td>-0.422747</td>\n",
       "      <td>0.118858</td>\n",
       "      <td>-0.056940</td>\n",
       "      <td>-0.028383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.193726</td>\n",
       "      <td>-0.091036</td>\n",
       "      <td>0.351631</td>\n",
       "      <td>0.080826</td>\n",
       "      <td>0.024973</td>\n",
       "      <td>0.143238</td>\n",
       "      <td>0.126042</td>\n",
       "      <td>0.072917</td>\n",
       "      <td>-0.112462</td>\n",
       "      <td>-0.029984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.118786</td>\n",
       "      <td>-0.107309</td>\n",
       "      <td>0.112027</td>\n",
       "      <td>0.168924</td>\n",
       "      <td>-0.044425</td>\n",
       "      <td>-0.127225</td>\n",
       "      <td>-0.485738</td>\n",
       "      <td>-0.087842</td>\n",
       "      <td>0.077962</td>\n",
       "      <td>-0.014586</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.471415</td>\n",
       "      <td>0.179074</td>\n",
       "      <td>0.452323</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.266642</td>\n",
       "      <td>0.126591</td>\n",
       "      <td>0.120691</td>\n",
       "      <td>0.058829</td>\n",
       "      <td>-0.294023</td>\n",
       "      <td>-0.180088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.019110</td>\n",
       "      <td>-0.148983</td>\n",
       "      <td>0.221716</td>\n",
       "      <td>0.134502</td>\n",
       "      <td>-0.158677</td>\n",
       "      <td>-0.114787</td>\n",
       "      <td>-0.451582</td>\n",
       "      <td>-0.186877</td>\n",
       "      <td>0.073118</td>\n",
       "      <td>-0.092988</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.230266</td>\n",
       "      <td>0.223334</td>\n",
       "      <td>0.257051</td>\n",
       "      <td>0.072100</td>\n",
       "      <td>0.021015</td>\n",
       "      <td>0.158723</td>\n",
       "      <td>0.245941</td>\n",
       "      <td>-0.148598</td>\n",
       "      <td>0.192446</td>\n",
       "      <td>0.064503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.176048</td>\n",
       "      <td>-0.148281</td>\n",
       "      <td>0.341300</td>\n",
       "      <td>0.020825</td>\n",
       "      <td>-0.146095</td>\n",
       "      <td>-0.004921</td>\n",
       "      <td>-0.512484</td>\n",
       "      <td>-0.072568</td>\n",
       "      <td>-0.079359</td>\n",
       "      <td>-0.025802</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.089730</td>\n",
       "      <td>0.417009</td>\n",
       "      <td>0.170452</td>\n",
       "      <td>0.164576</td>\n",
       "      <td>0.559334</td>\n",
       "      <td>0.247522</td>\n",
       "      <td>0.036557</td>\n",
       "      <td>0.004062</td>\n",
       "      <td>-0.024033</td>\n",
       "      <td>0.180207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1888</th>\n",
       "      <td>-0.117341</td>\n",
       "      <td>0.028862</td>\n",
       "      <td>0.136164</td>\n",
       "      <td>0.208377</td>\n",
       "      <td>-0.114606</td>\n",
       "      <td>-0.003114</td>\n",
       "      <td>-0.489909</td>\n",
       "      <td>-0.084179</td>\n",
       "      <td>-0.006406</td>\n",
       "      <td>-0.203908</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025312</td>\n",
       "      <td>0.264754</td>\n",
       "      <td>0.181911</td>\n",
       "      <td>0.096496</td>\n",
       "      <td>1.213056</td>\n",
       "      <td>0.510090</td>\n",
       "      <td>-0.240733</td>\n",
       "      <td>-0.259916</td>\n",
       "      <td>-0.091030</td>\n",
       "      <td>0.613524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889</th>\n",
       "      <td>-0.023117</td>\n",
       "      <td>0.019310</td>\n",
       "      <td>0.138129</td>\n",
       "      <td>0.104024</td>\n",
       "      <td>-0.043709</td>\n",
       "      <td>-0.025883</td>\n",
       "      <td>-0.515242</td>\n",
       "      <td>-0.089623</td>\n",
       "      <td>0.032116</td>\n",
       "      <td>-0.067125</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085490</td>\n",
       "      <td>0.127606</td>\n",
       "      <td>0.355455</td>\n",
       "      <td>-0.004345</td>\n",
       "      <td>1.160623</td>\n",
       "      <td>0.390218</td>\n",
       "      <td>-0.194015</td>\n",
       "      <td>-0.346576</td>\n",
       "      <td>-0.213058</td>\n",
       "      <td>0.623512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>-0.005001</td>\n",
       "      <td>-0.147860</td>\n",
       "      <td>0.263442</td>\n",
       "      <td>0.080779</td>\n",
       "      <td>0.019132</td>\n",
       "      <td>0.074155</td>\n",
       "      <td>-0.558621</td>\n",
       "      <td>-0.244804</td>\n",
       "      <td>0.123008</td>\n",
       "      <td>-0.094988</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145380</td>\n",
       "      <td>0.182243</td>\n",
       "      <td>0.245503</td>\n",
       "      <td>0.058192</td>\n",
       "      <td>2.074650</td>\n",
       "      <td>0.414970</td>\n",
       "      <td>-0.519282</td>\n",
       "      <td>-0.641491</td>\n",
       "      <td>-0.046649</td>\n",
       "      <td>0.676222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>-0.145681</td>\n",
       "      <td>0.085838</td>\n",
       "      <td>0.187571</td>\n",
       "      <td>0.059699</td>\n",
       "      <td>0.041959</td>\n",
       "      <td>-0.155613</td>\n",
       "      <td>-0.619032</td>\n",
       "      <td>-0.030259</td>\n",
       "      <td>0.111349</td>\n",
       "      <td>-0.023952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.152615</td>\n",
       "      <td>0.269339</td>\n",
       "      <td>0.417994</td>\n",
       "      <td>0.230248</td>\n",
       "      <td>1.897090</td>\n",
       "      <td>0.376180</td>\n",
       "      <td>-0.449664</td>\n",
       "      <td>-0.705842</td>\n",
       "      <td>-0.185300</td>\n",
       "      <td>0.792126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>0.004858</td>\n",
       "      <td>-0.202139</td>\n",
       "      <td>0.126055</td>\n",
       "      <td>0.410303</td>\n",
       "      <td>-0.068946</td>\n",
       "      <td>-0.043339</td>\n",
       "      <td>-0.486457</td>\n",
       "      <td>-0.149028</td>\n",
       "      <td>0.024939</td>\n",
       "      <td>-0.264458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011846</td>\n",
       "      <td>0.368192</td>\n",
       "      <td>0.269097</td>\n",
       "      <td>0.025218</td>\n",
       "      <td>1.466517</td>\n",
       "      <td>0.467084</td>\n",
       "      <td>-0.281799</td>\n",
       "      <td>-0.431197</td>\n",
       "      <td>-0.093611</td>\n",
       "      <td>0.805206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1893 rows × 9984 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      hidden_state-layer-0-1  hidden_state-layer-0-2  hidden_state-layer-0-3  \\\n",
       "0                  -0.222231               -0.199346                0.102250   \n",
       "1                  -0.046362               -0.132434                0.061459   \n",
       "2                  -0.118786               -0.107309                0.112027   \n",
       "3                  -0.019110               -0.148983                0.221716   \n",
       "4                  -0.176048               -0.148281                0.341300   \n",
       "...                      ...                     ...                     ...   \n",
       "1888               -0.117341                0.028862                0.136164   \n",
       "1889               -0.023117                0.019310                0.138129   \n",
       "1890               -0.005001               -0.147860                0.263442   \n",
       "1891               -0.145681                0.085838                0.187571   \n",
       "1892                0.004858               -0.202139                0.126055   \n",
       "\n",
       "      hidden_state-layer-0-4  hidden_state-layer-0-5  hidden_state-layer-0-6  \\\n",
       "0                   0.072660               -0.109572               -0.251374   \n",
       "1                   0.199972                0.023228                0.044123   \n",
       "2                   0.168924               -0.044425               -0.127225   \n",
       "3                   0.134502               -0.158677               -0.114787   \n",
       "4                   0.020825               -0.146095               -0.004921   \n",
       "...                      ...                     ...                     ...   \n",
       "1888                0.208377               -0.114606               -0.003114   \n",
       "1889                0.104024               -0.043709               -0.025883   \n",
       "1890                0.080779                0.019132                0.074155   \n",
       "1891                0.059699                0.041959               -0.155613   \n",
       "1892                0.410303               -0.068946               -0.043339   \n",
       "\n",
       "      hidden_state-layer-0-7  hidden_state-layer-0-8  hidden_state-layer-0-9  \\\n",
       "0                  -0.427017               -0.192565               -0.090373   \n",
       "1                  -0.422747                0.118858               -0.056940   \n",
       "2                  -0.485738               -0.087842                0.077962   \n",
       "3                  -0.451582               -0.186877                0.073118   \n",
       "4                  -0.512484               -0.072568               -0.079359   \n",
       "...                      ...                     ...                     ...   \n",
       "1888               -0.489909               -0.084179               -0.006406   \n",
       "1889               -0.515242               -0.089623                0.032116   \n",
       "1890               -0.558621               -0.244804                0.123008   \n",
       "1891               -0.619032               -0.030259                0.111349   \n",
       "1892               -0.486457               -0.149028                0.024939   \n",
       "\n",
       "      hidden_state-layer-0-10  ...  hidden_state-layer-12-759  \\\n",
       "0                   -0.170221  ...                  -0.080227   \n",
       "1                   -0.028383  ...                   0.193726   \n",
       "2                   -0.014586  ...                  -0.471415   \n",
       "3                   -0.092988  ...                  -0.230266   \n",
       "4                   -0.025802  ...                  -0.089730   \n",
       "...                       ...  ...                        ...   \n",
       "1888                -0.203908  ...                  -0.025312   \n",
       "1889                -0.067125  ...                  -0.085490   \n",
       "1890                -0.094988  ...                  -0.145380   \n",
       "1891                -0.023952  ...                  -0.152615   \n",
       "1892                -0.264458  ...                  -0.011846   \n",
       "\n",
       "      hidden_state-layer-12-760  hidden_state-layer-12-761  \\\n",
       "0                      0.069681                  -0.214584   \n",
       "1                     -0.091036                   0.351631   \n",
       "2                      0.179074                   0.452323   \n",
       "3                      0.223334                   0.257051   \n",
       "4                      0.417009                   0.170452   \n",
       "...                         ...                        ...   \n",
       "1888                   0.264754                   0.181911   \n",
       "1889                   0.127606                   0.355455   \n",
       "1890                   0.182243                   0.245503   \n",
       "1891                   0.269339                   0.417994   \n",
       "1892                   0.368192                   0.269097   \n",
       "\n",
       "      hidden_state-layer-12-762  hidden_state-layer-12-763  \\\n",
       "0                     -0.155730                   0.703937   \n",
       "1                      0.080826                   0.024973   \n",
       "2                      0.054650                   0.266642   \n",
       "3                      0.072100                   0.021015   \n",
       "4                      0.164576                   0.559334   \n",
       "...                         ...                        ...   \n",
       "1888                   0.096496                   1.213056   \n",
       "1889                  -0.004345                   1.160623   \n",
       "1890                   0.058192                   2.074650   \n",
       "1891                   0.230248                   1.897090   \n",
       "1892                   0.025218                   1.466517   \n",
       "\n",
       "      hidden_state-layer-12-764  hidden_state-layer-12-765  \\\n",
       "0                     -0.156755                   0.002902   \n",
       "1                      0.143238                   0.126042   \n",
       "2                      0.126591                   0.120691   \n",
       "3                      0.158723                   0.245941   \n",
       "4                      0.247522                   0.036557   \n",
       "...                         ...                        ...   \n",
       "1888                   0.510090                  -0.240733   \n",
       "1889                   0.390218                  -0.194015   \n",
       "1890                   0.414970                  -0.519282   \n",
       "1891                   0.376180                  -0.449664   \n",
       "1892                   0.467084                  -0.281799   \n",
       "\n",
       "      hidden_state-layer-12-766  hidden_state-layer-12-767  \\\n",
       "0                     -0.229858                  -0.056037   \n",
       "1                      0.072917                  -0.112462   \n",
       "2                      0.058829                  -0.294023   \n",
       "3                     -0.148598                   0.192446   \n",
       "4                      0.004062                  -0.024033   \n",
       "...                         ...                        ...   \n",
       "1888                  -0.259916                  -0.091030   \n",
       "1889                  -0.346576                  -0.213058   \n",
       "1890                  -0.641491                  -0.046649   \n",
       "1891                  -0.705842                  -0.185300   \n",
       "1892                  -0.431197                  -0.093611   \n",
       "\n",
       "      hidden_state-layer-12-768  \n",
       "0                     -0.035686  \n",
       "1                     -0.029984  \n",
       "2                     -0.180088  \n",
       "3                      0.064503  \n",
       "4                      0.180207  \n",
       "...                         ...  \n",
       "1888                   0.613524  \n",
       "1889                   0.623512  \n",
       "1890                   0.676222  \n",
       "1891                   0.792126  \n",
       "1892                   0.805206  \n",
       "\n",
       "[1893 rows x 9984 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate control activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = 'bert-base-cased'\n",
    "language = 'english'\n",
    "name = 'bert-base-cased_control'\n",
    "prediction_type = 'sentence'\n",
    "saving_path_folder = '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/stimuli-representations/{}'.format(language)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_layer(model, layer_nb):\n",
    "    \"\"\"Randomize layer weights and put bias to zero.\n",
    "    The input \"layer_nb\" goes from 1 to 12 to be coherent with the rest of the analysis.\n",
    "    It is then transfomed in the function.\n",
    "    \"\"\"\n",
    "    layer_nb = layer_nb - 1\n",
    "    model.encoder.layer[layer_nb].attention.self.query.weight = torch.nn.parameter.Parameter(torch.rand_like(model.encoder.layer[layer_nb].attention.self.query.weight))\n",
    "    model.encoder.layer[layer_nb].attention.self.query.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.encoder.layer[layer_nb].attention.self.query.bias))\n",
    "    model.encoder.layer[layer_nb].attention.self.key.weight = torch.nn.parameter.Parameter(torch.rand_like(model.encoder.layer[layer_nb].attention.self.key.weight))\n",
    "    model.encoder.layer[layer_nb].attention.self.key.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.encoder.layer[layer_nb].attention.self.key.bias))\n",
    "    model.encoder.layer[layer_nb].attention.self.value.weight = torch.nn.parameter.Parameter(torch.rand_like(model.encoder.layer[layer_nb].attention.self.value.weight))\n",
    "    model.encoder.layer[layer_nb].attention.self.value.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.encoder.layer[layer_nb].attention.self.value.bias))\n",
    "    model.encoder.layer[layer_nb].attention.output.dense.weight = torch.nn.parameter.Parameter(torch.rand_like(model.encoder.layer[layer_nb].attention.output.dense.weight))\n",
    "    model.encoder.layer[layer_nb].attention.output.dense.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.encoder.layer[layer_nb].attention.output.dense.bias))\n",
    "    model.encoder.layer[layer_nb].attention.output.LayerNorm.weight = torch.nn.parameter.Parameter(torch.rand_like(model.encoder.layer[layer_nb].attention.output.LayerNorm.weight))\n",
    "    model.encoder.layer[layer_nb].attention.output.LayerNorm.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.encoder.layer[layer_nb].attention.output.LayerNorm.bias))\n",
    "    model.encoder.layer[layer_nb].intermediate.dense.weight = torch.nn.parameter.Parameter(torch.rand_like(model.encoder.layer[layer_nb].intermediate.dense.weight))\n",
    "    model.encoder.layer[layer_nb].intermediate.dense.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.encoder.layer[layer_nb].intermediate.dense.bias))\n",
    "    model.encoder.layer[layer_nb].output.dense.weight = torch.nn.parameter.Parameter(torch.rand_like(model.encoder.layer[layer_nb].output.dense.weight))\n",
    "    model.encoder.layer[layer_nb].output.dense.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.encoder.layer[layer_nb].output.dense.bias))\n",
    "    model.encoder.layer[layer_nb].output.LayerNorm.weight = torch.nn.parameter.Parameter(torch.rand_like(model.encoder.layer[layer_nb].output.LayerNorm.weight))\n",
    "    model.encoder.layer[layer_nb].output.LayerNorm.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.encoder.layer[layer_nb].output.LayerNorm.bias))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_attention_query(model, layer_nb):\n",
    "    \"\"\"Randomize attention query weights of a given layer and put bias to zero.\n",
    "    \"\"\"\n",
    "    layer_nb = layer_nb - 1\n",
    "    model.encoder.layer[layer_nb].attention.self.query.weight = torch.nn.parameter.Parameter(torch.rand_like(model.encoder.layer[layer_nb].attention.self.query.weight))\n",
    "    model.encoder.layer[layer_nb].attention.self.query.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.encoder.layer[layer_nb].attention.self.query.bias))\n",
    "    return model\n",
    "\n",
    "def randomize_attention_key(model, layer_nb):\n",
    "    \"\"\"Randomize attention key weights of a given layer and put bias to zero.\n",
    "    \"\"\"\n",
    "    layer_nb = layer_nb - 1\n",
    "    model.encoder.layer[layer_nb].attention.self.key.weight = torch.nn.parameter.Parameter(torch.rand_like(model.encoder.layer[layer_nb].attention.self.key.weight))\n",
    "    model.encoder.layer[layer_nb].attention.self.key.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.encoder.layer[layer_nb].attention.self.key.bias))\n",
    "    return model\n",
    "\n",
    "def randomize_attention_value(model, layer_nb):\n",
    "    \"\"\"Randomize attention value weights of a given layer and put bias to zero.\n",
    "    \"\"\"\n",
    "    layer_nb = layer_nb - 1\n",
    "    model.encoder.layer[layer_nb].attention.self.value.weight = torch.nn.parameter.Parameter(torch.rand_like(model.encoder.layer[layer_nb].attention.self.value.weight))\n",
    "    model.encoder.layer[layer_nb].attention.self.value.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.encoder.layer[layer_nb].attention.self.value.bias))\n",
    "    return model\n",
    "\n",
    "def randomize_attention_output_dense(model, layer_nb):\n",
    "    \"\"\"Randomize attention dense network weights of a given layer and put bias to zero.\n",
    "    \"\"\"\n",
    "    layer_nb = layer_nb - 1\n",
    "    model.encoder.layer[layer_nb].attention.output.dense.weight = torch.nn.parameter.Parameter(torch.rand_like(model.encoder.layer[layer_nb].attention.output.dense.weight))\n",
    "    model.encoder.layer[layer_nb].attention.output.dense.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.encoder.layer[layer_nb].attention.output.dense.bias))\n",
    "    return model\n",
    "\n",
    "\n",
    "def randomize_intermediate_dense(model, layer_nb):\n",
    "    \"\"\"Randomize intermediate dense network weights of a given layer and put bias to zero.\n",
    "    \"\"\"\n",
    "    layer_nb = layer_nb - 1\n",
    "    model.encoder.layer[layer_nb].intermediate.dense.weight = torch.nn.parameter.Parameter(torch.rand_like(model.encoder.layer[layer_nb].intermediate.dense.weight))\n",
    "    model.encoder.layer[layer_nb].intermediate.dense.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.encoder.layer[layer_nb].intermediate.dense.bias))\n",
    "    return model\n",
    "\n",
    "def randomize_outptut_dense(model, layer_nb):\n",
    "    \"\"\"Randomize output dense network weights of a given layer and put bias to zero.\n",
    "    \"\"\"\n",
    "    layer_nb = layer_nb - 1\n",
    "    model.encoder.layer[layer_nb].output.dense.weight = torch.nn.parameter.Parameter(torch.rand_like(model.encoder.layer[layer_nb].output.dense.weight))\n",
    "    model.encoder.layer[layer_nb].output.dense.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.encoder.layer[layer_nb].output.dense.bias))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_embeddings(model):\n",
    "    \"\"\"Randomize embeddings weights and put bias to zero.\n",
    "    \"\"\"\n",
    "    model.embeddings.word_embeddings.weight = torch.nn.parameter.Parameter(torch.rand_like(model.embeddings.word_embeddings.weight))\n",
    "    model.embeddings.position_embeddings.weight = torch.nn.parameter.Parameter(torch.rand_like(model.embeddings.position_embeddings.weight))\n",
    "    model.embeddings.token_type_embeddings.weight = torch.nn.parameter.Parameter(torch.rand_like(model.embeddings.token_type_embeddings.weight))\n",
    "    model.embeddings.LayerNorm.weight = torch.nn.parameter.Parameter(torch.rand_like(model.embeddings.LayerNorm.weight))\n",
    "    model.embeddings.LayerNorm.bias = torch.nn.parameter.Parameter(torch.zeros_like(model.embeddings.LayerNorm.bias))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-cased  - Extracting activations for layer 0...\n",
      "############# Run 0 #############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:23, 23.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Run 1 #############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:50, 24.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Run 2 #############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [01:23, 26.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Run 3 #############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [01:44, 34.92s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-c3f1e35a56a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"############# Run {} #############\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mcheck_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_path_folders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mactivations\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mhidden_states_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mattention_heads_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/Parietal/NLP_models/BERT/model.py\u001b[0m in \u001b[0;36mextract_activations\u001b[0;34m(self, iterator, language)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sentence'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_classic_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mhidden_states_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mattention_heads_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/Parietal/NLP_models/BERT/model.py\u001b[0m in \u001b[0;36mget_classic_activations\u001b[0;34m(self, iterator, language)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0mencoded_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# last_hidden_state, pooler_output, hidden_states, attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# last_hidden_state dimension: (batch_size, sequence_length, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# pooler_output dimension: (batch_size, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/Parietal/NLP_models/BERT/modeling_hacked_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m         )\n\u001b[1;32m    782\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/Parietal/NLP_models/BERT/modeling_hacked_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             layer_outputs = layer_module(\n\u001b[0;32m--> 396\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m             )\n\u001b[1;32m    398\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/Parietal/NLP_models/BERT/modeling_hacked_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add cross attentions if we output attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/Parietal/NLP_models/BERT/modeling_hacked_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for layer in range(13):\n",
    "    extractor = BertExtractor(bert_model, language, name, prediction_type, output_hidden_states=True, output_attentions=True, config_path=None)\n",
    "    if layer==0:\n",
    "        extractor.model = randomize_embeddings(extractor.model)\n",
    "    else:\n",
    "        extractor.model = randomize_layer(extractor.model, layer)\n",
    "    print(extractor.name, ' - Extracting activations for layer {}...'.format(layer))\n",
    "    for run_index, iterator in tqdm(enumerate(iterator_list)):\n",
    "        print(\"############# Run {} #############\".format(run_index))\n",
    "        check_folder(saving_path_folders[index])\n",
    "        activations  = extractor.extract_activations(iterator, language)\n",
    "        hidden_states_activations = activations[0]\n",
    "        attention_heads_activations = activations[1]\n",
    "        (cls_hidden_states_activations, cls_attention_activations) = activations[2]\n",
    "        (sep_hidden_states_activations, sep_attention_activations) = activations[3]\n",
    "        activations = pd.concat([hidden_states_activations, attention_heads_activations], axis=1)\n",
    "        cls_activations = pd.concat([cls_hidden_states_activations, cls_attention_activations], axis=1)\n",
    "        sep_activations = pd.concat([sep_hidden_states_activations, sep_attention_activations], axis=1)\n",
    "\n",
    "        # activations\n",
    "        heads = np.arange(1, 13)\n",
    "        columns_to_retrieve = ['hidden_state-layer-{}-{}'.format(layer, i) for i in range(1, 769)]\n",
    "        if layer > 0:\n",
    "            columns_to_retrieve += ['attention-layer-{}-head-{}-{}'.format(layer, head, i) for head in heads for i in range(1, 65)]\n",
    "        activations = activations[columns_to_retrieve]\n",
    "\n",
    "        # CLS\n",
    "        heads = np.arange(1, 13)\n",
    "        columns_to_retrieve = ['CLS-hidden_state-layer-{}-{}'.format(layer, i) for i in range(1, 769)]\n",
    "        if layer > 0:\n",
    "            columns_to_retrieve += ['CLS-attention-layer-{}-head-{}-{}'.format(layer, head, i) for head in heads for i in range(1, 65)]\n",
    "        cls_activations = cls_activations[columns_to_retrieve]\n",
    "\n",
    "        # SEP\n",
    "        heads = np.arange(1, 13)\n",
    "        columns_to_retrieve = ['SEP-hidden_state-layer-{}-{}'.format(layer, i) for i in range(1, 769)]\n",
    "        if layer > 0:\n",
    "            columns_to_retrieve += ['SEP-attention-layer-{}-head-{}-{}'.format(layer, head, i) for head in heads for i in range(1, 65)]\n",
    "        sep_activations = sep_activations[columns_to_retrieve]\n",
    "    \n",
    "        check_folder(os.path.join(saving_path_folder, name + '_layer-{}'.format(layer)))\n",
    "        activations.to_csv(os.path.join(saving_path_folder, name + '_layer-{}'.format(layer), 'activations_run{}.csv'.format(run_index + 1)), index=False)\n",
    "        cls_activations.to_csv(os.path.join(saving_path_folder, name + '_layer-{}'.format(layer), 'cls_run{}.csv'.format(run_index + 1)), index=False)\n",
    "        sep_activations.to_csv(os.path.join(saving_path_folder, name + '_layer-{}'.format(layer), 'sep_run{}.csv'.format(run_index + 1)), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
