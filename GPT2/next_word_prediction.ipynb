{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ap263679/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "/home/ap263679/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/ap263679/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/home/ap263679/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/home/ap263679/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/ap263679/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/home/ap263679/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/home/ap263679/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/ap263679/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/home/ap263679/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/home/ap263679/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/online_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from model import GPT2Extractor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tokenizer import tokenize\n",
    "#from utils import set_seed\n",
    "from numpy import linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import gpt2_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(activations, path, name, run_index, n_layers_hidden=13, n_layers_attention=12, hidden_size=768):\n",
    "    assert activations.values.shape[1] == (n_layers_hidden + n_layers_attention) * hidden_size\n",
    "    indexes = [[index*hidden_size, (index+1)*hidden_size] for index in range(n_layers_hidden + n_layers_attention)]\n",
    "    for order in [2]: # np.inf\n",
    "        matrices = []\n",
    "        for i, index in enumerate(indexes):\n",
    "            matrix = activations.values[:, index[0]:index[1]]\n",
    "            #with_std = True if order=='std' else False\n",
    "            #scaler = StandardScaler(with_mean=True, with_std=with_std)\n",
    "            #scaler.fit(matrix)\n",
    "            #matrix = scaler.transform(matrix)\n",
    "            if order is not None and order != 'std':\n",
    "                matrix = matrix / np.mean(la.norm(matrix, ord=order, axis=1))\n",
    "            matrices.append(matrix)\n",
    "        matrices = np.hstack(matrices)\n",
    "        new_data = pd.DataFrame(matrices, columns=activations.columns)\n",
    "        new_path = path + '_norm-' + str(order).replace('np.', '')\n",
    "        check_folder(new_path)\n",
    "        new_data.to_csv(os.path.join(new_path, name + '_run{}.csv'.format(run_index + 1)), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_tokenized_to_untokenized(tokenized_sent, untokenized_sent, connection_character='Ġ', eos_token='<|endoftext|>'):\n",
    "    '''Aligns tokenized and untokenized sentence given non-subwords \"Ġ\" prefixed\n",
    "    Assuming that each subword token that does start a new word is prefixed\n",
    "    by \"Ġ\", computes an alignment between the un-subword-tokenized\n",
    "    and subword-tokenized sentences.\n",
    "    Args:\n",
    "      tokenized_sent: a list of strings describing a subword-tokenized sentence\n",
    "      untokenized_sent: a list of strings describing a sentence, no subword tok.\n",
    "    Returns:\n",
    "      A dictionary of type {int: list(int)} mapping each untokenized sentence\n",
    "      index to a list of subword-tokenized sentence indices\n",
    "    '''\n",
    "    mapping = defaultdict(list)\n",
    "    untokenized_sent_index = 0\n",
    "    tokenized_sent_index = 0\n",
    "    while (untokenized_sent_index < len(untokenized_sent) and tokenized_sent_index < len(tokenized_sent)):\n",
    "        while (tokenized_sent_index + 1  < len(tokenized_sent) and (not tokenized_sent[tokenized_sent_index + 1].startswith(connection_character)) and tokenized_sent[tokenized_sent_index+1]!=eos_token):\n",
    "            mapping[untokenized_sent_index].append(tokenized_sent_index)\n",
    "            tokenized_sent_index += 1\n",
    "        mapping[untokenized_sent_index].append(tokenized_sent_index)\n",
    "        untokenized_sent_index += 1\n",
    "        tokenized_sent_index += 1\n",
    "    return mapping\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/data/text/english/text_english_run*.txt' # path to text input\n",
    "language = 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '/Users/alexpsq/Code/Parietal/data/text_english_run*.txt' # path to text input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:00<00:00, 337042.29it/s]\n",
      "100%|██████████| 135/135 [00:00<00:00, 284109.90it/s]\n",
      "100%|██████████| 176/176 [00:00<00:00, 453996.00it/s]\n",
      "100%|██████████| 173/173 [00:00<00:00, 47494.08it/s]\n",
      "100%|██████████| 177/177 [00:00<00:00, 25170.09it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 512136.61it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 500659.92it/s]\n",
      "100%|██████████| 145/145 [00:00<00:00, 398280.34it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 536269.88it/s]\n"
     ]
    }
   ],
   "source": [
    "paths = sorted(glob.glob(template))\n",
    "iterator_list = [tokenize(path, language, train=False) for path in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [02:08,  7.57s/it]"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "for attention_length_before in [1, 5, 15, 45]:\n",
    "\n",
    "    config = {\n",
    "        'number_of_sentence': 1, \n",
    "        'number_of_sentence_before': 10, \n",
    "        'attention_length_before': attention_length_before, \n",
    "        'stop_attention_at_sent_before': 1,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "    extractor_masked = GPT2Extractor('gpt2', \n",
    "                                  'english', \n",
    "                                  'test', \n",
    "                                  'control-context', \n",
    "                                  output_hidden_states=True, \n",
    "                                  output_attentions=False, \n",
    "                                  attention_length_before=config['attention_length_before'],\n",
    "                                  config_path=None, \n",
    "                                  number_of_sentence=config['number_of_sentence'], \n",
    "                                  number_of_sentence_before=config['number_of_sentence_before'], \n",
    "                                  prediction=True\n",
    "                                 )\n",
    "    \n",
    "    for run in range(1, 9):\n",
    "        # Tokens are masked\n",
    "        batches_masked, indexes_masked = gpt2_utils.batchify_with_detailed_indexes(\n",
    "                    iterator_list[run], \n",
    "                    config['number_of_sentence'], \n",
    "                    config['number_of_sentence_before'], \n",
    "                    'gpt2',\n",
    "                    add_prefix_space=True\n",
    "                    )\n",
    "\n",
    "        # Preprocessing masked\n",
    "        indexes_masked_tmp = [(indexes_masked[i][-config['number_of_sentence']][0], indexes_masked[i][-1][1]) for i in range(len(indexes_masked))]\n",
    "        indexes_masked_tmp[0] = (indexes_masked[0][0][0], indexes_masked[0][-1][1])\n",
    "\n",
    "        # activation generation masked\n",
    "        output = []\n",
    "        activations = []\n",
    "        for index_batch, batch in tqdm(enumerate(batches_masked)):\n",
    "            batch = batch.strip() # Remove trailing character\n",
    "            batch = '<|endoftext|> ' + batch + ' <|endoftext|>'\n",
    "\n",
    "            tokenized_text = tokenizer.tokenize(batch, add_prefix_space=False)\n",
    "            mapping = gpt2_utils.match_tokenized_to_untokenized(tokenized_text, batch)\n",
    "\n",
    "            beg = indexes_masked_tmp[index_batch][0] \n",
    "            end = indexes_masked_tmp[index_batch][1] \n",
    "\n",
    "            inputs_ids = torch.tensor([tokenizer.convert_tokens_to_ids(tokenized_text)])\n",
    "            inputs_ids = torch.cat(inputs_ids.size(1) * [inputs_ids])\n",
    "            inputs_ids = inputs_ids[beg:end, :]\n",
    "\n",
    "            attention_mask =  torch.diag_embed(torch.tensor([0 for x in tokenized_text]))\n",
    "            for i in range(min(len(tokenized_text), config['attention_length_before'])):\n",
    "                attention_mask = torch.add(attention_mask, torch.diag_embed(torch.tensor([1 for x in range(len(tokenized_text) - i)]), offset=-i))\n",
    "            attention_mask = attention_mask[beg:end, :]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                encoded_layers = extractor_masked.model(inputs_ids, attention_mask=attention_mask, labels=inputs_ids, return_dict=False) # last_hidden_state, pooler_output, hidden_states, attentions       \n",
    "                #print(beg, len(tokenized_text) - encoded_layers[2][0].size(0) - 1)\n",
    "                #hidden_states_activations_ = np.vstack([torch.cat([encoded_layers[2][layer][i,len(tokenized_text) - encoded_layers[2][layer].size(0) + i - 1,:].unsqueeze(0) for i in range(encoded_layers[2][layer].size(0))], dim=0).unsqueeze(0).detach().numpy() for layer in range(len(encoded_layers[2]))])\n",
    "                #hidden_states_activations_ = np.concatenate([np.zeros((hidden_states_activations_.shape[0], indexes_masked_tmp[index_batch][0] , hidden_states_activations_.shape[-1])), hidden_states_activations_, np.zeros((hidden_states_activations_.shape[0], len(tokenized_text) - indexes_masked_tmp[index_batch][1], hidden_states_activations_.shape[-1]))], axis=1)\n",
    "                # retrieve all the hidden states (dimension = layer_count * len(tokenized_text) * feature_count)\n",
    "                loss_ = torch.cat([encoded_layers[0][i,len(tokenized_text) - encoded_layers[0].size(0) + i - 2].unsqueeze(0) for i in range(encoded_layers[0].size(0))], dim=0).unsqueeze(0).detach().numpy()\n",
    "\n",
    "                activations.append(loss_)\n",
    "\n",
    "        np.save(os.path.join(\n",
    "            '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/derivatives/fMRI/16_masking_before_vs_after_softmax', \n",
    "            f\"masking_before_at_{config['attention_length_before']}_run{run+1}.npy\"), \n",
    "                np.hstack(activations).reshape(-1)\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-50642704a182>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#assert ' '.join(output) == ' '.join(iterator_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(\n",
    "    '/neurospin/unicog/protocols/IRMf/LePetitPrince_Pallier_2018/LePetitPrince/derivatives/fMRI/16_masking_before_vs_after_softmax', \n",
    "    f\"masking_before_at_{config['attention_length_before']}.npy\"), \n",
    "        np.hstack(activations).reshape(-1)\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
