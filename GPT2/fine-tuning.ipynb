{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "import time\n",
    "import yaml\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import inspect\n",
    "import logging\n",
    "import datetime\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AdamW, GPT2Config, get_linear_schedule_with_warmup\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel, WEIGHTS_NAME, CONFIG_NAME\n",
    "\n",
    "from language_modeling import LMDataset, LMProcessor\n",
    "from utils import read_yaml, set_seed, format_time, filter_args, get_device, save, check_folder, save_yaml\n",
    "from processors import DataProcessor, ModelProcessor\n",
    "from reporting import Report\n",
    "from dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file = 'fine_tuning_template.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = read_yaml(yaml_file)\n",
    "check_folder(parameters['output_dir'])\n",
    "save_yaml(parameters, os.path.join(parameters['output_dir'], 'config.yml'))\n",
    "logging.basicConfig(filename=os.path.join(parameters['output_dir'], parameters['log_file']), filemode='w+', level=logging.INFO)\n",
    "logging.info(\"Parameters fetched.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 GPU(s) available.\n",
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Setting seed for reproductibility...\") \n",
    "set_seed(parameters['seed'])\n",
    "logging.info(\"\\tDone.\")\n",
    "\n",
    "logging.info(\"Set and retrieve the device on which to run...\")\n",
    "device = get_device()\n",
    "task = parameters['task'].lower()\n",
    "logging.info(\"\\tDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Instanciating dataset and data processor...\")\n",
    "if task in ['language_modeling']:\n",
    "    data = LMDataset(task, parameters['dataset_name'].lower(), dataset_dir=parameters['dataset_dir'])\n",
    "    processor = LMProcessor()\n",
    "logging.info(\"\\tDone.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:00<00:00, 403011.42it/s]\n",
      "100%|██████████| 135/135 [00:00<00:00, 295373.52it/s]\n",
      "100%|██████████| 176/176 [00:00<00:00, 430435.86it/s]\n",
      "100%|██████████| 173/173 [00:00<00:00, 433720.62it/s]\n",
      "100%|██████████| 177/177 [00:00<00:00, 356234.07it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 440004.69it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 345123.25it/s]\n",
      "100%|██████████| 145/145 [00:00<00:00, 283135.05it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 533305.24it/s]\n",
      "100%|██████████| 135/135 [00:00<00:00, 123066.95it/s]\n",
      "100%|██████████| 135/135 [00:00<00:00, 416959.53it/s]\n",
      "100%|██████████| 176/176 [00:00<00:00, 324197.41it/s]\n",
      "100%|██████████| 173/173 [00:00<00:00, 554327.42it/s]\n",
      "100%|██████████| 177/177 [00:00<00:00, 294483.07it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 378433.44it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 389613.07it/s]\n",
      "100%|██████████| 145/145 [00:00<00:00, 366149.36it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 287490.37it/s]\n",
      "100%|██████████| 135/135 [00:00<00:00, 725007.73it/s]\n",
      "100%|██████████| 135/135 [00:00<00:00, 711345.53it/s]\n",
      "100%|██████████| 176/176 [00:00<00:00, 893701.58it/s]\n",
      "100%|██████████| 173/173 [00:00<00:00, 882742.81it/s]\n",
      "100%|██████████| 177/177 [00:00<00:00, 776560.47it/s]\n",
      "100%|██████████| 216/216 [00:00<00:00, 1102152.88it/s]\n",
      "100%|██████████| 196/196 [00:00<00:00, 1064875.11it/s]\n",
      "100%|██████████| 145/145 [00:00<00:00, 581984.77it/s]\n",
      "100%|██████████| 207/207 [00:00<00:00, 618390.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n",
      "Tokenizing...\n",
      "Preprocessing...\n",
      "Preprocessed.\n",
      "Tokenized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Fetching data (training + validation) and parameters...\")\n",
    "data._fetch_dataset()\n",
    "for set_type in ['train', 'dev']:\n",
    "    data.process_dataset(set_type)\n",
    "if parameters['do_test']:\n",
    "    data.process_dataset('test')\n",
    "logging.info(\"\\tDone.\")\n",
    "\n",
    "logging.info(\"Fetching pre-trained GPT-2 model: {} and Tokenizer: {} for the task: {}...\".format(parameters['pretrained_model'],\n",
    "                                                                                                parameters['pretrained_tokenizer'],\n",
    "                                                                                                parameters['task']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if task in ['language_modeling']:\n",
    "    model = GPT2LMHeadModel.from_pretrained(\n",
    "                parameters['pretrained_model'],\n",
    "                output_attentions=parameters['output_attentions'], # Whether the model returns attentions weights.\n",
    "                output_hidden_states=parameters['output_hidden_states'], # Whether the model returns all hidden-states.\n",
    "    )\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(parameters['pretrained_tokenizer'])\n",
    "model.to(device)\n",
    "logging.info(\"\\tDone.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Get input examples...\")\n",
    "train_examples = processor.get_train_examples(data)\n",
    "dev_examples = processor.get_dev_examples(data)\n",
    "if parameters['do_test']:\n",
    "    test_examples = processor.get_test_examples(data)\n",
    "logging.info(\"\\tDone.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " parameters['max_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1560/1560 [00:06<00:00, 236.11it/s]\n",
      "100%|██████████| 1560/1560 [00:06<00:00, 232.70it/s]\n",
      "100%|██████████| 1560/1560 [00:06<00:00, 239.02it/s]\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Get input features...\")\n",
    "train_features = processor.convert_examples_to_features(train_examples, parameters['max_length'], tokenizer) \n",
    "dev_features = processor.convert_examples_to_features(dev_examples, parameters['max_length'], tokenizer)\n",
    "if parameters['do_test']:\n",
    "    test_features = processor.convert_examples_to_features(test_examples, parameters['max_length'], tokenizer) \n",
    "logging.info(\"\\tDone.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Creating data loaders...\")\n",
    "train_dataloader = processor.get_data_loader(train_features, \n",
    "                                                batch_size=parameters['batch_size'], \n",
    "                                                local_rank=parameters['local_rank'], \n",
    "                                                set_type='train')\n",
    "dev_dataloader = processor.get_data_loader(dev_features, \n",
    "                                            batch_size=parameters['batch_size'], \n",
    "                                            local_rank=parameters['local_rank'], \n",
    "                                            set_type='dev')\n",
    "if parameters['do_test']:\n",
    "    test_dataloader = processor.get_data_loader(test_features, \n",
    "                                                batch_size=parameters['batch_size'], \n",
    "                                                local_rank=parameters['local_rank'], \n",
    "                                                set_type='test')\n",
    "logging.info(\"\\tDone.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Creating optimizer and learning rate scheduler...\")\n",
    "optimizer = AdamW(\n",
    "                model.parameters(),\n",
    "                lr=float(parameters['learning_rate']),\n",
    "                eps=float(parameters['adam_epsilon'])\n",
    "            )\n",
    "total_steps = len(train_dataloader) * parameters['nb_epochs'] # Total number of training steps is [nb batches] x [nb epochs]. \n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, \n",
    "                num_warmup_steps=parameters['num_warmup_steps'],\n",
    "                num_training_steps=total_steps\n",
    "            )\n",
    "logging.info(\"\\tDone.\")\n",
    "\n",
    "logging.info(\"Fine-tuning the model.\")\n",
    "model_processor = ModelProcessor(model, optimizer, tokenizer, \n",
    "                                    scheduler, device, \n",
    "                                    parameters['metric_name'], \n",
    "                                    parameters['nb_epochs'],\n",
    "                                    parameters['use_output_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [02:03, 24.80s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-161d1f10dd8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output_dir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Code/Parietal/NLP_models/GPT2/processors.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataloader, validation_dataloader, output_dir)\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0;31m# Report progress.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melapsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_train_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;31m# Calculate the average loss over all of the batches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/Parietal/NLP_models/GPT2/processors.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch, total_train_loss)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;31m# Perform a backward pass to calculate the gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;31m# Clip the norm of the gradients to 1.0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# This is to help prevent the \"exploding gradients\" problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/parietal/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_stats = model_processor.train(train_dataloader, dev_dataloader, parameters['output_dir'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Validation reports: \")\n",
    "for stat in training_stats:\n",
    "    logging.info(stat['report'])\n",
    "test_accuracy, test_loss = None, None\n",
    "if parameters['do_test']:\n",
    "    logging.info(\"Evaluation report: \")\n",
    "    test_accuracy, test_loss, test_time, report = model_processor.evaluate(test_dataloader) \n",
    "    logging.info(report)\n",
    "logging.info(\"\\tDone.\")\n",
    "\n",
    "logging.info(\"Saving fine-tuned model to {}...\".format(os.path.join(parameters['output_dir'], 'fine_tuned')))\n",
    "save(model, tokenizer, parameters['output_dir'], 'fine_tuned')\n",
    "logging.info(\"\\tDone.\")\n",
    "\n",
    "logging.info(\"Plotting training and validation losses...\")\n",
    "Report.plots_train_val_loss(training_stats, parameters['nb_epochs'], \n",
    "                            os.path.join(parameters['output_dir'], 'train_val_loss.png'), \n",
    "                            test_accuracy=test_accuracy, test_loss=test_loss)\n",
    "logging.info(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
